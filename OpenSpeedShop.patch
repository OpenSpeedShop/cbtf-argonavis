diff -Naur OpenSpeedShop.ORIGINAL/cmake/FindCBTF-Argo.cmake OpenSpeedShop.NEW-CUPTI/cmake/FindCBTF-Argo.cmake
--- OpenSpeedShop.ORIGINAL/cmake/FindCBTF-Argo.cmake	2015-08-01 13:54:01.000000000 -0500
+++ OpenSpeedShop.NEW-CUPTI/cmake/FindCBTF-Argo.cmake	1969-12-31 18:00:00.000000000 -0600
@@ -1,51 +0,0 @@
-################################################################################
-# Copyright (c) 2006-2015 Krell Institute. All Rights Reserved.
-#
-# This program is free software; you can redistribute it and/or modify it under
-# the terms of the GNU General Public License as published by the Free Software
-# Foundation; either version 2 of the License, or (at your option) any later
-# version.
-#
-# This program is distributed in the hope that it will be useful, but WITHOUT
-# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
-# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
-# details.
-#
-# You should have received a copy of the GNU General Public License along with
-# this program; if not, write to the Free Software Foundation, Inc., 59 Temple
-# Place, Suite 330, Boston, MA  02111-1307  USA
-################################################################################
-
-include(FindPackageHandleStandardArgs)
-
-set(CBTF_ARGO_XMLDIR ${CBTF_ARGO_DIR}/share/KrellInstitute/xml)
-
-find_library(CBTF_MESSAGES_CUDA_SHARED_LIBRARY
-    NAMES libcbtf-messages-cuda.so
-    HINTS ${CBTF_ARGO_DIR} $ENV{CBTF_ARGO_DIR}
-    PATH_SUFFIXES lib lib64
-    )
-
-find_path(CBTF_MESSAGES_CUDA_INCLUDE_DIR
-    KrellInstitute/Messages/CUDA_data.h
-    HINTS ${CBTF_ARGO_DIR} $ENV{CBTF_ARGO_DIR}
-    PATH_SUFFIXES include
-    )
-
-find_package_handle_standard_args(
-    CBTF-Argo-MessagesCuda DEFAULT_MSG
-    CBTF_MESSAGES_CUDA_SHARED_LIBRARY
-    CBTF_MESSAGES_CUDA_INCLUDE_DIR
-    )
-
-set(CBTF_MESSAGES_CUDA_LIBRARIES
-    ${CBTF_MESSAGES_CUDA_SHARED_LIBRARY}
-    )
-
-set(CBTF_MESSAGES_CUDA_INCLUDE_DIRS ${CBTF_MESSAGES_CUDA_INCLUDE_DIR})
-
-mark_as_advanced(
-    CBTF_ARGO_XMLDIR
-    CBTF_MESSAGES_CUDA_SHARED_LIBRARY
-    CBTF_MESSAGES_INCLUDE_DIR
-    )
diff -Naur OpenSpeedShop.ORIGINAL/cmake/FindCBTF-ArgoNavis.cmake OpenSpeedShop.NEW-CUPTI/cmake/FindCBTF-ArgoNavis.cmake
--- OpenSpeedShop.ORIGINAL/cmake/FindCBTF-ArgoNavis.cmake	1969-12-31 18:00:00.000000000 -0600
+++ OpenSpeedShop.NEW-CUPTI/cmake/FindCBTF-ArgoNavis.cmake	2016-02-26 16:07:37.770987879 -0600
@@ -0,0 +1,93 @@
+################################################################################
+# Copyright (c) 2006-2015 Krell Institute. All Rights Reserved.
+# Copyright (c) 2015,2016 Argo Navis Technologies. All Rights Reserved.
+#
+# This program is free software; you can redistribute it and/or modify it under
+# the terms of the GNU General Public License as published by the Free Software
+# Foundation; either version 2 of the License, or (at your option) any later
+# version.
+#
+# This program is distributed in the hope that it will be useful, but WITHOUT
+# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
+# details.
+#
+# You should have received a copy of the GNU General Public License along with
+# this program; if not, write to the Free Software Foundation, Inc., 59 Temple
+# Place, Suite 330, Boston, MA  02111-1307  USA
+################################################################################
+
+include(FindPackageHandleStandardArgs)
+
+set(CBTF_ARGONAVIS_XMLDIR ${CBTF_ARGONAVIS_DIR}/share/KrellInstitute/xml)
+mark_as_advanced(CBTF_ARGONAVIS_XMLDIR)
+
+find_library(CBTF_MESSAGES_CUDA_SHARED_LIBRARY
+    NAMES libcbtf-messages-cuda.so
+    HINTS ${CBTF_ARGONAVIS_DIR} $ENV{CBTF_ARGONAVIS_DIR}
+    PATH_SUFFIXES lib lib64
+    )
+
+find_path(CBTF_MESSAGES_CUDA_INCLUDE_DIR
+    KrellInstitute/Messages/CUDA_data.h
+    HINTS ${CBTF_ARGONAVIS_DIR} $ENV{CBTF_ARGONAVIS_DIR}
+    PATH_SUFFIXES include
+    )
+
+find_package_handle_standard_args(
+    CBTF-Messages-CUDA DEFAULT_MSG
+    CBTF_MESSAGES_CUDA_SHARED_LIBRARY
+    CBTF_MESSAGES_CUDA_INCLUDE_DIR
+    )
+
+set(CBTF_MESSAGES_CUDA_LIBRARIES ${CBTF_MESSAGES_CUDA_SHARED_LIBRARY})
+set(CBTF_MESSAGES_CUDA_INCLUDE_DIRS ${CBTF_MESSAGES_CUDA_INCLUDE_DIR})
+mark_as_advanced(CBTF_MESSAGES_CUDA_SHARED_LIBRARY CBTF_MESSAGES_INCLUDE_DIR)
+
+find_library(ARGONAVIS_BASE_SHARED_LIBRARY
+    NAMES libargonavis-base.so
+    HINTS ${CBTF_ARGONAVIS_DIR} $ENV{CBTF_ARGONAVIS_DIR}
+    PATH_SUFFIXES lib lib64
+    )
+
+find_path(ARGONAVIS_BASE_INCLUDE_DIR
+    ArgoNavis/Base/LinkedObject.hpp
+    HINTS ${CBTF_ARGONAVIS_DIR} $ENV{CBTF_ARGONAVIS_DIR}
+    PATH_SUFFIXES include
+    )
+
+find_package_handle_standard_args(
+    ArgoNavis-Base DEFAULT_MSG
+    ARGONAVIS_BASE_SHARED_LIBRARY
+    ARGONAVIS_BASE_INCLUDE_DIR
+    )
+
+set(ARGONAVIS_BASE_LIBRARIES ${ARGONAVIS_BASE_SHARED_LIBRARY})
+set(ARGONAVIS_BASE_INCLUDE_DIRS ${ARGONAVIS_BASE_INCLUDE_DIR})
+mark_as_advanced(ARGONAVIS_BASE_SHARED_LIBRARY ARGONAVIS_BASE_INCLUDE_DIR)
+
+find_library(ARGONAVIS_CUDA_SHARED_LIBRARY
+    NAMES libargonavis-cuda.so
+    HINTS ${CBTF_ARGONAVIS_DIR} $ENV{CBTF_ARGONAVIS_DIR}
+    PATH_SUFFIXES lib lib64
+    )
+
+find_path(ARGONAVIS_CUDA_INCLUDE_DIR
+    ArgoNavis/CUDA/PerformanceData.hpp
+    HINTS ${CBTF_ARGONAVIS_DIR} $ENV{CBTF_ARGONAVIS_DIR}
+    PATH_SUFFIXES include
+    )
+
+find_package_handle_standard_args(
+    ArgoNavis-CUDA DEFAULT_MSG
+    ARGONAVIS_CUDA_SHARED_LIBRARY
+    ARGONAVIS_CUDA_INCLUDE_DIR
+    )
+
+set(ARGONAVIS_CUDA_LIBRARIES ${ARGONAVIS_CUDA_SHARED_LIBRARY})
+set(ARGONAVIS_CUDA_INCLUDE_DIRS ${ARGONAVIS_CUDA_INCLUDE_DIR})
+mark_as_advanced(ARGONAVIS_CUDA_SHARED_LIBRARY ARGONAVIS_CUDA_INCLUDE_DIR)
+
+if (CBTF-MESSAGES-CUDA_FOUND AND ARGONAVIS-BASE_FOUND AND ARGONAVIS-CUDA_FOUND)
+  set(BUILD_CUDA_SUPPORT "TRUE")
+endif()
diff -Naur OpenSpeedShop.ORIGINAL/CMakeLists.txt OpenSpeedShop.NEW-CUPTI/CMakeLists.txt
--- OpenSpeedShop.ORIGINAL/CMakeLists.txt	2016-02-19 00:37:33.000000000 -0600
+++ OpenSpeedShop.NEW-CUPTI/CMakeLists.txt	2016-02-26 16:25:00.331536747 -0600
@@ -1,5 +1,6 @@
 ################################################################################
 # Copyright (c) 2014-2016 Krell Institute. All Rights Reserved.
+# Copyright (c) 2015,2016 Argo Navis Technologies. All Rights Reserved.
 #
 # This program is free software; you can redistribute it and/or modify it under
 # the terms of the GNU General Public License as published by the Free Software
@@ -283,7 +284,8 @@
     find_package(CBTF-Krell)
     find_package(MRNet 4.0.0)
     find_package(XercesC 3.0)
-    find_package(CBTF-Argo)
+    find_package(CBTF-ArgoNavis)
+
     message(STATUS "CBTF-ARGO-MESSAGESCUDA_FOUND=" ${CBTF-ARGO-MESSAGESCUDA_FOUND} )
 
     if (NOT "${CBTF_KRELL_CN_RUNTIME_DIR}" STREQUAL "")
@@ -514,23 +516,16 @@
 	add_subdirectory(openss)
     endif()
 
-
-# Are there really any runtime needs for cuda????
+    # This is not quite what we would want. Instrumentor choice should not
+    # prevent a user from viewing cuda databases. Not sure if ossdumpsym
+    # should be tied to instrumentor cbtf.
     if (INSTRUMENTOR MATCHES "cbtf")
-
         add_subdirectory(ossdumpsym)
+    endif()
 
-        # The cuda code for the client does not depend on the cupti or cuda
-        # runtime systems.  The cuda client code does depend on the cuda messages
-        # from the cbtf-argonavis package.
-        #
-        # For this to work we need a cbtf-argnoavis find package that sets
-        # a package name var like CBTF-ArgNavis-MessagesCuda.
-
-        if (CBTF-ARGO-MESSAGESCUDA_FOUND)
-            add_subdirectory(libopenss-queries-cuda)
-            add_subdirectory(ossdumpcuda)
-        endif()
+    if(BUILD_CUDA_SUPPORT)
+        add_subdirectory(libopenss-queries-cuda)
+        add_subdirectory(ossdumpcuda)
     endif()
 
 else()
@@ -571,18 +566,17 @@
     #--------------------------------------------------------------------------------
     add_subdirectory(openss)
 
-    # this is not quite what we would want. Instrumentor choice should not
+    # This is not quite what we would want. Instrumentor choice should not
     # prevent a user from viewing cuda databases. Not sure if ossdumpsym
     # should be tied to instrumentor cbtf.
     if (INSTRUMENTOR MATCHES "cbtf")
         add_subdirectory(ossdumpsym)
-#   if (CUDA_FOUND)
-    if (CBTF-ARGO-MESSAGESCUDA_FOUND)
+    endif()
+
+    if(BUILD_CUDA_SUPPORT)
         add_subdirectory(libopenss-queries-cuda)
         add_subdirectory(ossdumpcuda)
     endif()
-#   endif()
-    endif()
 
     # Install <openss related>.py files needed when running the OSS python interface
     add_subdirectory(pyscripting)
diff -Naur OpenSpeedShop.ORIGINAL/libopenss-queries-cuda/CMakeLists.txt OpenSpeedShop.NEW-CUPTI/libopenss-queries-cuda/CMakeLists.txt
--- OpenSpeedShop.ORIGINAL/libopenss-queries-cuda/CMakeLists.txt	2015-08-01 13:13:28.000000000 -0500
+++ OpenSpeedShop.NEW-CUPTI/libopenss-queries-cuda/CMakeLists.txt	2016-02-26 16:07:37.770987879 -0600
@@ -1,5 +1,6 @@
 ################################################################################
 # Copyright (c) 2014-2015 Krell Institute. All Rights Reserved.
+# Copyright (c) 2015,2016 Argo Navis Technologies. All Rights Reserved.
 #
 # This program is free software; you can redistribute it and/or modify it under
 # the terms of the GNU General Public License as published by the Free Software
@@ -16,77 +17,46 @@
 # Place, Suite 330, Boston, MA  02111-1307  USA
 ################################################################################
 
-include(CheckIncludeFile)
-
-check_include_file("stdint.h" STDINT_H_FOUND)
-if (STDINT_H_FOUND)
-    add_definitions(-DHAVE_STDINT_H=1)
-endif()
-if (OpenMP_FLAG_DETECTED)
-    add_definitions(-fopenmp)
-endif()
-
-set(QUERIES_CUDA_SOURCES
-	CUDAData.hxx
-	CUDAData.cxx
+add_library(openss-queries-cuda SHARED
 	CUDAExecXferBalance.hxx
-	CUDAQueries.hxx
-	CUDAQueries.cxx
+	CUDAQueries.hxx CUDAQueries.cxx
 	CUDAXferRate.hxx
     )
 
-add_library(openss-queries-cuda SHARED
-	${QUERIES_CUDA_SOURCES}
-    )
-
-add_definitions(
-  -DOpenSpeedShop_LIBRARY_FILE_DIR="${CMAKE_INSTALL_PREFIX}/lib${LIB_SUFFIX}"
-  )
-  
-
 target_include_directories(openss-queries-cuda PUBLIC
-    ${CUDA_INCLUDE_DIR}
-    ${Boost_INCLUDE_DIRS}
-    ${CBTF_MESSAGES_INCLUDE_DIRS}
-    ${CBTF_MESSAGES_CUDA_INCLUDE_DIRS}
     ${CMAKE_CURRENT_SOURCE_DIR}
-    ${CMAKE_CURRENT_BINARY_DIR}
-    ${PROJECT_SOURCE_DIR}/libopenss-cli
     ${PROJECT_SOURCE_DIR}/libopenss-framework
+    ${Boost_INCLUDE_DIRS}
+    ${CBTF_MESSAGES_CUDA_INCLUDE_DIRS}
+    ${ARGONAVIS_BASE_INCLUDE_DIRS}
+    ${ARGONAVIS_CUDA_INCLUDE_DIRS}
     )
 
 target_link_libraries(openss-queries-cuda
     openss-framework
-    ${CUDA_LIBRARIES}
-    ${CBTF_MESSAGES_LIBRARIES}
     ${CBTF_MESSAGES_CUDA_LIBRARIES}
+    ${ARGONAVIS_BASE_LIBRARIES}
+    ${ARGONAVIS_CUDA_LIBRARIES}
     ${CMAKE_DL_LIBS}
     )
 
-
 set_target_properties(openss-queries-cuda PROPERTIES VERSION 1.1.0)
 
-install(TARGETS openss-queries-cuda
-    LIBRARY DESTINATION lib${LIB_SUFFIX}
-    )
+install(TARGETS openss-queries-cuda LIBRARY DESTINATION lib${LIB_SUFFIX})
 
-add_executable(osscuda2xml
-    osscuda2xml.cxx
-    )
+add_executable(osscuda2xml osscuda2xml.cxx)
 
 target_include_directories(osscuda2xml PUBLIC
     ${CMAKE_CURRENT_SOURCE_DIR}
-    ${CMAKE_CURRENT_SOURCE_DIR}/../libopenss-framework
+    ${PROJECT_SOURCE_DIR}/libopenss-framework
     ${Boost_INCLUDE_DIRS}
     )
 
 target_link_libraries(osscuda2xml
+    openss-queries-cuda
     openss-framework
     openss-framework-cbtf
-    openss-queries-cuda
     ${Boost_LIBRARIES}
     )
 
-install(TARGETS osscuda2xml
-    RUNTIME DESTINATION bin
-    )
+install(TARGETS osscuda2xml RUNTIME DESTINATION bin)
diff -Naur OpenSpeedShop.ORIGINAL/libopenss-queries-cuda/CUDAData.cxx OpenSpeedShop.NEW-CUPTI/libopenss-queries-cuda/CUDAData.cxx
--- OpenSpeedShop.ORIGINAL/libopenss-queries-cuda/CUDAData.cxx	2015-06-30 21:03:01.000000000 -0500
+++ OpenSpeedShop.NEW-CUPTI/libopenss-queries-cuda/CUDAData.cxx	1969-12-31 18:00:00.000000000 -0600
@@ -1,931 +0,0 @@
-////////////////////////////////////////////////////////////////////////////////
-// Copyright (c) 2014 Argo Navis Technologies. All Rights Reserved.
-//
-// This library is free software; you can redistribute it and/or modify it under
-// the terms of the GNU Lesser General Public License as published by the Free
-// Software Foundation; either version 2.1 of the License, or (at your option)
-// any later version.
-//
-// This library is distributed in the hope that it will be useful, but WITHOUT
-// ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
-// FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more
-// details.
-//
-// You should have received a copy of the GNU Lesser General Public License
-// along with this library; if not, write to the Free Software Foundation, Inc.,
-// 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
-////////////////////////////////////////////////////////////////////////////////
-
-/** @file Definition of the CUDAData class. */
-
-#include <boost/algorithm/string/predicate.hpp>
-#include <cstring>
-#include <utility>
-
-#include "KrellInstitute/Messages/CUDA_data.h"
-
-#include "AddressRange.hxx"
-#include "Assert.hxx"
-#include "Blob.hxx"
-#include "Database.hxx"
-#include "EntrySpy.hxx"
-#include "Function.hxx"
-#include "LinkedObject.hxx"
-#include "Path.hxx"
-
-#include "CUDAData.hxx"
-
-using namespace boost;
-using namespace OpenSpeedShop;
-using namespace OpenSpeedShop::Framework;
-using namespace OpenSpeedShop::Queries;
-using namespace std;
-
-
-
-/** Anonymous namespace hiding implementation details. */
-namespace {
-
-    /** Convert a CUDA_CachePreference value to CUDAData::CachePreferences. */
-    CUDAData::CachePreferences convert(const CUDA_CachePreference& value)
-    {
-        switch (value)
-        {
-        case InvalidCachePreference: return CUDAData::kInvalidCachePreference;
-        case NoPreference: return CUDAData::kNoPreference;
-        case PreferShared: return CUDAData::kPreferShared;
-        case PreferCache: return CUDAData::kPreferCache;
-        case PreferEqual: return CUDAData::kPreferEqual;
-        default: return CUDAData::kInvalidCachePreference;
-        }
-    }
-
-    /** Convert a CUDA_CopyKind value to CUDAData::CopyKinds. */
-    CUDAData::CopyKinds convert(const CUDA_CopyKind& value)
-    {
-        switch (value)
-        {
-        case InvalidCopyKind: return CUDAData::kInvalidCopyKind;
-        case UnknownCopyKind: return CUDAData::kUnknownCopyKind;
-        case HostToDevice: return CUDAData::kHostToDevice;
-        case DeviceToHost: return CUDAData::kDeviceToHost;
-        case HostToArray: return CUDAData::kHostToArray;
-        case ArrayToHost: return CUDAData::kArrayToHost;
-        case ArrayToArray: return CUDAData::kArrayToArray;
-        case ArrayToDevice: return CUDAData::kArrayToDevice;
-        case DeviceToArray: return CUDAData::kDeviceToArray;
-        case DeviceToDevice: return CUDAData::kDeviceToDevice;
-        case HostToHost: return CUDAData::kHostToHost;
-        default: return CUDAData::kInvalidCopyKind;
-        }
-    }
-
-    /** Convert a CUDA_MemoryKind value to CUDAData::MemoryKinds. */
-    CUDAData::MemoryKinds convert(const CUDA_MemoryKind& value)
-    {
-        switch (value)
-        {
-        case InvalidMemoryKind: return CUDAData::kInvalidMemoryKind;
-        case UnknownMemoryKind: return CUDAData::kUnknownMemoryKind;
-        case Pageable: return CUDAData::kPageable;
-        case Pinned: return CUDAData::kPinned;
-        case Device: return CUDAData::kDevice;
-        case Array: return CUDAData::kArray;
-        default: return CUDAData::kInvalidMemoryKind;
-        }
-    }
-
-    /** Find the first|last event whose time succeeds|preceeds or equals t. */
-    template <typename T>
-    typename vector<T>::size_type find(const vector<T>& events,
-                                       const Time& t,
-                                       bool is_lower)
-    {
-        int64_t min = 0, max = events.size() - 1;
-        
-        while (max > min)
-        {
-            int64_t mid = (min + max) / 2;
-            
-            if (events[mid].time == t)
-            {
-                min = mid;
-                max = mid;
-            }
-            else if (events[mid].time < t)
-            {
-                min = mid + 1;
-            }
-            else
-            {
-                max = mid - 1;
-            }
-        }
-
-        return is_lower ? min : max;
-    }
-
-    /** Find the first|last sample whose time succeeds|preceeds or equals t. */
-    vector<uint64_t>::size_type find(const vector<uint64_t>& samples,
-                                     const vector<uint64_t>::size_type& N,
-                                     const Time& t,
-                                     bool is_lower)
-    {
-        int64_t min = 0, max = (samples.size() - 1) / N;
-
-        while (max > min)
-        {
-            int64_t mid = (min + max) / 2;
-            
-            if (Time(samples[mid * N]) == t)
-            {
-                min = mid;
-                max = mid;
-            }
-            else if (Time(samples[mid * N]) < t)
-            {
-                min = mid + 1;
-            }
-            else
-            {
-                max = mid - 1;
-            }
-        }
-
-        return (is_lower ? min : max) * N;
-    }
-    
-} // namespace <anonymous>
-
-
-
-//------------------------------------------------------------------------------
-//------------------------------------------------------------------------------
-string CUDAData::stringify(CachePreferences value)
-{
-    switch (value)
-    {
-    case kInvalidCachePreference: return "InvalidCachePreference";
-    case kNoPreference: return "NoPreference";
-    case kPreferShared: return "PreferShared";
-    case kPreferCache: return "PreferCache";
-    case kPreferEqual: return "PreferEqual";
-    }
-    return "?";
-}
-
-
-    
-//------------------------------------------------------------------------------
-//------------------------------------------------------------------------------
-string CUDAData::stringify(CopyKinds value)
-{
-    switch (value)
-    {
-    case kInvalidCopyKind: return "InvalidCopyKind";
-    case kUnknownCopyKind: return "UnknownCopyKind";
-    case kHostToDevice: return "HostToDevice";
-    case kDeviceToHost: return "DeviceToHost";
-    case kHostToArray: return "HostToArray";
-    case kArrayToHost: return "ArrayToHost";
-    case kArrayToArray: return "ArrayToArray";
-    case kArrayToDevice: return "ArrayToDevice";
-    case kDeviceToArray: return "DeviceToArray";
-    case kDeviceToDevice: return "DeviceToDevice";
-    case kHostToHost: return "HostToHost";
-    }
-    return "?";
-}
-
-
-
-//------------------------------------------------------------------------------
-//------------------------------------------------------------------------------
-string CUDAData::stringify(MemoryKinds value)
-{
-    switch (value)
-    {
-    case kInvalidMemoryKind: return "InvalidMemoryKind";
-    case kUnknownMemoryKind: return "UnknownMemoryKind";
-    case kPageable: return "Pageable";
-    case kPinned: return "Pinned";
-    case kDevice: return "Device";
-    case kArray: return "Array";
-    }
-    return "?";
-}
-
-
-
-//------------------------------------------------------------------------------
-//------------------------------------------------------------------------------
-CUDAData::CUDAData(const Collector& collector, const Thread& thread) :
-    dm_call_sites(),
-    dm_counters(),
-    dm_devices(),
-    dm_kernel_executions(),
-    dm_known_devices(),
-    dm_known_contexts(),
-    dm_memory_copies(),
-    dm_memory_sets(),
-    dm_periodic_samples(),
-    dm_requests(),
-    dm_time_origin(Time::TheEnd())
-{
-    Assert(collector.getMetadata().getUniqueId() == "cuda");
-    Assert(collector.inSameDatabase(thread));
-
-    SmartPtr<Database> database = EntrySpy(collector).getDatabase();
-        
-    BEGIN_TRANSACTION(database);
-    database->prepareStatement(
-        "SELECT time_begin, data FROM Data WHERE collector = ? AND thread = ?;"
-        );
-    database->bindArgument(1, EntrySpy(collector).getEntry());
-    database->bindArgument(2, EntrySpy(thread).getEntry());
-    while (database->executeStatement())
-    {
-        CBTF_cuda_data data;
-        memset(&data, 0, sizeof(data));
-
-        Time t = database->getResultAsTime(1);
-        if (t < dm_time_origin)
-        {
-            dm_time_origin = t;
-        }
-        
-        Blob blob = database->getResultAsBlob(2);
-        blob.getXDRDecoding(
-            reinterpret_cast<xdrproc_t>(xdr_CBTF_cuda_data), &data
-            );
-
-        const CUDA_OverflowSamples* overflow_samples = NULL;
-        const CUDA_PeriodicSamples* periodic_samples = NULL;
-        
-        for (u_int i = 0; i < data.messages.messages_len; ++i)
-        {
-            const CBTF_cuda_message& message = data.messages.messages_val[i];
-
-            switch (message.type)
-            {
-
-            case ContextInfo:
-                process(message.CBTF_cuda_message_u.context_info);
-                break;
-
-            case CopiedMemory:
-                process(message.CBTF_cuda_message_u.copied_memory);
-                break;
-
-            case DeviceInfo:
-                process(message.CBTF_cuda_message_u.device_info);
-                break;
-                
-            case EnqueueRequest:
-                process(message.CBTF_cuda_message_u.enqueue_request,
-                        data, thread);
-                break;
-
-            case ExecutedKernel:
-                process(message.CBTF_cuda_message_u.executed_kernel);
-                break;
-
-            case OverflowSamples:
-                if (overflow_samples == NULL)
-                {
-                    overflow_samples = 
-                        &message.CBTF_cuda_message_u.overflow_samples;
-                }
-#ifndef NDEBUG
-                else
-                {
-                    cerr << "WARNING: CUDAData ignored an extra "
-                         << "CUDA_OverflowSamples message within a "
-                         << "performance data blob."
-                         << endl;
-                }
-#endif
-                break;
-
-            case PeriodicSamples:
-                if (periodic_samples == NULL)
-                {
-                    periodic_samples =
-                        &message.CBTF_cuda_message_u.periodic_samples;
-                }
-#ifndef NDEBUG
-                else
-                {
-                    cerr << "WARNING: CUDAData ignored an extra "
-                         << "CUDA_PeriodicSamples message within a "
-                         << "performance data blob."
-                         << endl;
-                }
-#endif
-                break;
-
-            case SamplingConfig:
-                process(message.CBTF_cuda_message_u.sampling_config);
-                break;
-                
-            case SetMemory:
-                process(message.CBTF_cuda_message_u.set_memory);
-                break;
-                
-            }
-        }
-
-        if (overflow_samples)
-        {
-            process(*overflow_samples);
-        }
-
-        if (periodic_samples)
-        {
-            process(*periodic_samples);
-        }
-        
-        xdr_free(reinterpret_cast<xdrproc_t>(xdr_CBTF_cuda_data),
-                 reinterpret_cast<char*>(&data));        
-    }
-    END_TRANSACTION(database);
-}
-
-
-
-//------------------------------------------------------------------------------
-//------------------------------------------------------------------------------
-CUDAData::~CUDAData()
-{
-}
-
-
-
-//------------------------------------------------------------------------------
-//------------------------------------------------------------------------------
-vector<uint64_t> CUDAData::get_counts(const TimeInterval& interval) const
-{
-    vector<uint64_t> counts(dm_counters.size(), 0);
-    
-    if (dm_periodic_samples.empty())
-    {
-        return counts;
-    }
-    
-    vector<uint64_t>::size_type N = 1 + dm_counters.size();
-    
-    vector<uint64_t>::size_type i_min = 
-        find(dm_periodic_samples, N, interval.getBegin(), true);
-    vector<uint64_t>::size_type i_max = 
-        find(dm_periodic_samples, N, interval.getEnd(), false);
-    
-    if (i_min >= i_max)
-    {
-        return counts;
-    }
-    
-    TimeInterval sample_interval(
-        Time(dm_periodic_samples[i_min]), Time(dm_periodic_samples[i_max])
-        );
-    
-    uint64_t interval_width = (interval & sample_interval).getWidth();
-    uint64_t sample_width = sample_interval.getWidth();
-    
-    for (vector<uint64_t>::size_type c = 1; c < N; ++c)
-    {
-        uint64_t count_delta = 
-            dm_periodic_samples[i_max + c] - dm_periodic_samples[i_min + c];
-        
-        counts[c - 1] = count_delta * interval_width / sample_width;
-    }
-    
-    return counts;
-}
-
-
-
-//------------------------------------------------------------------------------
-//------------------------------------------------------------------------------
-void CUDAData::visit_kernel_executions(
-    function<void (const KernelExecutionDetails&)>& visitor,
-    const TimeInterval& interval
-    ) const
-{
-    if (dm_kernel_executions.empty())
-    {
-        return;
-    }
-
-    vector<KernelExecutionDetails>::size_type i_min = 
-        find(dm_kernel_executions, interval.getBegin(), true);
-    vector<KernelExecutionDetails>::size_type i_max = 
-        find(dm_kernel_executions, interval.getEnd(), false);
-    
-    for (vector<KernelExecutionDetails>::size_type i = i_min; i <= i_max; ++i)
-    {
-        const KernelExecutionDetails& details = dm_kernel_executions[i];
-        
-        if (TimeInterval(details.time,
-                         details.time_end).doesIntersect(interval))
-        {
-            visitor(details);
-        }
-    }
-}
-
-
-
-//------------------------------------------------------------------------------
-//------------------------------------------------------------------------------
-void CUDAData::visit_memory_copies(
-    function<void (const MemoryCopyDetails&)>& visitor,
-    const TimeInterval& interval
-    ) const
-{
-    if (dm_memory_copies.empty())
-    {
-        return;
-    }
-
-    vector<MemoryCopyDetails>::size_type i_min = 
-        find(dm_memory_copies, interval.getBegin(), true);
-    vector<MemoryCopyDetails>::size_type i_max = 
-        find(dm_memory_copies, interval.getEnd(), false);
-    
-    for (vector<MemoryCopyDetails>::size_type i = i_min; i <= i_max; ++i)
-    {
-        const MemoryCopyDetails& details = dm_memory_copies[i];
-        
-        if (TimeInterval(details.time,
-                         details.time_end).doesIntersect(interval))
-        {
-            visitor(details);
-        }
-    }
-}
-
-
-
-//------------------------------------------------------------------------------
-//------------------------------------------------------------------------------
-void CUDAData::visit_memory_sets(
-    function<void (const MemorySetDetails&)>& visitor,
-    const TimeInterval& interval
-    ) const
-{
-    if (dm_memory_sets.empty())
-    {
-        return;
-    }
-
-    vector<MemorySetDetails>::size_type i_min = 
-        find(dm_memory_sets, interval.getBegin(), true);
-    vector<MemorySetDetails>::size_type i_max = 
-        find(dm_memory_sets, interval.getEnd(), false);
-    
-    for (vector<MemorySetDetails>::size_type i = i_min; i <= i_max; ++i)
-    {
-        const MemorySetDetails& details = dm_memory_sets[i];
-        
-        if (TimeInterval(details.time,
-                         details.time_end).doesIntersect(interval))
-        {
-            visitor(details);
-        }
-    }
-}
-
-
-
-//------------------------------------------------------------------------------
-//------------------------------------------------------------------------------
-void CUDAData::visit_periodic_samples(
-    function<void (const Time&, const vector<uint64_t>&)>& visitor,
-    const TimeInterval& interval
-    ) const
-{
-    if (dm_periodic_samples.empty())
-    {
-        return;
-    }
-    
-    vector<uint64_t>::size_type N = 1 + dm_counters.size();
-    
-    vector<uint64_t>::size_type i_min = 
-        find(dm_periodic_samples, N, interval.getBegin(), true);
-    vector<uint64_t>::size_type i_max = 
-        find(dm_periodic_samples, N, interval.getEnd(), false);
-    
-    vector<uint64_t> counts(dm_counters.size(), 0);
-    
-    for (vector<uint64_t>::size_type i = i_min; i <= i_max; i += N)
-    {
-        Time t(dm_periodic_samples[i]);
-        
-        if (interval.doesContain(t))
-        {   
-            for (vector<uint64_t>::size_type c = 1; c < N; ++c)
-            {
-                counts[c - 1] = dm_periodic_samples[i + c];
-            }
-            
-            visitor(t, counts);
-        }
-    }
-}
-
-
-
-//------------------------------------------------------------------------------
-//------------------------------------------------------------------------------
-vector<CUDAData::DeviceDetails>::size_type CUDAData::device_from_context(
-    const Address& context
-    ) const
-{
-    map<Address, uint32_t>::const_iterator i = dm_known_contexts.find(context);
-    
-    if (i == dm_known_contexts.end())
-    {
-#ifndef NDEBUG
-        cerr << "WARNING: CUDAData encountered an unknown CUDA "
-             << "context (" << context << ")." << endl;
-#endif
-        
-        return 0;
-    }
-    
-    map<uint32_t, vector<DeviceDetails>::size_type>::const_iterator j =
-        dm_known_devices.find(i->second);
-
-    if (j == dm_known_devices.end())
-    {
-#ifndef NDEBUG
-        cerr << "WARNING: CUDAData encountered an unknown CUDA "
-             << "device (" << i->second << ")." << endl;
-#endif
-        
-        return 0;
-    }
-
-    return j->second;
-}
-
-
-
-//------------------------------------------------------------------------------
-//------------------------------------------------------------------------------
-void CUDAData::process(const CUDA_ContextInfo& message)
-{
-    if (dm_known_contexts.find(message.context) != dm_known_contexts.end())
-    {
-        return;
-    }
-    
-    dm_known_contexts.insert(make_pair(message.context, message.device));
-}
-
-
-
-//------------------------------------------------------------------------------
-//------------------------------------------------------------------------------
-void CUDAData::process(const CUDA_CopiedMemory& message)
-{
-    list<Request>::iterator i = dm_requests.begin();
-    for (; i != dm_requests.end(); ++i)
-    {
-        if ((i->message->type == MemoryCopy) &&
-            (i->message->context == message.context) &&
-            (i->message->stream == message.stream))
-        {
-            break;
-        }
-    }
-    
-    if (i == dm_requests.end())
-    {
-#ifndef NDEBUG
-        cerr << "WARNING: CUDAData encountered an unmatched "
-             << "CUDA_CopiedMemory request." << endl;
-#endif
-        return;
-    }
-
-    MemoryCopyDetails details;
-    details.device = device_from_context(i->message->context);
-    details.call_site = i->call_site;
-    details.time = i->message->time;
-    details.time_begin = message.time_begin;
-    details.time_end = message.time_end;
-    details.size = message.size;
-    details.kind = convert(message.kind);
-    details.source_kind = convert(message.source_kind);
-    details.destination_kind = convert(message.destination_kind);
-    details.asynchronous = message.asynchronous;
-
-    dm_memory_copies.push_back(details);
-    dm_requests.erase(i);
-}
-
-
-
-//------------------------------------------------------------------------------
-//------------------------------------------------------------------------------
-void CUDAData::process(const CUDA_DeviceInfo& message)
-{
-    if (dm_known_devices.find(message.device) != dm_known_devices.end())
-    {
-#ifndef NDEBUG
-        cerr << "WARNING: CUDAData encountered a duplicate "
-             << "CUDA_DeviceInfo (device " << message.device
-             << " )." << endl;
-#endif
-        return;
-    }
-    
-    DeviceDetails details;
-    details.name = message.name;
-    details.compute_capability = Vector2u(message.compute_capability[0],
-                                          message.compute_capability[1]);
-    details.max_grid = Vector3u(message.max_grid[0],
-                                message.max_grid[1],
-                                message.max_grid[2]);
-    details.max_block = Vector3u(message.max_block[0],
-                                 message.max_block[1],
-                                 message.max_block[2]);
-    details.global_memory_bandwidth = message.global_memory_bandwidth;
-    details.global_memory_size = message.global_memory_size;
-    details.constant_memory_size = message.constant_memory_size;
-    details.l2_cache_size = message.l2_cache_size;
-    details.threads_per_warp = message.threads_per_warp;
-    details.core_clock_rate = message.core_clock_rate;
-    details.memcpy_engines = message.memcpy_engines;
-    details.multiprocessors = message.multiprocessors;
-    details.max_ipc = message.max_ipc;
-    details.max_warps_per_multiprocessor = message.max_warps_per_multiprocessor;
-    details.max_blocks_per_multiprocessor = 
-        message.max_blocks_per_multiprocessor;
-    details.max_registers_per_block = message.max_registers_per_block;
-    details.max_shared_memory_per_block = message.max_shared_memory_per_block;
-    details.max_threads_per_block = message.max_threads_per_block;
-    
-    dm_devices.push_back(details);
-    dm_known_devices.insert(make_pair(message.device, dm_devices.size() - 1));
-}
-
-
-
-//------------------------------------------------------------------------------
-//------------------------------------------------------------------------------
-void CUDAData::process(const CUDA_EnqueueRequest& message,
-                       const CBTF_cuda_data& data, const Thread& thread)
-{
-    // Initially include ALL frames for the call site of this CUDA request
-    StackTrace trace(thread, message.time);
-    for (uint32_t i = message.call_site;
-         (i < data.stack_traces.stack_traces_len) &&
-             (data.stack_traces.stack_traces_val[i] != 0);
-         ++i)
-    {
-        trace.push_back(data.stack_traces.stack_traces_val[i]);
-    }
-    
-    //
-    // Trim all of the frames that preceeded the actual entry into main(),
-    // assuming, of course, that main() can actually be found.
-    //
-    
-    for (int i = 0; i < trace.size(); ++i)
-    {
-        pair<bool, Function> function = trace.getFunctionAt(i);
-        if (function.first && (function.second.getName() == "main"))
-        {
-            trace.erase(trace.begin() + i + 1, trace.end());
-            break;
-        }            
-    }
-
-    //
-    // Now iterate over the frames from main() towards the final call site,
-    // looking for the first frame that is part of the CUDA implementation
-    // or our collector. Trim the stack trace from that point all the way
-    // to the final call site.
-    //
-    
-    for (int i = trace.size(); i > 0; --i)
-    {
-        pair<bool, LinkedObject> linked_object = trace.getLinkedObjectAt(i);
-        if (linked_object.first)
-        {
-            Path base_name = linked_object.second.getPath().getBaseName();
-            if (starts_with(base_name, "cuda-collector") ||
-                starts_with(base_name, "libcu"))
-            {
-                trace.erase(trace.begin(), trace.begin() + i + 1);
-                break;
-            }
-        }
-        
-        pair<bool, Function> function = trace.getFunctionAt(i);
-        if (function.first &&
-            starts_with(function.second.getName(), "__device_stub"))
-        {
-            trace.erase(trace.begin(), trace.begin() + i + 1);
-            break;
-        }
-    }
-
-    //
-    // Search the existing call sites for this stack trace. The existing call
-    // site is reused if one is found. Otherwise this stack trace is added to
-    // the call sites.
-    //
-    // Note that a linear search is currently being employed here because the
-    // number of unique CUDA call sites is expected to be low. If that ends up
-    // not being the case, and performance is inadequate, a hash of each stack
-    // trace could be computed and used to accelerate this search.
-    //
-
-    vector<StackTrace>::size_type call_site;
-    for (call_site = 0; call_site < dm_call_sites.size(); ++call_site)
-    {
-        if (dm_call_sites[call_site] == trace)
-        {
-            break;
-        }
-    }
-
-    if (call_site == dm_call_sites.size())
-    {
-        dm_call_sites.push_back(trace);
-    }
-
-    // Add this request to the list of pending requests
-
-    Request request;
-    request.message = shared_ptr<CUDA_EnqueueRequest>(
-        new CUDA_EnqueueRequest(message)
-        );
-    request.call_site = call_site;
-    dm_requests.push_back(request);
-}
-
-
-
-//------------------------------------------------------------------------------
-//------------------------------------------------------------------------------
-void CUDAData::process(const CUDA_ExecutedKernel& message)
-{
-    list<Request>::iterator i = dm_requests.begin();
-    for (; i != dm_requests.end(); ++i)
-    {
-        if ((i->message->type == LaunchKernel) &&
-            (i->message->context == message.context) &&
-            (i->message->stream == message.stream))
-        {
-            break;
-        }
-    }
-    
-    if (i == dm_requests.end())
-    {
-#ifndef NDEBUG
-        cerr << "WARNING: CUDAData encountered an unmatched "
-             << "CUDA_ExecutedKernel request." << endl;
-#endif
-        return;
-    }
-
-    KernelExecutionDetails details;
-    details.device = device_from_context(i->message->context);
-    details.call_site = i->call_site;
-    details.time = i->message->time;
-    details.time_begin = message.time_begin;
-    details.time_end = message.time_end;
-    details.function = message.function;
-    details.grid = Vector3u(message.grid[0], message.grid[1], message.grid[2]);
-    details.block = 
-        Vector3u(message.block[0], message.block[1], message.block[2]);
-    details.cache_preference = convert(message.cache_preference);
-    details.registers_per_thread = message.registers_per_thread;
-    details.static_shared_memory = message.static_shared_memory;
-    details.dynamic_shared_memory = message.dynamic_shared_memory;
-    details.local_memory = message.local_memory;
-
-    dm_kernel_executions.push_back(details);
-    dm_requests.erase(i);
-}
-
-
-
-//------------------------------------------------------------------------------
-//------------------------------------------------------------------------------
-void CUDAData::process(const CUDA_OverflowSamples& message)
-{
-    // TODO: Eventually we probably should make overflow samples available too.
-
-#ifndef NDEBUG
-    cerr << "WARNING: CUDAData ignored a CUDA_OverflowSamples message." << endl;
-#endif
-}
-
-
-
-//------------------------------------------------------------------------------
-//------------------------------------------------------------------------------
-void CUDAData::process(const CUDA_PeriodicSamples& message)
-{
-    static int kAdditionalBytes[4] = { 0, 2, 3, 8 };
-
-    if (dm_counters.empty())
-    {
-#ifndef NDEBUG
-        cerr << "WARNING: CUDAData ignored a CUDA_PeriodicSamples message."
-             << endl;
-#endif
-        return;
-    }
-
-    vector<uint64_t>::size_type n = 0, N = 1 + dm_counters.size();
-    vector<uint64_t> samples(N, 0);
-
-    for (const uint8_t* ptr = &message.deltas.deltas_val[0];
-         ptr != &message.deltas.deltas_val[message.deltas.deltas_len];)
-    {
-        uint8_t encoding = *ptr >> 6;
-
-        uint64_t delta = 0;
-        if (encoding < 3)
-        {
-            delta = static_cast<uint64_t>(*ptr) & 0x3F;
-        }
-        ++ptr;
-        for (int i = 0; i < kAdditionalBytes[encoding]; ++i)
-        {
-            delta <<= 8;
-            delta |= static_cast<uint64_t>(*ptr++);
-        }
-
-        samples[n++] += delta;
-
-        if (n == N)
-        {
-            dm_periodic_samples.insert(
-                dm_periodic_samples.end(), samples.begin(), samples.end()
-                );
-            n = 0;
-        }
-    }
-}
-
-
-
-//------------------------------------------------------------------------------
-//------------------------------------------------------------------------------
-void CUDAData::process(const CUDA_SamplingConfig& message)
-{
-    if (dm_counters.empty())
-    {
-        for (u_int i = 0; i < message.events.events_len; ++i)
-        {
-            dm_counters.push_back(message.events.events_val[i].name);
-        }
-    }
-}
-
-
-
-//------------------------------------------------------------------------------
-//------------------------------------------------------------------------------
-void CUDAData::process(const CUDA_SetMemory& message)
-{
-    list<Request>::iterator i = dm_requests.begin();
-    for (; i != dm_requests.end(); ++i)
-    {
-        if ((i->message->type == MemorySet) &&
-            (i->message->context == message.context) &&
-            (i->message->stream == message.stream))
-        {
-            break;
-        }
-    }
-    
-    if (i == dm_requests.end())
-    {
-#ifndef NDEBUG
-        cerr << "WARNING: CUDAData encountered an unmatched "
-             << "CUDA_SetMemory request." << endl;
-#endif
-        return;
-    }
-
-    MemorySetDetails details;
-    details.device = device_from_context(i->message->context);
-    details.call_site = i->call_site;
-    details.time = i->message->time;
-    details.time_begin = message.time_begin;
-    details.time_end = message.time_end;
-    details.size = message.size;
-
-    dm_memory_sets.push_back(details);
-    dm_requests.erase(i);
-}
diff -Naur OpenSpeedShop.ORIGINAL/libopenss-queries-cuda/CUDAData.hxx OpenSpeedShop.NEW-CUPTI/libopenss-queries-cuda/CUDAData.hxx
--- OpenSpeedShop.ORIGINAL/libopenss-queries-cuda/CUDAData.hxx	2014-12-17 23:39:49.000000000 -0600
+++ OpenSpeedShop.NEW-CUPTI/libopenss-queries-cuda/CUDAData.hxx	1969-12-31 18:00:00.000000000 -0600
@@ -1,473 +0,0 @@
-////////////////////////////////////////////////////////////////////////////////
-// Copyright (c) 2014 Argo Navis Technologies. All Rights Reserved.
-//
-// This library is free software; you can redistribute it and/or modify it under
-// the terms of the GNU Lesser General Public License as published by the Free
-// Software Foundation; either version 2.1 of the License, or (at your option)
-// any later version.
-//
-// This library is distributed in the hope that it will be useful, but WITHOUT
-// ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
-// FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more
-// details.
-//
-// You should have received a copy of the GNU Lesser General Public License
-// along with this library; if not, write to the Free Software Foundation, Inc.,
-// 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
-////////////////////////////////////////////////////////////////////////////////
-
-/** @file Declaration of the CUDAData class. */
-
-#pragma once
-
-#ifdef HAVE_CONFIG_H
-#include "config.h"
-#endif
-
-#include <boost/function.hpp>
-#include <boost/noncopyable.hpp>
-#include <boost/shared_ptr.hpp>
-#include <boost/tuple/tuple.hpp>
-#include <list>
-#include <map>
-#include <stdint.h>
-#include <string>
-#include <vector>
-
-#include "Address.hxx"
-#include "Collector.hxx"
-#include "StackTrace.hxx"
-#include "Thread.hxx"
-#include "Time.hxx"
-#include "TimeInterval.hxx"
-
-namespace OpenSpeedShop { namespace Queries {
-        
-    /**
-     * High-level interface for accessing one thread's CUDA performance data.
-     */
-    class CUDAData :
-        boost::noncopyable
-    {
-
-    public:
-
-        /** Vector of two unsigned integers. */
-        typedef boost::tuple<uint32_t, uint32_t> Vector2u;
-        
-        /** Vector of three unsigned integers. */
-        typedef boost::tuple<uint32_t, uint32_t, uint32_t> Vector3u;
-
-        /** Enumeration of the different cache preferences. */
-        enum CachePreferences
-        {
-            kInvalidCachePreference, kNoPreference,
-            kPreferShared, kPreferCache, kPreferEqual
-        };
-
-        /** Enumeration of the different memory copy kinds. */
-        enum CopyKinds
-        {
-            kInvalidCopyKind, kUnknownCopyKind,
-            kHostToDevice, kDeviceToHost, kHostToArray,
-            kArrayToHost, kArrayToArray, kArrayToDevice,
-            kDeviceToArray, kDeviceToDevice, kHostToHost
-        };
-     
-        /** Enumeration of the different memory kinds. */
-        enum MemoryKinds
-        {
-            kInvalidMemoryKind, kUnknownMemoryKind,
-            kPageable, kPinned, kDevice, kArray
-        };
-
-        /** Stringify the specified CachePreferences value. */
-        static std::string stringify(CachePreferences value);
-            
-        /** Stringify the specified CopyKinds value. */
-        static std::string stringify(CopyKinds value);
-
-        /** Stringify the specified MemoryKinds value. */
-        static std::string stringify(MemoryKinds value);
-        
-        /** Detailed information for a CUDA device. */
-        struct DeviceDetails
-        {
-            /** Name of this device. */
-            std::string name;
-            
-            /** Compute capability (major/minor) of this device. */
-            Vector2u compute_capability;
-            
-            /** Maximum allowed dimensions of grids with this device. */
-            Vector3u max_grid;
-            
-            /** Maximum allowed dimensions of blocks with this device. */
-            Vector3u max_block;
-            
-            /** Global memory bandwidth of this device (in KBytes/sec). */
-            uint64_t global_memory_bandwidth;
-            
-            /** Global memory size of this device (in bytes). */
-            uint64_t global_memory_size;
-            
-            /** Constant memory size of this device (in bytes). */
-            uint32_t constant_memory_size;
-            
-            /** L2 cache size of this device (in bytes). */
-            uint32_t l2_cache_size;
-
-            /** Number of threads per warp for this device. */
-            uint32_t threads_per_warp;
-            
-            /** Core clock rate of this device (in KHz). */
-            uint32_t core_clock_rate;
-            
-            /** Number of memory copy engines on this device. */
-            uint32_t memcpy_engines;
-            
-            /** Number of multiprocessors on this device. */
-            uint32_t multiprocessors;
-            
-            /** 
-             * Maximum instructions/cycle possible on this device's
-             * multiprocessors.
-             */
-            uint32_t max_ipc;
-            
-            /** Maximum warps/multiprocessor for this device. */
-            uint32_t max_warps_per_multiprocessor;
-            
-            /** Maximum blocks/multiprocessor for this device. */
-            uint32_t max_blocks_per_multiprocessor;
-            
-            /** Maximum registers/block for this device. */
-            uint32_t max_registers_per_block;
-            
-            /** Maximium shared memory / block for this device. */
-            uint32_t max_shared_memory_per_block;
-            
-            /** Maximum threads/block for this device. */
-            uint32_t max_threads_per_block;
-        };
-
-        /** Detailed information for a CUDA kernel execution. */
-        struct KernelExecutionDetails
-        {
-            /** Device on which the kernel execution occurred. */
-            size_t device;
-            
-            /** Call site of the kernel execution. */
-            size_t call_site;
-            
-            /** Time at which the kernel execution was requested. */
-            Framework::Time time;
-
-            /** Time at which the kernel execution began. */
-            Framework::Time time_begin;
-            
-            /** Time at which the kernel execution ended. */
-            Framework::Time time_end;
-
-            /** Name of the kernel function being executed. */
-            std::string function;
-
-            /** Dimensions of the grid. */
-            Vector3u grid;
-    
-            /** Dimensions of each block. */
-            Vector3u block;
-
-            /** Cache preference used. */
-            CachePreferences cache_preference;
-
-            /** Registers required for each thread. */
-            uint32_t registers_per_thread;
-
-            /** Total amount (in bytes) of static shared memory reserved. */
-            uint32_t static_shared_memory;
-
-            /** Total amount (in bytes) of dynamic shared memory reserved. */
-            uint32_t dynamic_shared_memory;
-
-            /** Total amount (in bytes) of local memory reserved. */
-            uint32_t local_memory;
-        };
-
-        /** Detailed information for a CUDA memory copy. */
-        struct MemoryCopyDetails
-        {
-            /** Device on which the memory copy occurred. */
-            size_t device;
-            
-            /** Call site of the memory copy. */
-            size_t call_site;
-            
-            /** Time at which the memory copy was requested. */
-            Framework::Time time;
-            
-            /** Time at which the memory copy began. */
-            Framework::Time time_begin;
-            
-            /** Time at which the memory copy ended. */
-            Framework::Time time_end;
-            
-            /** Number of bytes being copied. */
-            uint64_t size;
-
-            /** Kind of copy performed. */
-            CopyKinds kind;
-            
-            /** Kind of memory from which the copy was performed. */
-            MemoryKinds source_kind;
-            
-            /** Kind of memory to which the copy was performed. */
-            MemoryKinds destination_kind;
-            
-            /** Was the copy asynchronous? */
-            bool asynchronous;  
-        };
-
-        /** Detailed information for a CUDA memory set. */
-        struct MemorySetDetails
-        {
-            /** Device on which the memory set occurred. */
-            size_t device;
-            
-            /** Call site of the memory set. */
-            size_t call_site;
-            
-            /** Time at which the memory set was requested. */
-            Framework::Time time;
-            
-            /** Time at which the memory set began. */
-            Framework::Time time_begin;
-            
-            /** Time at which the memory set ended. */
-            Framework::Time time_end;
-            
-            /** Number of bytes being set. */
-            uint64_t size;            
-        };
-        
-        /**
-         * Preprocess the data for the given collector and thread.
-         *
-         * @pre    Can only be performed for a CUDA collector. An assertion
-         *         failure occurs if a different collector is used.
-         *
-         * @pre    The thread must be in the same experiment as the collector.
-         *         An assertion failure occurs if the thread is in a different
-         *         experiment than the collector.
-         *
-         * @param collector    Collector for which to access performance data.
-         * @parma thread       Thread for which to access performance data.
-         */
-        CUDAData(const Framework::Collector& collector,
-                 const Framework::Thread& thread);
-        
-        /** Destructor. */
-        virtual ~CUDAData();
-        
-        /** Call sites for all known CUDA requests. */       
-        const std::vector<Framework::StackTrace>& call_sites() const
-        {
-            return dm_call_sites;
-        }
-        
-        /** Names of all sampled hardware performance counters. */
-        const std::vector<std::string>& counters() const
-        {
-            return dm_counters;
-        }
-        
-        /** Detailed information for all known CUDA devices. */
-        const std::vector<DeviceDetails>& devices() const
-        {
-            return dm_devices;
-        }
-
-        /** Earliest observed time. */
-        const Framework::Time& time_origin() const
-        {
-            return dm_time_origin;
-        }
-        
-        /**
-         * Get the counts for all sampled hardware peformance counters between
-         * the specified time interval (if any).
-         *
-         * @note    The name of the counter corresponding to any particular
-         *          count in the returned vector can be found by using that
-         *          count's index within the vector returned by counters().
-         *
-         * @note    When the specified time interval does not lie exactly on
-         *          the hardware performance counter sample times, the counts
-         *          provided are estimates based on the nearest samples.
-         *
-         * @param interval    Time interval over which to get counts. Has
-         *                    a default value meaning "all possible time".
-         * @return            Counts over the specified time interval.
-         */
-        std::vector<uint64_t> get_counts(
-            const Framework::TimeInterval& interval = Framework::TimeInterval(
-                Framework::Time::TheBeginning(),
-                Framework::Time::TheEnd()
-                )
-            ) const;
-            
-        /**
-         * Visit those CUDA kernel executions whose request-to-completion time
-         * interval intersects the specified time interval (if any).
-         *
-         * @param visitor     Visitor invoked for each kernel execution.
-         * @param interval    Time interval for the visitation. Has a
-         *                    default value meaning "all possible time".
-         */
-        void visit_kernel_executions(
-            boost::function<void (const KernelExecutionDetails&)>& visitor,
-            const Framework::TimeInterval& interval = Framework::TimeInterval(
-                Framework::Time::TheBeginning(),
-                Framework::Time::TheEnd()
-                )
-            ) const;
-        
-        /**
-         * Visit those CUDA memory copies whose request-to-completion time
-         * interval intersects the specified time interval (if any).
-         *
-         * @param visitor     Visitor invoked for each memory copy.
-         * @param interval    Time interval for the visitation. Has a
-         *                    default value meaning "all possible time".
-         */
-        void visit_memory_copies(
-            boost::function<void (const MemoryCopyDetails&)>& visitor,
-            const Framework::TimeInterval& interval = Framework::TimeInterval(
-                Framework::Time::TheBeginning(),
-                Framework::Time::TheEnd()
-                )
-            ) const;
-
-        /**
-         * Visit those CUDA memory sets whose request-to-completion time
-         * interval intersects the specified time interval (if any).
-         *
-         * @param visitor     Visitor invoked for each memory set.
-         * @param interval    Time interval for the visitation. Has a
-         *                    default value meaning "all possible time".
-         */
-        void visit_memory_sets(
-            boost::function<void (const MemorySetDetails&)>& visitor,
-            const Framework::TimeInterval& interval = Framework::TimeInterval(
-                Framework::Time::TheBeginning(),
-                Framework::Time::TheEnd()
-                )
-            ) const;
-
-        /**
-         * Visit those hardware performance counter periodic samples whose
-         * sample time is within the specified time interval (if any).
-         *
-         * @note    The name of the counter corresponding to any particular
-         *          count in the vector passed into the visitor can be found
-         *          by using that count's index within the vector returned by
-         *          counters().
-         *
-         * @param visitor     Visitor invoked for each periodic sample.
-         * @param interval    Time interval for the visitation. Has a
-         *                    default value meaning "all possible time".
-         */
-        void visit_periodic_samples(
-            boost::function<
-                void (const Framework::Time&, const std::vector<uint64_t>&)
-                >& visitor,
-            const Framework::TimeInterval& interval = Framework::TimeInterval(
-                Framework::Time::TheBeginning(),
-                Framework::Time::TheEnd()
-                )
-            ) const;
-
-    private:
-
-        /** Description of a single pending request. */
-        struct Request
-        {
-            /** Original CUDA message describing the enqueued request. */
-            boost::shared_ptr<struct CUDA_EnqueueRequest> message;
-            
-            /** Call site of the enqueued request. */
-            std::vector<Framework::StackTrace>::size_type call_site;
-        };
-
-        /** Call sites for all known CUDA requests. */
-        std::vector<Framework::StackTrace> dm_call_sites;
-        
-        /** Names of all sampled hardware performance counters. */
-        std::vector<std::string> dm_counters;
-
-        /** Detailed information for all known CUDA devices. */
-        std::vector<DeviceDetails> dm_devices;
-
-        /** Ordered list of CUDA kernel executions. */
-        std::vector<KernelExecutionDetails> dm_kernel_executions;
-
-        /** Index in dm_devices for each known device ID. */
-        std::map<
-            uint32_t, std::vector<DeviceDetails>::size_type
-            > dm_known_devices;
-        
-        /** Device ID for each known context address. */
-        std::map<Framework::Address, uint32_t> dm_known_contexts;
-
-        /** Ordered list of CUDA memory copies. */
-        std::vector<MemoryCopyDetails> dm_memory_copies;
-
-        /** Ordered list of CUDA memory sets. */
-        std::vector<MemorySetDetails> dm_memory_sets;
-
-        /** Ordered list of periodic samples. */
-        std::vector<uint64_t> dm_periodic_samples;
-        
-        /** Pending requests. */
-        std::list<Request> dm_requests;
-
-        /** Earliest observed time. */
-        Framework::Time dm_time_origin;
-        
-        /** Get the index in dm_devices for the specified context address. */
-        std::vector<DeviceDetails>::size_type device_from_context(
-            const Framework::Address& context
-            ) const;
-        
-        /** Process a CUDA_ContextInfo message. */
-        void process(const struct CUDA_ContextInfo& message);
-
-        /** Process a CUDA_CopiedMemory message. */
-        void process(const struct CUDA_CopiedMemory& message);
-
-        /** Process a CUDA_DeviceInfo message. */
-        void process(const struct CUDA_DeviceInfo& message);
-        
-        /** Process a CUDA_EnqueueRequest message. */
-        void process(const struct CUDA_EnqueueRequest& message,
-                     const struct CBTF_cuda_data& data,
-                     const Framework::Thread& thread);
-        
-        /** Process a CUDA_ExecutedKernel message. */
-        void process(const struct CUDA_ExecutedKernel& message);
-
-        /** Process a CUDA_OverflowSamples message. */
-        void process(const struct CUDA_OverflowSamples& message);
-        
-        /** Process a CUDA_PeriodicSamples message. */
-        void process(const struct CUDA_PeriodicSamples& message);
-        
-        /** Process a CUDA_SamplingConfig message. */
-        void process(const struct CUDA_SamplingConfig& message);
-
-        /** Process a CUDA_SetMemory message. */
-        void process(const struct CUDA_SetMemory& message);
-
-    }; // class CUDAData
-
-} } // namespace OpenSpeedShop::Queries
diff -Naur OpenSpeedShop.ORIGINAL/libopenss-queries-cuda/CUDAQueries.cxx OpenSpeedShop.NEW-CUPTI/libopenss-queries-cuda/CUDAQueries.cxx
--- OpenSpeedShop.ORIGINAL/libopenss-queries-cuda/CUDAQueries.cxx	2014-11-20 00:29:06.000000000 -0600
+++ OpenSpeedShop.NEW-CUPTI/libopenss-queries-cuda/CUDAQueries.cxx	2016-02-26 16:13:03.088180556 -0600
@@ -1,5 +1,5 @@
 ////////////////////////////////////////////////////////////////////////////////
-// Copyright (c) 2014 Argo Navis Technologies. All Rights Reserved.
+// Copyright (c) 2014-2016 Argo Navis Technologies. All Rights Reserved.
 //
 // This library is free software; you can redistribute it and/or modify it under
 // the terms of the GNU Lesser General Public License as published by the Free
@@ -18,14 +18,34 @@
 
 /** @file Definition of CUDA extensions to the Queries namespace. */
 
+#include <boost/algorithm/string/predicate.hpp>
 #include <boost/bind.hpp>
-#include <boost/function.hpp>
+#include <boost/cstdint.hpp>
 #include <boost/ref.hpp>
 #include <cstring>
 
+#include <ArgoNavis/Base/AddressVisitor.hpp>
+#include <ArgoNavis/Base/ThreadVisitor.hpp>
+
+#include <ArgoNavis/CUDA/DataTransfer.hpp>
+#include <ArgoNavis/CUDA/DataTransferVisitor.hpp>
+#include <ArgoNavis/CUDA/KernelExecution.hpp>
+#include <ArgoNavis/CUDA/KernelExecutionVisitor.hpp>
+#include <ArgoNavis/CUDA/stringify.hpp>
+
+#include "AddressRange.hxx"
+#include "Assert.hxx"
+#include "Blob.hxx"
+#include "Database.hxx"
+#include "EntrySpy.hxx"
+#include "Function.hxx"
+#include "LinkedObject.hxx"
+#include "Path.hxx"
+
 #include "CUDAQueries.hxx"
 
 using namespace boost;
+using namespace ArgoNavis;
 using namespace OpenSpeedShop;
 using namespace OpenSpeedShop::Framework;
 using namespace OpenSpeedShop::Queries;
@@ -38,29 +58,79 @@
 
     /** Visitor used to accumulate the total event time. */
     template <typename T>
-    void accumulateEventTime(const T& details,
+    bool accumulateEventTime(const T& details,
                              const TimeInterval& query_interval,
                              double& time)
     {
-        TimeInterval event_interval(details.time_begin, details.time_end);
+        TimeInterval event_interval(
+            Time(details.time_begin), Time(details.time_end)
+            );
 
         time += static_cast<double>(
             (query_interval & event_interval).getWidth()
             ) / 1000000000.0 /* ns/s */;
+
+        return true; // Always continue the visitation
     }
 
     /** Visitor used to accumulate the total data transfer size and time. */
-    void accumulateXferSizeAndTime(const CUDAData::MemoryCopyDetails& details,
+    bool accumulateXferSizeAndTime(const CUDA::DataTransfer& details,
                                    const TimeInterval& query_interval,
                                    uint64_t& size, double& time)
     {
         size += details.size;
 
-        TimeInterval event_interval(details.time_begin, details.time_end);
+        TimeInterval event_interval(
+            Time(details.time_begin), Time(details.time_end)
+            );
         
         time += static_cast<double>(
             (query_interval & event_interval).getWidth()
             ) / 1000000000.0 /* ns/s */;
+
+        return true; // Always continue the visitation
+    }
+
+    /** Visitor used to call another visitor. */
+    bool callExec(const Base::ThreadName& thread,
+                  const CUDA::PerformanceData& data,
+                  const TimeInterval& query_interval,
+                  CUDA::KernelExecutionVisitor& visitor)
+    {
+        data.visitKernelExecutions(
+            thread, ConvertToArgoNavis(query_interval), visitor
+            );
+        
+        return true; // Always continue the visitation
+    }
+
+    /** Visitor used to call another visitor. */
+    bool callXfer(const Base::ThreadName& thread,
+                  const CUDA::PerformanceData& data,
+                  const TimeInterval& query_interval,
+                  CUDA::DataTransferVisitor& visitor)
+    {
+        data.visitDataTransfers(
+            thread, ConvertToArgoNavis(query_interval), visitor
+            );
+        
+        return true; // Always continue the visitation
+    }
+
+    /** Visitor used to insert an address into a buffer of unique addresses. */
+    bool insertIntoAddressBuffer(Base::Address address, PCBuffer& buffer)
+    {
+        UpdatePCBuffer(address, &buffer);
+
+        return true; // Always continue the visitation
+    }
+
+    /** Visitor used to insert an address into a set of unique addresses. */
+    bool insertIntoAddressSet(Base::Address address, set<Address>& addresses)
+    {
+        addresses.insert(Address(address));
+
+        return true; // Always continue the visitation
     }
     
 } // namespace <anonymous>
@@ -69,25 +139,211 @@
 
 //------------------------------------------------------------------------------
 //------------------------------------------------------------------------------
+string Queries::ConvertFromArgoNavis(const CUDA::CopyKind& kind)
+{
+    return CUDA::stringify<CUDA::CopyKind>(kind);
+}
+
+
+
+//------------------------------------------------------------------------------
+//------------------------------------------------------------------------------
+string Queries::ConvertFromArgoNavis(const CUDA::MemoryKind& kind)
+{
+    return CUDA::stringify<CUDA::MemoryKind>(kind);
+}
+
+
+
+//------------------------------------------------------------------------------
+//------------------------------------------------------------------------------
+StackTrace Queries::ConvertFromArgoNavis(const Base::StackTrace& trace,
+                                         const Thread& thread,
+                                         const Time& time)
+{
+    StackTrace result(thread, time);
+
+    // Initially include ALL frames for the call site of this CUDA request
+    for (Base::StackTrace::const_iterator
+             i = trace.begin(); i != trace.end(); ++i)
+    {
+        result.push_back(Address(static_cast<boost::uint64_t>(*i)));
+    }
+
+    //
+    // Trim all of the frames that preceeded the actual entry into main(),
+    // assuming, of course, that main() can actually be found.
+    //
+    
+    for (int i = 0; i < result.size(); ++i)
+    {
+        pair<bool, Function> function = result.getFunctionAt(i);
+        if (function.first && (function.second.getName() == "main"))
+        {
+            result.erase(result.begin() + i + 1, result.end());
+            break;
+        }            
+    }
+
+    //
+    // Now iterate over the frames from main() towards the final call site,
+    // looking for the first frame that is part of the CUDA implementation
+    // or our collector. Trim the stack trace from that point all the way
+    // to the final call site.
+    //
+    
+    for (int i = result.size(); i > 0; --i)
+    {
+        pair<bool, LinkedObject> linked_object = result.getLinkedObjectAt(i);
+        if (linked_object.first)
+        {
+            Path base_name = linked_object.second.getPath().getBaseName();
+            if (starts_with(base_name, "cuda-collector") ||
+                starts_with(base_name, "libcu"))
+            {
+                result.erase(result.begin(), result.begin() + i + 1);
+                break;
+            }
+        }
+        
+        pair<bool, Function> function = result.getFunctionAt(i);
+        if (function.first &&
+            (starts_with(function.second.getName(), "__device_stub") ||
+             starts_with(function.second.getName(), "cudart::")))
+        {
+            result.erase(result.begin(), result.begin() + i + 1);
+            break;
+        }
+    }
+        
+    return result;
+}
+
+
+
+//------------------------------------------------------------------------------
+//------------------------------------------------------------------------------
+Time Queries::ConvertFromArgoNavis(const Base::Time& time)
+{
+    return Time(static_cast<boost::uint64_t>(time));
+}
+
+
+
+//------------------------------------------------------------------------------
+//------------------------------------------------------------------------------
+TimeInterval Queries::ConvertFromArgoNavis(const Base::TimeInterval& interval)
+{
+    return TimeInterval(
+        Time(static_cast<boost::uint64_t>(interval.begin())),
+        Time(static_cast<boost::uint64_t>(interval.end() + 1))
+        );
+}
+
+
+
+//------------------------------------------------------------------------------
+//------------------------------------------------------------------------------
+Base::ThreadName Queries::ConvertToArgoNavis(const Thread& thread)
+{
+    pair<bool, pthread_t> tid = thread.getPosixThreadId();
+    pair<bool, int> rank = thread.getMPIRank();
+    
+    return Base::ThreadName(
+        thread.getHost(),
+        static_cast<uint64_t>(thread.getProcessId()),
+        tid.first ?
+            optional<uint64_t>(static_cast<uint64_t>(tid.second)) :
+            none,
+        rank.first ?
+            optional<uint32_t>(static_cast<uint32_t>(rank.second)) : 
+            none
+        );
+}
+
+
+
+//------------------------------------------------------------------------------
+//------------------------------------------------------------------------------
+Base::TimeInterval Queries::ConvertToArgoNavis(const TimeInterval& interval)
+{
+    return Base::TimeInterval(
+        Base::Time(interval.getBegin().getValue()),
+        Base::Time(interval.getEnd().getValue()) - 1
+        );
+}
+
+
+
+//------------------------------------------------------------------------------
+//------------------------------------------------------------------------------
+void Queries::GetCUDAPerformanceData(const Collector& collector,
+                                     const Thread& thread,
+                                     CUDA::PerformanceData& data)
+{
+    Assert(collector.getMetadata().getUniqueId() == "cuda");
+    Assert(collector.inSameDatabase(thread));
+
+    Base::ThreadName name = ConvertToArgoNavis(thread);
+    
+    SmartPtr<Database> database = EntrySpy(collector).getDatabase();
+    
+    BEGIN_TRANSACTION(database);
+    database->prepareStatement(
+        "SELECT data FROM Data WHERE collector = ? AND thread = ?;"
+        );
+    database->bindArgument(1, EntrySpy(collector).getEntry());
+    database->bindArgument(2, EntrySpy(thread).getEntry());
+    while (database->executeStatement())
+    {
+        CBTF_cuda_data message;
+        memset(&message, 0, sizeof(message));
+
+        Blob blob = database->getResultAsBlob(1);
+        blob.getXDRDecoding(
+            reinterpret_cast<xdrproc_t>(xdr_CBTF_cuda_data), &message
+            );
+    
+        data.apply(name, message);
+
+        xdr_free(reinterpret_cast<xdrproc_t>(xdr_CBTF_cuda_data),
+                 reinterpret_cast<char*>(&message));
+    }
+    END_TRANSACTION(database);
+}
+
+
+
+//------------------------------------------------------------------------------
+//------------------------------------------------------------------------------
 CUDAExecXferBalance Queries::GetCUDAExecXferBalance(
-    const CUDAData& data, const TimeInterval& interval
+    const CUDA::PerformanceData& data, const TimeInterval& interval
     )
 {
     CUDAExecXferBalance result;
     memset(&result, 0, sizeof(CUDAExecXferBalance));
 
-    function<void (const CUDAData::KernelExecutionDetails&)> exec_time = bind(
-        &accumulateEventTime<CUDAData::KernelExecutionDetails>,
+    CUDA::KernelExecutionVisitor exec_time = bind(
+        &accumulateEventTime<CUDA::KernelExecution>,
         _1, cref(interval), ref(result.exec_time)
         );
 
-    function<void (const CUDAData::MemoryCopyDetails&)> xfer_time = bind(
-        &accumulateEventTime<CUDAData::MemoryCopyDetails>,
+    Base::ThreadVisitor call_exec_time = bind(
+        &callExec, _1, cref(data), cref(interval), ref(exec_time)
+        );
+
+    data.visitThreads(call_exec_time);
+
+    CUDA::DataTransferVisitor xfer_time = bind(
+        &accumulateEventTime<CUDA::DataTransfer>,
         _1, cref(interval), ref(result.xfer_time)
         );
 
-    data.visit_kernel_executions(exec_time, interval);
-    data.visit_memory_copies(xfer_time, interval);
+    Base::ThreadVisitor call_xfer_time = bind(
+        &callXfer, _1, cref(data), cref(interval), ref(xfer_time)
+        );
+
+    data.visitThreads(call_xfer_time);
     
     return result;
 }
@@ -97,18 +353,69 @@
 //------------------------------------------------------------------------------
 //------------------------------------------------------------------------------
 CUDAXferRate Queries::GetCUDAXferRate(
-    const CUDAData& data, const TimeInterval& interval
+    const CUDA::PerformanceData& data, const TimeInterval& interval
     )
 {
     CUDAXferRate result;
     memset(&result, 0, sizeof(CUDAXferRate));
     
-    function<void (const CUDAData::MemoryCopyDetails&)> xfer_rate = bind(
+    CUDA::DataTransferVisitor xfer_rate = bind(
         &accumulateXferSizeAndTime,
         _1, cref(interval), ref(result.size), ref(result.time)
         );
-    
-    data.visit_memory_copies(xfer_rate, interval);
+
+    Base::ThreadVisitor call_xfer_rate = bind(
+        &callXfer, _1, cref(data), cref(interval), ref(xfer_rate)
+        );
+
+    data.visitThreads(call_xfer_rate);
     
     return result;
 }
+
+
+
+//------------------------------------------------------------------------------
+//------------------------------------------------------------------------------
+void Queries::GetCUDAUniquePCs(const Blob& blob, PCBuffer* buffer)
+{
+    CBTF_cuda_data message;
+    memset(&message, 0, sizeof(message));
+
+    blob.getXDRDecoding(
+        reinterpret_cast<xdrproc_t>(xdr_CBTF_cuda_data), &message
+        );
+
+    Base::AddressVisitor visitor = bind(
+        &insertIntoAddressBuffer, _1, ref(*buffer)
+        );
+
+    CUDA::PerformanceData::visitPCs(message, visitor);
+
+    xdr_free(reinterpret_cast<xdrproc_t>(xdr_CBTF_cuda_data),
+             reinterpret_cast<char*>(&message));
+}
+
+
+
+//------------------------------------------------------------------------------
+//------------------------------------------------------------------------------
+void Queries::GetCUDAUniquePCs(const Blob& blob,
+                               std::set<Address>& addresses)
+{
+    CBTF_cuda_data message;
+    memset(&message, 0, sizeof(message));
+
+    blob.getXDRDecoding(
+        reinterpret_cast<xdrproc_t>(xdr_CBTF_cuda_data), &message
+        );
+
+    Base::AddressVisitor visitor = bind(
+        &insertIntoAddressSet, _1, ref(addresses)
+        );
+
+    CUDA::PerformanceData::visitPCs(message, visitor);
+
+    xdr_free(reinterpret_cast<xdrproc_t>(xdr_CBTF_cuda_data),
+             reinterpret_cast<char*>(&message));
+}
diff -Naur OpenSpeedShop.ORIGINAL/libopenss-queries-cuda/CUDAQueries.hxx OpenSpeedShop.NEW-CUPTI/libopenss-queries-cuda/CUDAQueries.hxx
--- OpenSpeedShop.ORIGINAL/libopenss-queries-cuda/CUDAQueries.hxx	2014-11-20 00:29:06.000000000 -0600
+++ OpenSpeedShop.NEW-CUPTI/libopenss-queries-cuda/CUDAQueries.hxx	2016-02-26 16:07:37.770987879 -0600
@@ -1,5 +1,5 @@
 ////////////////////////////////////////////////////////////////////////////////
-// Copyright (c) 2014 Argo Navis Technologies. All Rights Reserved.
+// Copyright (c) 2014-2016 Argo Navis Technologies. All Rights Reserved.
 //
 // This library is free software; you can redistribute it and/or modify it under
 // the terms of the GNU Lesser General Public License as published by the Free
@@ -24,26 +24,132 @@
 #include "config.h"
 #endif
 
+#include <set>
+#include <string>
+
+#include <ArgoNavis/Base/StackTrace.hpp>
+#include <ArgoNavis/Base/ThreadName.hpp>
+#include <ArgoNavis/Base/Time.hpp>
+#include <ArgoNavis/Base/TimeInterval.hpp>
+
+#include <ArgoNavis/CUDA/CopyKind.hpp>
+#include <ArgoNavis/CUDA/MemoryKind.hpp>
+#include <ArgoNavis/CUDA/PerformanceData.hpp>
+
+#include "Address.hxx"
+#include "Blob.hxx"
+#include "Collector.hxx"
+#include "PCBuffer.hxx"
+#include "StackTrace.hxx"
+#include "Thread.hxx"
 #include "Time.hxx"
 #include "TimeInterval.hxx"
 
-#include "CUDAData.hxx"
 #include "CUDAExecXferBalance.hxx"
 #include "CUDAXferRate.hxx"
 
 namespace OpenSpeedShop { namespace Queries {
 
     /**
+     * Convert the given ArgoNavis::CUDA::CopyKind to a string.
+     *
+     * @param kind    Copy kind to be converted.
+     * @return        Converted copy kind.
+     */
+    std::string ConvertFromArgoNavis(const ArgoNavis::CUDA::CopyKind& kind);
+
+    /**
+     * Convert the given ArgoNavis::CUDA::MemoryKind to a string.
+     *
+     * @param kind    Memory kind to be converted.
+     * @return        Converted memory kind.
+     */
+    std::string ConvertFromArgoNavis(const ArgoNavis::CUDA::MemoryKind& kind);
+
+    /**
+     * Convert the given ArgoNavis::Base::StackTrace to a stack trace.
+     *
+     * @param trace     Stack trace to be converted.
+     * @param thread    Thread in which this stack trace was recorded.
+     * @param time      Time at which this stack trace was recorded.
+     * @return          Converted stack trace.
+     */
+    Framework::StackTrace ConvertFromArgoNavis(
+        const ArgoNavis::Base::StackTrace& trace,
+        const Framework::Thread& thread,
+        const Framework::Time& time
+        );
+    
+    /**
+     * Convert the given ArgoNavis::Base::Time to a time.
+     *
+     * @param time    Time to be converted.
+     * @return        Converted time.
+     */
+    Framework::Time ConvertFromArgoNavis(const ArgoNavis::Base::Time& time);
+
+    /**
+     * Convert the given ArgoNavis::Base::TimeInterval to a time interval.
+     *
+     * @param interval    Time interval to be converted.
+     * @return            Converted time interval.
+     */
+    Framework::TimeInterval ConvertFromArgoNavis(
+        const ArgoNavis::Base::TimeInterval& interval
+        );
+
+    /**
+     * Convert the given thread to an ArgoNavis::Base::ThreadName.
+     *
+     * @param thread    Thread to be converted.
+     * @return          Converted thread name.
+     */
+    ArgoNavis::Base::ThreadName ConvertToArgoNavis(
+        const Framework::Thread& thread
+        );
+
+    /**
+     * Convert the given time interval to an ArgoNavis::Base::TimeInterval.
+     *
+     * @param interval    Time interval to be converted.
+     * @return            Converted time interval.
+     */
+    ArgoNavis::Base::TimeInterval ConvertToArgoNavis(
+        const Framework::TimeInterval& interval
+        );
+
+    /**
+     * Get the CUDA performance data for the given collector and thread,
+     * adding it to the specified ArgoNavis::CUDA::PerformanceData object
+     * for use by use by subsequent queries.
+     *
+     * @pre    Can only be performed for a CUDA collector. An assertion
+     *         failure occurs if a different collector is used.
+     *
+     * @pre    The thread must be in the same experiment as the collector.
+     *         An assertion failure occurs if the thread is in a different
+     *         experiment than the collector.
+     *
+     * @param collector    Collector of the performance data to extract.
+     * @param thread       Thread generating the performance data to extract.
+     * @param data         Object to which the extracted performance data
+     *                     should be added.
+     */
+    void GetCUDAPerformanceData(const Framework::Collector& collector,
+                                const Framework::Thread& thread,
+                                ArgoNavis::CUDA::PerformanceData& data);
+
+    /**
      * Get metrics for evaluating the balance between the time spent in
      * CUDA kernel executions versus the time spent in CUDA data transfers.
      *
-     * @param data        CUDA data for which to get the metrics.
+     * @param data        CUDA performance data for which to get the metrics.
      * @param interval    Time interval over which to get the metrics. Has
      *                    a default value meaning of "all possible time".
      * @return            Metrics for evaluating the balance.
      */
     CUDAExecXferBalance GetCUDAExecXferBalance(
-        const CUDAData& data,
+        const ArgoNavis::CUDA::PerformanceData& data,
         const Framework::TimeInterval& interval = Framework::TimeInterval(
             Framework::Time::TheBeginning(),
             Framework::Time::TheEnd()
@@ -53,17 +159,34 @@
     /**
      * Get metrics for evaluating the CUDA data transfer rate.
      *
-     * @param data        CUDA data for which to get the metrics.
+     * @param data        CUDA performance data for which to get the metrics.
      * @param interval    Time interval over which to get the metrics. Has
      *                    a default value meaning of "all possible time".
      * @return            Metrics for evaluating the data transfer rate.
      */
     CUDAXferRate GetCUDAXferRate(
-        const CUDAData& data,
+        const ArgoNavis::CUDA::PerformanceData& data,
         const Framework::TimeInterval& interval = Framework::TimeInterval(
             Framework::Time::TheBeginning(),
             Framework::Time::TheEnd()
             )
         );
-    
+
+    /**
+     * Get the unique PCs in the given blob containing CUDA performance data.
+     *
+     * @param blob      Blob containing CUDA performance data.
+     * @param buffer    Buffer of unique PC values.
+     */
+    void GetCUDAUniquePCs(const Framework::Blob& blob, PCBuffer* buffer);
+
+    /**
+     * Get the unique PCs in the given blob containing CUDA performance data.
+     *
+     * @param blob         Blob containing CUDA performance data.
+     * @param addresses    Set of unique PC values.
+     */
+    void GetCUDAUniquePCs(const Framework::Blob& blob,
+                          std::set<Framework::Address>& addresses);
+        
 } } // namespace OpenSpeedShop::Queries
diff -Naur OpenSpeedShop.ORIGINAL/libopenss-queries-cuda/CUDAXferRate.hxx OpenSpeedShop.NEW-CUPTI/libopenss-queries-cuda/CUDAXferRate.hxx
--- OpenSpeedShop.ORIGINAL/libopenss-queries-cuda/CUDAXferRate.hxx	2014-11-20 00:29:06.000000000 -0600
+++ OpenSpeedShop.NEW-CUPTI/libopenss-queries-cuda/CUDAXferRate.hxx	2016-02-26 16:07:37.770987879 -0600
@@ -1,5 +1,5 @@
 ////////////////////////////////////////////////////////////////////////////////
-// Copyright (c) 2014 Argo Navis Technologies. All Rights Reserved.
+// Copyright (c) 2014-2016 Argo Navis Technologies. All Rights Reserved.
 //
 // This library is free software; you can redistribute it and/or modify it under
 // the terms of the GNU Lesser General Public License as published by the Free
@@ -24,7 +24,7 @@
 #include "config.h"
 #endif
 
-#include <stdint.h>
+#include <boost/cstdint.hpp>
 
 namespace OpenSpeedShop { namespace Queries {
 
@@ -35,7 +35,7 @@
     struct CUDAXferRate
     {
         /** Size (in bytes) of the CUDA data transfers. */
-        uint64_t size;
+        boost::uint64_t size;
         
         /** Time (in seconds) spent in CUDA data transfers. */
         double time;
diff -Naur OpenSpeedShop.ORIGINAL/libopenss-queries-cuda/Makefile.am OpenSpeedShop.NEW-CUPTI/libopenss-queries-cuda/Makefile.am
--- OpenSpeedShop.ORIGINAL/libopenss-queries-cuda/Makefile.am	2014-12-11 04:48:56.000000000 -0600
+++ OpenSpeedShop.NEW-CUPTI/libopenss-queries-cuda/Makefile.am	1969-12-31 18:00:00.000000000 -0600
@@ -1,86 +0,0 @@
-################################################################################
-# Copyright (c) 2014 Argo Navis Technologies. All Rights Reserved.
-#
-# This library is free software; you can redistribute it and/or modify it under
-# the terms of the GNU Lesser General Public License as published by the Free
-# Software Foundation; either version 2.1 of the License, or (at your option)
-# any later version.
-#
-# This library is distributed in the hope that it will be useful, but WITHOUT
-# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
-# FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more
-# details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this library; if not, write to the Free Software Foundation, Inc.,
-# 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
-################################################################################
-
-bin_PROGRAMS =
-lib_LTLIBRARIES =
-noinst_PROGRAMS =
-
-if BUILD_CBTF
-if HAVE_CBTF_MESSAGES_CUDA
-
-bin_PROGRAMS += osscuda2xml
-lib_LTLIBRARIES += libopenss-queries-cuda.la
-noinst_PROGRAMS += test
-
-libopenss_queries_cuda_la_CXXFLAGS = \
-	@BOOST_CPPFLAGS@ \
-	@MESSAGES_CPPFLAGS@ \
-	@MESSAGES_CUDA_CPPFLAGS@ \
-	-I$(top_srcdir)/libopenss-framework \
-	$(LTDLINCL)
-
-libopenss_queries_cuda_la_LDFLAGS = \
-	@MESSAGES_LDFLAGS@ \
-	@MESSAGES_CUDA_LDFLAGS@ \
-	-L$(top_srcdir)/libopenss-framework \
-	-export-dynamic -version-info 0:0:0
-
-libopenss_queries_cuda_la_LIBADD = \
-	@MESSAGES_LIBS@ \
-	@MESSAGES_CUDA_LIBS@ \
-	-lopenss-framework \
-	$(LIBLTDL)
-
-libopenss_queries_cuda_la_SOURCES = \
-	CUDAData.hxx CUDAData.cxx \
-	CUDAExecXferBalance.hxx \
-	CUDAQueries.hxx CUDAQueries.cxx \
-	CUDAXferRate.hxx
-
-osscuda2xml_CXXFLAGS =\
-	@BOOST_CPPFLAGS@ \
-	-I$(top_srcdir)/libopenss-framework \
-	-I$(top_srcdir)/libopenss-queries-cuda
-
-osscuda2xml_LDFLAGS = \
-	@BOOST_LDFLAGS@ \
-	-L$(top_srcdir)/libopenss-framework \
-	-L$(top_srcdir)/libopenss-queries-cuda
-
-osscuda2xml_LDADD = \
-	@BOOST_PROGRAM_OPTIONS_LIB@ \
-	-lopenss-framework \
-	-lopenss-queries-cuda
-
-osscuda2xml_SOURCES = osscuda2xml.cxx
-
-test_CXXFLAGS =\
-	@BOOST_CPPFLAGS@ \
-	-I$(top_srcdir)/libopenss-framework \
-	-I$(top_srcdir)/libopenss-queries-cuda
-
-test_LDFLAGS = \
-	-L$(top_srcdir)/libopenss-framework \
-	-L$(top_srcdir)/libopenss-queries-cuda
-
-test_LDADD = -lopenss-framework -lopenss-queries-cuda
-
-test_SOURCES = test.cxx
-
-endif
-endif
diff -Naur OpenSpeedShop.ORIGINAL/libopenss-queries-cuda/osscuda2xml.cxx OpenSpeedShop.NEW-CUPTI/libopenss-queries-cuda/osscuda2xml.cxx
--- OpenSpeedShop.ORIGINAL/libopenss-queries-cuda/osscuda2xml.cxx	2014-12-17 23:39:49.000000000 -0600
+++ OpenSpeedShop.NEW-CUPTI/libopenss-queries-cuda/osscuda2xml.cxx	2016-02-26 16:07:37.770987879 -0600
@@ -1,5 +1,5 @@
 ////////////////////////////////////////////////////////////////////////////////
-// Copyright (c) 2014 Argo Navis Technologies. All Rights Reserved.
+// Copyright (c) 2014-2016 Argo Navis Technologies. All Rights Reserved.
 //
 // This library is free software; you can redistribute it and/or modify it under
 // the terms of the GNU Lesser General Public License as published by the Free
@@ -17,6 +17,7 @@
 ////////////////////////////////////////////////////////////////////////////////
 
 #include <boost/bind.hpp>
+#include <boost/cstdint.hpp>
 #include <boost/function.hpp>
 #include <boost/optional.hpp>
 #include <boost/program_options.hpp>
@@ -31,9 +32,20 @@
 #include <utility>
 #include <vector>
 
+#include <ArgoNavis/Base/StackTrace.hpp>
+#include <ArgoNavis/Base/Time.hpp>
+
+#include <ArgoNavis/CUDA/DataTransfer.hpp>
+#include <ArgoNavis/CUDA/Device.hpp>
+#include <ArgoNavis/CUDA/KernelExecution.hpp>
+#include <ArgoNavis/CUDA/PerformanceData.hpp>
+#include <ArgoNavis/CUDA/stringify.hpp>
+#include <ArgoNavis/CUDA/Vector.hpp>
+
 #include "ToolAPI.hxx"
-#include "CUDAData.hxx"
+#include "CUDAQueries.hxx"
 
+using namespace ArgoNavis;
 using namespace boost;
 using namespace boost::program_options;
 using namespace OpenSpeedShop::Framework;
@@ -76,7 +88,7 @@
 
 
 /** Create a XML element with x, y, and z attributes from a Vector3u value. */
-string xyz(const string& tag, const CUDAData::Vector3u& value)
+string xyz(const string& tag, const CUDA::Vector3u& value)
 {
     stringstream stream;
     stream << "  <" << tag
@@ -89,94 +101,14 @@
 
 
 
-/** Convert a thread into XML and output it to a stream. */
-void convert(const Thread& thread, ostream& xml)
-{
-    xml << endl << "<Thread>" << endl;
-    xml << "  <Host>" << thread.getHost() << "</Host>" << endl;
-    xml << "  <ProcessId>" << thread.getProcessId() << "</ProcessId>" << endl;
-    
-    pair<bool, pthread_t> posix = thread.getPosixThreadId();
-    if (posix.first)
-    {
-        xml << "  <PosixThreadId>" << posix.second 
-            << "</PosixThreadId>" << endl;
-    }
-
-    pair<bool, int> openmp = thread.getOpenMPThreadId();
-    if (openmp.first)
-    {
-        xml << "  <OpenMPThreadId>" << openmp.second
-            << "</OpenMPThreadId>" << endl;
-    }
-            
-    pair<bool, int> mpi = thread.getMPIRank();
-    if (mpi.first)
-    {
-        xml << "  <MPIRank>" << mpi.second << "</MPIRank>" << endl;
-    }
-    
-    xml << "</Thread>" << endl;    
-}
-
-
-
-/** Convert call sites into XML and output them to a stream. */
-void convert(const vector<StackTrace>& call_sites, ostream& xml)
-{
-    for (vector<StackTrace>::size_type i = 0; i < call_sites.size(); ++i)
-    {
-        const StackTrace& trace = call_sites[i];
-
-        xml << endl << "<CallSite id=\"" << i << "\">" << endl;
-        
-        for (vector<Address>::size_type j = 0; j < trace.size(); ++j)
-        {
-            xml << "  <Frame>" << endl;
-            xml << "    <Address>" << trace[j] << "</Address>" << endl;
-
-            pair<bool, LinkedObject> linked_object = trace.getLinkedObjectAt(j);
-            if (linked_object.first)
-            {
-                xml << "    <LinkedObject>"
-                    << linked_object.second.getPath()
-                    << "</LinkedObject>" << endl;
-            }
-
-            pair<bool, Function> function = trace.getFunctionAt(j);
-            if (function.first)
-            {
-                xml << "    <Function>"
-                    << function.second.getDemangledName()
-                    << "</Function>" << endl;
-            }
-            
-            set<Statement> statements = trace.getStatementsAt(j);
-            for (set<Statement>::const_iterator
-                     k = statements.begin(); k != statements.end(); ++k)
-            {
-                xml << "    <Statement>"
-                    << k->getPath() << ", " << k->getLine()
-                    << "</Statement>" << endl;
-            }
-            
-            xml << "  </Frame>" << endl;
-        }
-
-        xml << "</CallSite>" << endl;
-    }
-}
-
-
-
 /** Convert counters into XML and output them to a stream. */
-void convert(const vector<string>& counters, ostream& xml)
+void convert_counters(const CUDA::PerformanceData& data, ostream& xml)
 {
     xml << endl;
-    for (vector<string>::size_type i = 0; i < counters.size(); ++i)
+    for (vector<string>::size_type i = 0; i < data.counters().size(); ++i)
     {
         xml << "<Counter id=\"" << i << "\">"
-            << counters[i]
+            << data.counters()[i]
             << "</Counter>" << endl;
     }
 }
@@ -184,12 +116,11 @@
 
 
 /** Convert devices into XML and output them to a stream. */
-void convert(const vector<CUDAData::DeviceDetails>& devices, ostream& xml)
+void convert_devices(const CUDA::PerformanceData& data, ostream& xml)
 {
-    for (vector<CUDAData::DeviceDetails>::size_type 
-             i = 0; i < devices.size(); ++i)
+    for (vector<CUDA::Device>::size_type i = 0; i < data.devices().size(); ++i)
     {
-        const CUDAData::DeviceDetails& device = devices[i];
+        const CUDA::Device& device = data.devices()[i];
         
         xml << endl << "<Device id=\"" << i << "\">" << endl;
         xml << text("Name", device.name);
@@ -209,6 +140,8 @@
         xml << text("MemcpyEngines", device.memcpy_engines);
         xml << text("Multiprocessors", device.multiprocessors);
         xml << text("MaxIPC", device.max_ipc);
+        xml << text("MaxWarpsPerMultiprocessor",
+                    device.max_warps_per_multiprocessor);
         xml << text("MaxBlocksPerMultiprocessor",
                     device.max_blocks_per_multiprocessor);
         xml << text("MaxRegistersPerBlock", device.max_registers_per_block);
@@ -221,89 +154,203 @@
 
 
 
-/** Convert a kernel execution into XML and output it to a stream. */
-void convert_kernel_execution(
-    const Time& time_origin,
-    const CUDAData::KernelExecutionDetails& details,
-    ostream& xml
-    )
+/** Convert call sites into Open|SpeedShop Framework StackTrace objects. */
+template <typename T>
+bool convert_sites_in_event(const CUDA::PerformanceData& data,
+                            const Thread& thread,
+                            const T& details,
+                            vector<shared_ptr<StackTrace> >& sites,
+                            size_t& sites_found)
 {
-    xml << endl << "<KernelExecution"
-        << " call_site=\"" << details.call_site << "\""
-        << " device=\"" << details.device << "\""
-        ">" << endl;
-    xml << text("Time", details.time - time_origin);
-    xml << text("TimeBegin", details.time_begin - time_origin);
-    xml << text("TimeEnd", details.time_end - time_origin);
-    xml << text("Function", demangle(details.function));
-    xml << xyz("Grid", details.grid);
-    xml << xyz("Block", details.block);
-    xml << text("CachePreference",
-                CUDAData::stringify(details.cache_preference));
-    xml << text("RegistersPerThread", details.registers_per_thread);
-    xml << text("StaticSharedMemory", details.static_shared_memory);
-    xml << text("DynamicSharedMemory", details.dynamic_shared_memory);
-    xml << text("LocalMemory", details.local_memory);
-    xml << "</KernelExecution>" << endl;
+    size_t n = details.call_site;
+    
+    if (!sites[n])
+    {
+        sites[n].reset(new StackTrace(thread, Time(details.time)));
+        
+        for (Base::StackTrace::const_iterator
+                 i = data.sites()[n].begin(); i != data.sites()[n].end(); ++i)
+        {
+            sites[n]->push_back(Address(*i));
+        }
+        
+        sites_found++;
+    }
+    
+    return sites_found < data.sites().size();
+}
+
+
+
+/** Convert call sites into Open|SpeedShop Framework StackTrace objects. */
+bool convert_sites_in_thread(const CUDA::PerformanceData& data,
+                             const map<Base::ThreadName, Thread>& threads,
+                             const Base::ThreadName& thread,
+                             vector<shared_ptr<StackTrace> >& sites,
+                             size_t& sites_found)
+{
+    data.visitDataTransfers(
+        thread, data.interval(),
+        bind(&convert_sites_in_event<CUDA::DataTransfer>,
+             cref(data), cref(threads.find(thread)->second), _1,
+             ref(sites), ref(sites_found))
+        );
+    
+    if (sites_found == data.sites().size())
+    {
+        return false;
+    }
+    
+    data.visitKernelExecutions(
+        thread, data.interval(),
+        bind(&convert_sites_in_event<CUDA::KernelExecution>,
+             cref(data), cref(threads.find(thread)->second), _1,
+             ref(sites), ref(sites_found))
+        );
+    
+    return sites_found < data.sites().size();
 }
 
 
 
-/** Convert a memory copy into XML and output it to a stream. */
-void convert_memory_copy(
-    const Time& time_origin,
-    const CUDAData::MemoryCopyDetails& details,
-    ostream& xml
-    )
+/** Convert call sites into XML and output them to a stream. */
+void convert_sites(const CUDA::PerformanceData& data, 
+                   const map<Base::ThreadName, Thread>& threads,
+                   ostream& xml)
+{
+    vector<shared_ptr<StackTrace> > sites(data.sites().size());
+    size_t sites_found = 0;
+    
+    data.visitThreads(
+        bind(&convert_sites_in_thread,
+             cref(data), cref(threads), _1, ref(sites), ref(sites_found))
+        );
+    
+    for (size_t i = 0; i < sites.size(); ++i)
+    {
+        xml << endl << "<CallSite id=\"" << i << "\">" << endl;
+
+        if (sites[i])
+        {
+            const StackTrace& trace = *sites[i];
+            
+            for (StackTrace::size_type j = 0; j < trace.size(); ++j)
+            {
+                xml << "  <Frame>" << endl;
+                xml << "    <Address>" << trace[j] << "</Address>" << endl;
+                
+                pair<bool, LinkedObject> linked_object = 
+                    trace.getLinkedObjectAt(j);
+                if (linked_object.first)
+                {
+                    xml << "    <LinkedObject>"
+                        << linked_object.second.getPath()
+                        << "</LinkedObject>" << endl;
+                }
+                
+                pair<bool, Function> function = trace.getFunctionAt(j);
+                if (function.first)
+                {
+                    xml << "    <Function>"
+                        << function.second.getDemangledName()
+                        << "</Function>" << endl;
+                }
+                
+                set<Statement> statements = trace.getStatementsAt(j);
+                for (set<Statement>::const_iterator
+                         k = statements.begin(); k != statements.end(); ++k)
+                {
+                    xml << "    <Statement>"
+                        << k->getPath() << ", " << k->getLine()
+                        << "</Statement>" << endl;
+                }
+                
+                xml << "  </Frame>" << endl;
+            }
+        }
+        else
+        {
+            const Base::StackTrace& trace = data.sites()[i];
+            
+            for (StackTrace::size_type j = 0; j < trace.size(); ++j)
+            {
+                xml << "  <Frame>" << endl;
+                xml << "    <Address>" << trace[j] << "</Address>" << endl;
+                xml << "  </Frame>" << endl;
+            }
+        }
+
+        xml << "</CallSite>" << endl;
+    }
+}
+
+
+
+/** Convert a data transfer into XML and output it to a stream. */
+bool convert_data_transfer(const Base::Time& time_origin,
+                           const CUDA::DataTransfer& details,
+                           ostream& xml)
 {
-    xml << endl << "<MemoryCopy"
+    xml << endl << "<DataTransfer"
         << " call_site=\"" << details.call_site << "\""
         << " device=\"" << details.device << "\""
         ">" << endl;
-    xml << text("Time", details.time - time_origin);
-    xml << text("TimeBegin", details.time_begin - time_origin);
-    xml << text("TimeEnd", details.time_end - time_origin);
+    xml << text("Time",
+                static_cast<uint64_t>(details.time - time_origin));
+    xml << text("TimeBegin",
+                static_cast<uint64_t>(details.time_begin - time_origin));
+    xml << text("TimeEnd",
+                static_cast<uint64_t>(details.time_end - time_origin));
     xml << text("Size", details.size);
-    xml << text("Kind", CUDAData::stringify(details.kind));
-    xml << text("SourceKind", CUDAData::stringify(details.source_kind));
-    xml << text("DestinationKind",
-                CUDAData::stringify(details.destination_kind));
+    xml << text("Kind", CUDA::stringify(details.kind));
+    xml << text("SourceKind", CUDA::stringify(details.source_kind));
+    xml << text("DestinationKind", CUDA::stringify(details.destination_kind));
     xml << text("Asynchronous", (details.asynchronous ? "true" : "false"));
-    xml << "</MemoryCopy>" << endl;
+    xml << "</DataTransfer>" << endl;
+
+    return true; // Always continue the visitation
 }
 
 
 
-/** Convert a memory set into XML and output it to a stream. */
-void convert_memory_set(
-    const Time& time_origin,
-    const CUDAData::MemorySetDetails& details,
-    ostream& xml
-    )
+/** Convert a kernel execution into XML and output it to a stream. */
+bool convert_kernel_execution(const Base::Time& time_origin,
+                              const CUDA::KernelExecution& details,
+                              ostream& xml)
 {
-    xml << endl << "<MemorySet"
+    xml << endl << "<KernelExecution"
         << " call_site=\"" << details.call_site << "\""
         << " device=\"" << details.device << "\""
         ">" << endl;
-    xml << text("Time", details.time - time_origin);
-    xml << text("TimeBegin", details.time_begin - time_origin);
-    xml << text("TimeEnd", details.time_end - time_origin);
-    xml << text("Size", details.size);
-    xml << "</MemorySet>" << endl;
+    xml << text("Time",
+                static_cast<uint64_t>(details.time - time_origin));
+    xml << text("TimeBegin",
+                static_cast<uint64_t>(details.time_begin - time_origin));
+    xml << text("TimeEnd",
+                static_cast<uint64_t>(details.time_end - time_origin));
+    xml << text("Function", demangle(details.function));
+    xml << xyz("Grid", details.grid);
+    xml << xyz("Block", details.block);
+    xml << text("CachePreference", CUDA::stringify(details.cache_preference));
+    xml << text("RegistersPerThread", details.registers_per_thread);
+    xml << text("StaticSharedMemory", details.static_shared_memory);
+    xml << text("DynamicSharedMemory", details.dynamic_shared_memory);
+    xml << text("LocalMemory", details.local_memory);
+    xml << "</KernelExecution>" << endl;
+
+    return true; // Always continue the visitation
 }
 
 
 
 /** Convert a periodic sample into XML and output it to a stream. */
-void convert_periodic_sample(
-    const Time& time_origin,
-    const Time& time,
-    const vector<uint64_t>& counts,
-    ostream& xml
-    )
+bool convert_periodic_sample(const Base::Time& time_origin,
+                             const Base::Time& time,
+                             const vector<uint64_t>& counts,
+                             ostream& xml)
 {
     xml << "<Sample>" << endl;
-    xml << text("Time", time - time_origin);
+    xml << text("Time", static_cast<uint64_t>(time - time_origin));
     for (vector<uint64_t>::size_type i = 0; i < counts.size(); ++i)
     {
         xml << "  <Count counter=\"" << i << "\">"
@@ -311,53 +358,72 @@
             << "</Count>" << endl;
     }
     xml << "</Sample>" << endl;
+
+    return true; // Always continue the visitation
 }
 
 
 
-/** Convert CUDA performance data into XML and output it to a stream. */
-void convert(const Time& time_origin, const CUDAData& data, ostream& xml)
+/** Convert a thread into XML and output it to a stream. */
+void convert_thread(const Base::ThreadName& thread, ostream& xml)
 {
-    convert(data.call_sites(), xml);
-    convert(data.counters(), xml);
-    convert(data.devices(), xml);
-
-    boost::function<
-        void (const CUDAData::KernelExecutionDetails&)
-        > kernel_execution_visitor(
-            boost::bind(&convert_kernel_execution,
-                        boost::cref(time_origin), _1, boost::ref(xml))
-            );
-    
-    data.visit_kernel_executions(kernel_execution_visitor);
-
-    boost::function<
-        void (const CUDAData::MemoryCopyDetails&)
-        > memory_copy_visitor(
-            boost::bind(&convert_memory_copy,
-                        boost::cref(time_origin), _1, boost::ref(xml))
-            );
-    
-    data.visit_memory_copies(memory_copy_visitor);
-
-    boost::function<
-        void (const CUDAData::MemorySetDetails&)
-        > memory_set_visitor(
-            boost::bind(&convert_memory_set,
-                        boost::cref(time_origin), _1, boost::ref(xml))
-            );
-    
-    data.visit_memory_sets(memory_set_visitor);
-
-    boost::function<
-        void (const Time&, const vector<uint64_t>&)
-        > periodic_sample_visitor(
-            boost::bind(&convert_periodic_sample,
-                        boost::cref(time_origin), _1, _2, boost::ref(xml))
-            );
+    xml << endl << "<Thread>" << endl;
+    xml << "  <Host>" << thread.host() << "</Host>" << endl;
+    xml << "  <ProcessId>" << thread.pid() << "</ProcessId>" << endl;
 
-    xml << endl;
-    data.visit_periodic_samples(periodic_sample_visitor);
+    if (thread.tid())
+    {
+        xml << "  <PosixThreadId>" << *thread.tid() 
+            << "</PosixThreadId>" << endl;
+    }
+
+    if (thread.mpi_rank())
+    {
+        xml << "  <MPIRank>" << *thread.mpi_rank() << "</MPIRank>" << endl;
+    }
+    
+    if (thread.omp_rank())
+    {
+        xml << "  <OpenMPThreadId>" << *thread.omp_rank()
+            << "</OpenMPThreadId>" << endl;
+    }
+    
+    xml << "</Thread>" << endl;    
+}
+
+
+
+/** Convert CUDA performance data into XML and output it to a stream. */
+bool convert_performance_data(const CUDA::PerformanceData& data,
+                              const map<Base::ThreadName, Thread>& threads,
+                              const Base::ThreadName& thread,
+                              ostream& xml)
+{
+    xml << endl << "<DataSet>" << endl;
+
+    convert_thread(thread, xml);
+
+    data.visitDataTransfers(
+        thread, data.interval(),
+        bind(&convert_data_transfer,
+             cref(data.interval().begin()), _1, ref(xml))
+        );
+
+    data.visitKernelExecutions(
+        thread, data.interval(),
+        bind(&convert_kernel_execution,
+             cref(data.interval().begin()), _1, ref(xml))
+        );
+
+    data.visitPeriodicSamples(
+        thread, data.interval(),
+        bind(&convert_periodic_sample,
+             cref(data.interval().begin()), _1, _2, ref(xml))
+        );
+    
+    xml << endl << "</DataSet>" << endl;
+
+    return true; // Always continue the visitation
 }
 
 
@@ -381,8 +447,6 @@
            << endl;
     string kExtraHelp = stream.str();
 
-    // Parse and validate the command-line arguments
-    
     options_description kNonPositionalOptions("osscuda2xml options");
     kNonPositionalOptions.add_options()
  
@@ -463,7 +527,7 @@
              << endl << kNonPositionalOptions << kExtraHelp;
         return 1;        
     }
-    
+
     set<int> ranks;
     if (values.count("rank") > 0)
     {
@@ -484,62 +548,37 @@
     *xml << "<?xml version=\"1.0\" encoding=\"utf-8\"?>" << endl;
     *xml << "<CUDA>" << endl;
 
-    //
-    // NOTE: Currently each dataset contains one (and only one) thread. Ideally
-    // in the future some sort of cluster analysis will be used to group similar
-    // performing threads together into a single dataset.
-    //
+    CUDA::PerformanceData data;
+    map<Base::ThreadName, Thread> threads;
 
-    typedef vector<pair<ThreadGroup, shared_ptr<CUDAData> > > DataSets;
-    
-    Time time_origin = Time::TheEnd();
-    DataSets datasets;
-    
-    ThreadGroup threads = experiment.getThreads();
+    ThreadGroup all_threads = experiment.getThreads();
     for (ThreadGroup::const_iterator
-             i = threads.begin(); i != threads.end(); ++i)
+             i = all_threads.begin(); i != all_threads.end(); ++i)
     {
         pair<bool, int> rank = i->getMPIRank();
         
         if (ranks.empty() || 
             (rank.first && (ranks.find(rank.second) != ranks.end())))
         {
-            ThreadGroup thread;
-            thread.insert(*i);
-
-            shared_ptr<CUDAData> data(new CUDAData(*collector, *i));
-            Time t = data->time_origin();
-            
-            if (t < time_origin)
-            {
-                time_origin = t;
-            }
-            
-            datasets.push_back(make_pair(thread, data));
+            GetCUDAPerformanceData(*collector, *i, data);
+            threads.insert(make_pair(ConvertToArgoNavis(*i), *i));
         }
     }
 
-    *xml << endl
-         << "<TimeOrigin>" << time_origin.getValue() << "</TimeOrigin>" << endl;
-
-    for (DataSets::const_iterator
-             i = datasets.begin(); i != datasets.end(); ++i)
-    {
-        *xml << endl << "<DataSet>" << endl;
-
-        for (ThreadGroup::const_iterator
-                 j = i->first.begin(); j != i->first.end(); ++j)
-        {
-            convert(*j, *xml);
-        }
-
-        convert(time_origin, *i->second.get(), *xml);
-
-        *xml << endl << "</DataSet>" << endl;        
-    }
+    *xml << endl << "<TimeOrigin>"
+         << static_cast<uint64_t>(data.interval().begin())
+         << "</TimeOrigin>" << endl;
+    
+    convert_counters(data, *xml);
+    convert_devices(data, *xml);
+    convert_sites(data, threads, *xml);
+    
+    data.visitThreads(bind(
+        &convert_performance_data, cref(data), cref(threads), _1, ref(*xml)
+        ));
     
     *xml << endl << "</CUDA>" << endl;
-    
+
     if (values.count("xml") == 1)
     {
         delete xml;
diff -Naur OpenSpeedShop.ORIGINAL/libopenss-queries-cuda/test.cxx OpenSpeedShop.NEW-CUPTI/libopenss-queries-cuda/test.cxx
--- OpenSpeedShop.ORIGINAL/libopenss-queries-cuda/test.cxx	2014-11-20 00:29:06.000000000 -0600
+++ OpenSpeedShop.NEW-CUPTI/libopenss-queries-cuda/test.cxx	1969-12-31 18:00:00.000000000 -0600
@@ -1,126 +0,0 @@
-////////////////////////////////////////////////////////////////////////////////
-// Copyright (c) 2014 Argo Navis Technologies. All Rights Reserved.
-//
-// This library is free software; you can redistribute it and/or modify it under
-// the terms of the GNU Lesser General Public License as published by the Free
-// Software Foundation; either version 2.1 of the License, or (at your option)
-// any later version.
-//
-// This library is distributed in the hope that it will be useful, but WITHOUT
-// ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
-// FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more
-// details.
-//
-// You should have received a copy of the GNU Lesser General Public License
-// along with this library; if not, write to the Free Software Foundation, Inc.,
-// 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
-////////////////////////////////////////////////////////////////////////////////
-
-#include <boost/format.hpp>
-#include <cmath>
-#include <iostream>
-#include <stdexcept>
-
-#include "ToolAPI.hxx"
-#include "CUDAQueries.hxx"
-
-using namespace boost;
-using namespace OpenSpeedShop::Framework;
-using namespace OpenSpeedShop::Queries;
-using namespace std;
-
-
-
-template <typename T>
-string sizeToString(const T& value)
-{
-    const struct { const double value; const char* label; } kUnits[] = {
-        { 1024.0 * 1024.0 * 1024.0 * 1024.0, "TB" },
-        {          1024.0 * 1024.0 * 1024.0, "GB" },
-        {                   1024.0 * 1024.0, "MB" },
-        {                            1024.0, "KB" },
-        {                               0.0, NULL } // End-Of-Table
-    };
-    
-    double x = static_cast<double>(value);
-    string label = "Bytes";
-    
-    for (int i = 0; kUnits[i].label != NULL; ++i)
-    {
-        if (static_cast<double>(value) >= kUnits[i].value)
-        {
-            x = static_cast<double>(value) / kUnits[i].value;
-            label = kUnits[i].label;
-            break;
-        }
-    }
-    
-    return str(
-        (x == floor(x)) ?
-        (format("%1% %2%") % static_cast<int>(x) % label) :
-        (format("%1$0.1f %2%") % x % label)
-        );
-}
-
-
-
-int main(int argc, char* argv[])
-{
-    if (argc < 2)
-    {
-        cout << endl 
-             << "Usage: " << argv[0] << " <experiment-database>" << endl 
-             << endl;
-        return 1;
-    }
-
-    try
-    {
-        Experiment experiment(argv[1]);
-
-        Collector collector = *experiment.getCollectors().begin();
-        if (collector.getMetadata().getUniqueId() != "cuda")
-        {
-            throw runtime_error(
-                "The specified experiment didn't use the CUDA collector."
-                );
-        }
-
-        Thread thread = *experiment.getThreads().begin();
-
-        CUDAData cuda_data(collector, thread);
-
-        CUDAExecXferBalance balance = GetCUDAExecXferBalance(cuda_data);
-        
-        cout << endl;
-        cout << "CUDAExecXferBalance.exec_time = " 
-             << balance.exec_time << " Seconds" << endl;
-        cout << "CUDAExecXferBalance.xfer_time = " 
-             << balance.xfer_time << " Seconds" << endl;
-        cout << "                        Ratio = " 
-             << (balance.exec_time / balance.xfer_time) << endl;
-
-        CUDAXferRate rate = GetCUDAXferRate(cuda_data);
-
-        cout << endl;
-        cout << "CUDAXferRate.time = " << rate.time << " Seconds" << endl;
-        cout << "CUDAXferRate.size = " << sizeToString(rate.size) << endl;
-        cout << "Average Xfer Rate = " 
-             << sizeToString(static_cast<double>(rate.size) / rate.time)
-             << "/Second" << endl;
-        
-        cout << endl << endl;           
-    }
-    catch (const std::exception& error)
-    {
-        cerr << endl << "ERROR: " << error.what() << endl << endl;
-        return 1;        
-    }
-    catch (const Exception& error)
-    {
-        cerr << endl << "ERROR: " << error.getDescription() << endl << endl;
-        return 1;
-    }
-    
-    return 0;
-}
diff -Naur OpenSpeedShop.ORIGINAL/openss/CMakeLists.txt OpenSpeedShop.NEW-CUPTI/openss/CMakeLists.txt
--- OpenSpeedShop.ORIGINAL/openss/CMakeLists.txt	2016-01-19 21:05:46.000000000 -0600
+++ OpenSpeedShop.NEW-CUPTI/openss/CMakeLists.txt	2016-02-26 16:07:37.770987879 -0600
@@ -1,5 +1,6 @@
 ################################################################################
 # Copyright (c) 2014-2016 Krell Institute. All Rights Reserved.
+# Copyright (c) 2015,2016 Argo Navis Technologies. All Rights Reserved.
 #
 # This program is free software; you can redistribute it and/or modify it under
 # the terms of the GNU General Public License as published by the Free Software
@@ -300,7 +301,6 @@
     	    WORKING_DIRECTORY ${CMAKE_INSTALL_PREFIX}/bin) ")
 
         # create oss cbtf related links here if instrumentor is cbtf
-
         if (${INSTRUMENTOR} MATCHES "cbtf")
             install(CODE " EXECUTE_PROCESS(COMMAND ln -sf ossdriver ossmem
 	        WORKING_DIRECTORY ${CMAKE_INSTALL_PREFIX}/bin) ")
@@ -310,19 +310,14 @@
 	        WORKING_DIRECTORY ${CMAKE_INSTALL_PREFIX}/bin) ")
             install(CODE " EXECUTE_PROCESS(COMMAND ln -sf ossdriver ossiop
 	        WORKING_DIRECTORY ${CMAKE_INSTALL_PREFIX}/bin) ")
+        endif()
 
-            # 
-            # If the Argo Navis cuda base libraries and messages were found 
-            # that means we have cuda based collectors and views, so install
-            # the osscuda convenience scripts.
-            #
-            if (CBTF-ARGO-MESSAGESCUDA_FOUND)
-                install(CODE " EXECUTE_PROCESS(COMMAND ln -sf ossdriver osscuda
-	            WORKING_DIRECTORY ${CMAKE_INSTALL_PREFIX}/bin) ")
-                install(CODE " EXECUTE_PROCESS(COMMAND ln -sf ossdriver osscudaio
-	            WORKING_DIRECTORY ${CMAKE_INSTALL_PREFIX}/bin) ")
-            endif()
+        # create cuda driver link
+        if(BUILD_CUDA_SUPPORT)
+            install(CODE " EXECUTE_PROCESS(COMMAND ln -sf ossdriver osscuda
+            WORKING_DIRECTORY ${CMAKE_INSTALL_PREFIX}/bin) ")
         endif()
+
     endif(CMAKE_HOST_UNIX)
 
 endif() #endif for RUNTIME_ONLY
diff -Naur OpenSpeedShop.ORIGINAL/openss/osscollect.cxx OpenSpeedShop.NEW-CUPTI/openss/osscollect.cxx
--- OpenSpeedShop.ORIGINAL/openss/osscollect.cxx	2015-11-25 11:07:53.000000000 -0600
+++ OpenSpeedShop.NEW-CUPTI/openss/osscollect.cxx	2016-02-26 16:07:37.770987879 -0600
@@ -1,5 +1,6 @@
 ////////////////////////////////////////////////////////////////////////////////
 // Copyright (c) 2011-2013 Krell Institute. All Rights Reserved.
+// Copyright (c) 2015,2016 Argo Navis Technologies. All Rights Reserved.
 //
 // This library is free software; you can redistribute it and/or modify it under
 // the terms of the GNU Lesser General Public License as published by the Free
@@ -385,11 +386,7 @@
     FW_Experiment->setBEprocCount( numBE );
     FW_Experiment->setInstrumentorUsesCBTF( false );
 
-    // FIXME: Is this cudaio override to io still needed with
-    // the latest cuda collector?
-    Collector mycollector = FW_Experiment->createCollector(
-        (collector == "cudaio") ? "io" : collector
-        );
+    Collector mycollector = FW_Experiment->createCollector(collector);
 
     ThreadGroup tg = FW_Experiment->createProcess(program, fenodename, numBE,
                                                      OutputCallback((void (*)(const char*, const int&, void*))NULL,(void *)NULL),
diff -Naur OpenSpeedShop.ORIGINAL/ossdumpcuda/CMakeLists.txt OpenSpeedShop.NEW-CUPTI/ossdumpcuda/CMakeLists.txt
--- OpenSpeedShop.ORIGINAL/ossdumpcuda/CMakeLists.txt	2015-06-02 17:38:24.000000000 -0500
+++ OpenSpeedShop.NEW-CUPTI/ossdumpcuda/CMakeLists.txt	2016-02-26 16:07:37.770987879 -0600
@@ -1,5 +1,6 @@
 ################################################################################
 # Copyright (c) 2015 Krell Institute. All Rights Reserved.
+# Copyright (c) 2015,2016 Argo Navis Technologies. All Rights Reserved.
 #
 # This program is free software; you can redistribute it and/or modify it under
 # the terms of the GNU General Public License as published by the Free Software
@@ -18,27 +19,28 @@
 
 add_executable(ossdumpcuda
 	ossdumpcuda.cxx
-)
+    )
 
 target_include_directories(ossdumpcuda PUBLIC
     ${CMAKE_CURRENT_SOURCE_DIR}
-    ${CMAKE_CURRENT_SOURCE_DIR}/../libopenss-framework
-    ${CMAKE_CURRENT_BINARY_DIR}
+    ${PROJECT_SOURCE_DIR}/libopenss-framework
     ${Boost_INCLUDE_DIRS}
-    ${CBTF_MESSAGES_INCLUDE_DIRS}
     ${CBTF_MESSAGES_CUDA_INCLUDE_DIRS}
-)
+    ${ARGONAVIS_BASE_INCLUDE_DIRS}
+    ${ARGONAVIS_CUDA_INCLUDE_DIRS}
+    )
 
 target_link_libraries(ossdumpcuda
     openss-framework
     openss-framework-cbtf
     openss-framework-symtabapi
     ${Boost_LIBRARIES}
-    ${CBTF_MESSAGES_LIBRARIES}
     ${CBTF_MESSAGES_CUDA_LIBRARIES}
+    ${ARGONAVIS_BASE_LIBRARIES}
+    ${ARGONAVIS_CUDA_LIBRARIES}
     ${CMAKE_DL_LIBS}
-)
+    )
 
 install(TARGETS ossdumpcuda
     RUNTIME DESTINATION bin
-)
+    )
diff -Naur OpenSpeedShop.ORIGINAL/ossdumpcuda/Makefile.am OpenSpeedShop.NEW-CUPTI/ossdumpcuda/Makefile.am
--- OpenSpeedShop.ORIGINAL/ossdumpcuda/Makefile.am	2014-10-30 00:27:56.000000000 -0500
+++ OpenSpeedShop.NEW-CUPTI/ossdumpcuda/Makefile.am	1969-12-31 18:00:00.000000000 -0600
@@ -1,50 +0,0 @@
-################################################################################
-# Copyright (c) 2014 Argo Navis Technologies. All Rights Reserved.
-#
-# This program is free software; you can redistribute it and/or modify it under
-# the terms of the GNU General Public License as published by the Free Software
-# Foundation; either version 2 of the License, or (at your option) any later
-# version.
-#
-# This program is distributed in the hope that it will be useful, but WITHOUT
-# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
-# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
-# details.
-#
-# You should have received a copy of the GNU General Public License along with
-# this program; if not, write to the Free Software Foundation, Inc., 59 Temple
-# Place, Suite 330, Boston, MA  02111-1307  USA
-################################################################################
-
-bin_PROGRAMS =
-
-if BUILD_CBTF
-if HAVE_CBTF_MESSAGES_CUDA
-
-bin_PROGRAMS += ossdumpcuda
-
-ossdumpcuda_CXXFLAGS = \
-	-I$(top_srcdir)/libopenss-framework \
-	@BOOST_CPPFLAGS@ \
-	@MESSAGES_CPPFLAGS@ \
-	@MESSAGES_CUDA_CPPFLAGS@
-
-ossdumpcuda_LDFLAGS = \
-	-L$(top_srcdir)/libopenss-framework \
-	@BOOST_LDFLAGS@ \
-	@MESSAGES_LDFLAGS@ \
-	@MESSAGES_CUDA_LDFLAGS@
-
-ossdumpcuda_LDADD = \
-	-lopenss-framework \
-	@BOOST_FILESYSTEM_LIB@ \
-	@BOOST_PROGRAM_OPTIONS_LIB@ \
-	@BOOST_SYSTEM_LIB@ \
-	@MESSAGES_LIBS@ \
-	@MESSAGES_CUDA_LIBS@
-
-ossdumpcuda_SOURCES = \
-	ossdumpcuda.cxx
-
-endif
-endif
diff -Naur OpenSpeedShop.ORIGINAL/ossdumpcuda/ossdumpcuda.cxx OpenSpeedShop.NEW-CUPTI/ossdumpcuda/ossdumpcuda.cxx
--- OpenSpeedShop.ORIGINAL/ossdumpcuda/ossdumpcuda.cxx	2014-10-30 00:27:56.000000000 -0500
+++ OpenSpeedShop.NEW-CUPTI/ossdumpcuda/ossdumpcuda.cxx	2016-02-26 16:07:37.770987879 -0600
@@ -1,5 +1,5 @@
 ////////////////////////////////////////////////////////////////////////////////
-// Copyright (c) 2014 Argo Navis Technologies. All Rights Reserved.
+// Copyright (c) 2014-2016 Argo Navis Technologies. All Rights Reserved.
 //
 // This program is free software; you can redistribute it and/or modify it under
 // the terms of the GNU General Public License as published by the Free Software
@@ -16,28 +16,23 @@
 // Place, Suite 330, Boston, MA  02111-1307  USA
 ////////////////////////////////////////////////////////////////////////////////
 
-#include <algorithm>
-#include <boost/assign/list_of.hpp>
 #include <boost/format.hpp>
 #include <boost/program_options.hpp>
-#include <boost/tuple/tuple.hpp>
-#include <cmath>
-#include <cstdlib>
-#include <cxxabi.h>
+#include <cstring>
 #include <iostream>
-#include <list>
 #include <set>
 #include <sstream>
+#include <stdexcept>
 #include <string>
 #include <vector>
 
-#include "KrellInstitute/Messages/CUDA_data.h"
+#include <KrellInstitute/Messages/CUDA_data.h>
+
+#include <ArgoNavis/CUDA/stringify.hpp>
 
 #include "ToolAPI.hxx"
 
 using namespace boost;
-using namespace boost::assign;
-using namespace boost::tuples;
 using namespace OpenSpeedShop::Framework;
 using namespace std;
 
@@ -49,52 +44,6 @@
 /** Enumeration of the colors supported by colorize(). */
 enum Color { None, Black, Red, Green, Yellow, Blue, Magenta, Cyan, White };
 
-/** Type of container used to store ordered fields (key/value pairs). */
-typedef list<tuple<string, string> > Fields;
-
-/**
- * Type representing a byte count. Its only reason for existence is to
- * allow the Stringify<> template below to be specialized for byte counts
- * versus other integer values.
- */
-class ByteCount
-{
-public:
-    ByteCount(const uint64_t& value) : dm_value(value) { }
-    operator uint64_t() const { return dm_value; }
-private:
-    uint64_t dm_value;
-};
-
-/**
- * Type representing a clock rate. Its only reason for existence is to
- * allow the Stringify<> template below to be specialized for clock rates
- * versus other integer values.
- */
-class ClockRate
-{
-public:
-    ClockRate(const uint64_t& value) : dm_value(value) { }
-    operator uint64_t() const { return dm_value; }
-private:
-    uint64_t dm_value;
-};
-
-/**
- * Type representing a function name. Its only reason for existence is to
- * allow the Stringify<> template below to be specialized for function names
- * versus other strings.
- */
-class FunctionName
-{
-public:
-    FunctionName(const char* value) : dm_value(value) { }
-    FunctionName(const string& value) : dm_value(value) { }
-    operator string() const { return dm_value; }
-private:
-    string dm_value;
-};
-
 
 
 /**
@@ -154,704 +103,6 @@
 
 
 /**
- * Implementation of the stringify() function. This template is used to
- * circumvent the C++ prohibition against partial template specialization
- * of functions.
- */
-template <typename T>
-struct Stringify
-{
-    static string impl(const T& value)
-    {
-        return str(format("%1%") % value);
-    }
-};
-
-/** Convert the specified value to a string. */
-template <typename T>
-string stringify(const T& value)
-{
-    return Stringify<T>::impl(value);
-}
-
-
-
-template <>
-struct Stringify<bool>
-{
-    static string impl(const bool& value)
-    {
-        return value ? "true" : "false";
-    }
-};
-    
-template <>
-struct Stringify<uint64_t>
-{
-    static string impl(const uint64_t& value)
-    {
-        return str(format("%016X") % value);
-    }
-};
-
-template <typename T>
-struct Stringify<vector<T> >
-{
-    static string impl(const vector<T>& value)
-    {
-        stringstream stream;
-
-        stream << "[";
-        for (typename vector<T>::size_type i = 0; i < value.size(); ++i)
-        {
-            stream << value[i];
-            if (i < (value.size() - 1))
-            {
-                stream << ", ";
-            }
-        }
-        stream << "]";
-
-        return stream.str();
-    }
-};
-
-template <>
-struct Stringify<Fields>
-{
-    static string impl(const Fields& value)
-    {
-        stringstream stream;
-        
-        int n = 0;
-        
-        for (Fields::const_iterator i = value.begin(); i != value.end(); ++i)
-        {
-            n = max<int>(n, i->get<0>().size());
-        }
-        
-        for (Fields::const_iterator i = value.begin(); i != value.end(); ++i)
-        {
-            stream << "    ";
-            for (int j = 0; j < (n - i->get<0>().size()); ++j)
-            {
-                stream << " ";
-            }
-            stream << i->get<0>() << " = " << i->get<1>() << endl;
-        }
-        
-        return stream.str();
-    }
-};
-
-template <>
-struct Stringify<ByteCount>
-{
-    static string impl(const ByteCount& value)
-    {
-        const struct { const double value; const char* label; } kUnits[] = {
-            { 1024.0 * 1024.0 * 1024.0 * 1024.0, "TB" },
-            {          1024.0 * 1024.0 * 1024.0, "GB" },
-            {                   1024.0 * 1024.0, "MB" },
-            {                            1024.0, "KB" },
-            {                               0.0, NULL } // End-Of-Table
-        };
-        
-        double x = static_cast<double>(value);
-        string label = "Bytes";
-
-        for (int i = 0; kUnits[i].label != NULL; ++i)
-        {
-            if (static_cast<double>(value) >= kUnits[i].value)
-            {
-                x = static_cast<double>(value) / kUnits[i].value;
-                label = kUnits[i].label;
-                break;
-            }
-        }
-
-        return str(
-            (x == floor(x)) ?
-            (format("%1% %2%") % static_cast<uint64_t>(x) % label) :
-            (format("%1$0.1f %2%") % x % label)
-            );
-    }
-};
-
-template <>
-struct Stringify<ClockRate>
-{
-    static string impl(const ClockRate& value)
-    {
-        const struct { const double value; const char* label; } kUnits[] = {
-            { 1024.0 * 1024.0 * 1024.0 * 1024.0, "THz" },
-            {          1024.0 * 1024.0 * 1024.0, "GHz" },
-            {                   1024.0 * 1024.0, "MHz" },
-            {                            1024.0, "KHz" },
-            {                               0.0, NULL  } // End-Of-Table
-        };
-
-        double x = static_cast<double>(value);
-        string label = "Hz";
-
-        for (int i = 0; kUnits[i].label != NULL; ++i)
-        {
-            if (static_cast<double>(value) >= kUnits[i].value)
-            {
-                x = static_cast<double>(value) / kUnits[i].value;
-                label = kUnits[i].label;
-                break;
-            }
-        }
-
-        return str(
-            (x == floor(x)) ?
-            (format("%1% %2%") % static_cast<uint64_t>(x) % label) :
-            (format("%1$0.1f %2%") % x % label)
-            );
-    }
-};
-
-template <>
-struct Stringify<FunctionName>
-{
-    static string impl(const FunctionName& value)
-    {
-        string retval = value;
-
-        int status = -2;
-        char* demangled =  abi::__cxa_demangle(
-            retval.c_str(), NULL, NULL, &status
-            );
-
-        if (demangled != NULL)
-        {
-            if (status == 0)
-            {
-                retval = string(demangled);
-            }
-            free(demangled);
-        }
-
-        return retval;
-    }
-};
-
-template <>
-struct Stringify<CBTF_Protocol_FileName>
-{
-    static string impl(const CBTF_Protocol_FileName& value)
-    {
-        return str(format("%1% (%2%)") % 
-                   value.path % stringify<uint64_t>(value.checksum));
-    }
-};
-
-template <>
-struct Stringify<CUDA_CachePreference>
-{
-    static string impl(const CUDA_CachePreference& value)
-    {
-        switch (value)
-        {
-        case InvalidCachePreference: return "InvalidCachePreference";
-        case NoPreference: return "NoPreference";
-        case PreferShared: return "PreferShared";
-        case PreferCache: return "PreferCache";
-        case PreferEqual: return "PreferEqual";
-        }
-        return "?";
-    }
-};
-
-template <>
-struct Stringify<CUDA_CopyKind>
-{
-    static string impl(const CUDA_CopyKind& value)
-    {
-        switch (value)
-        {
-        case InvalidCopyKind: return "InvalidCopyKind";
-        case UnknownCopyKind: return "UnknownCopyKind";
-        case HostToDevice: return "HostToDevice";
-        case DeviceToHost: return "DeviceToHost";
-        case HostToArray: return "HostToArray";
-        case ArrayToHost: return "ArrayToHost";
-        case ArrayToArray: return "ArrayToArray";
-        case ArrayToDevice: return "ArrayToDevice";
-        case DeviceToArray: return "DeviceToArray";
-        case DeviceToDevice: return "DeviceToDevice";
-        case HostToHost: return "HostToHost";
-        }
-        return "?";
-    }
-};
-
-template <>
-struct Stringify<CUDA_MemoryKind>
-{
-    static string impl(const CUDA_MemoryKind& value)
-    {
-        switch (value)
-        {
-        case InvalidMemoryKind: return "InvalidMemoryKind";
-        case UnknownMemoryKind: return "UnknownMemoryKind";
-        case Pageable: return "Pageable";
-        case Pinned: return "Pinned";
-        case Device: return "Device";
-        case Array: return "Array";
-        }
-        return "?";
-    }
-};
-
-template <>
-struct Stringify<CUDA_MessageTypes>
-{
-    static string impl(const CUDA_MessageTypes& value)
-    {
-        switch (value)
-        {
-        case ContextInfo: return "ContextInfo";
-        case CopiedMemory: return "CopiedMemory";
-        case DeviceInfo: return "DeviceInfo";
-        case EnqueueRequest: return "EnqueueRequest";
-        case ExecutedKernel: return "ExecutedKernel";
-        case LoadedModule: return "LoadedModule";
-        case OverflowSamples: return "OverflowSamples";
-        case PeriodicSamples: return "PeriodicSamples";
-        case ResolvedFunction: return "ResolvedFunction";
-        case SamplingConfig: return "SamplingConfig";
-        case SetMemory: return "SetMemory";
-        case UnloadedModule: return "UnloadedModule";
-        }
-        return "?";
-    }
-};
-
-template <>
-struct Stringify<CUDA_RequestTypes>
-{
-    static string impl(const CUDA_RequestTypes& value)
-    {
-        switch (value)
-        {
-        case LaunchKernel: return "LaunchKernel";
-        case MemoryCopy: return "MemoryCopy";
-        case MemorySet: return "MemorySet";
-        }
-        return "?";
-    }
-};
-
-template<>
-struct Stringify<CUDA_EventDescription>
-{
-    static string impl(const CUDA_EventDescription& value)
-    {
-        return (value.threshold == 0) ? string(value.name) :
-            str(format("%1% (threshold=%2%)") % value.name % value.threshold);
-    }
-};
-
-template <>
-struct Stringify<CUDA_ContextInfo>
-{
-    static string impl(const CUDA_ContextInfo& value)
-    {
-        return stringify<Fields>(
-            tuple_list_of
-            ("context", stringify(value.context))
-            ("device", stringify(value.device))
-            ("compute_api", stringify(value.compute_api))
-            );
-    }
-};
-
-template <>
-struct Stringify<CUDA_CopiedMemory>
-{
-    static string impl(const CUDA_CopiedMemory& value)
-    {
-        return stringify<Fields>(
-            tuple_list_of
-            ("context", stringify(value.context))
-            ("stream", stringify(value.stream))
-            ("time_begin", stringify(value.time_begin))
-            ("time_end", stringify(value.time_end))
-            ("size", stringify<ByteCount>(value.size))
-            ("kind", stringify(value.kind))
-            ("source_kind", stringify(value.source_kind))
-            ("destination_kind", stringify(value.destination_kind))
-            ("asynchronous", stringify<bool>(value.asynchronous))
-            );
-    }
-};
-
-template <>
-struct Stringify<CUDA_DeviceInfo>
-{
-    static string impl(const CUDA_DeviceInfo& value)
-    {
-        return stringify<Fields>(
-            tuple_list_of
-            ("device", stringify(value.device))
-            ("name", stringify(value.name))
-            ("compute_capability",
-             stringify<vector<uint32_t> >(
-                 list_of
-                 (value.compute_capability[0])
-                 (value.compute_capability[1])
-                 ))
-            ("max_grid",
-             stringify<vector<uint32_t> >(
-                 list_of
-                 (value.max_grid[0])
-                 (value.max_grid[1])
-                 (value.max_grid[2])
-                 ))
-            ("max_block",
-             stringify<vector<uint32_t> >(
-                 list_of
-                 (value.max_block[0])
-                 (value.max_block[1])
-                 (value.max_block[2])
-                 ))
-            ("global_memory_bandwidth", 
-             stringify<ByteCount>(
-                 1024ULL * value.global_memory_bandwidth
-                 ) + "/Second")
-            ("global_memory_size",
-             stringify<ByteCount>(value.global_memory_size))
-            ("constant_memory_size",
-             stringify<ByteCount>(value.constant_memory_size))
-            ("l2_cache_size", stringify<ByteCount>(value.l2_cache_size))
-            ("threads_per_warp", stringify(value.threads_per_warp))
-            ("core_clock_rate", 
-             stringify<ClockRate>(1024ULL * value.core_clock_rate))
-            ("memcpy_engines", stringify(value.memcpy_engines))
-            ("multiprocessors", stringify(value.multiprocessors))
-            ("max_ipc", stringify(value.max_ipc))
-            ("max_warps_per_multiprocessor",
-             stringify(value.max_warps_per_multiprocessor))
-            ("max_blocks_per_multiprocessor",
-             stringify(value.max_blocks_per_multiprocessor))
-            ("max_registers_per_block",
-             stringify(value.max_registers_per_block))
-            ("max_shared_memory_per_block",
-             stringify<ByteCount>(value.max_shared_memory_per_block))
-            ("max_threads_per_block", stringify(value.max_threads_per_block))
-            );
-    }
-};
-
-template <>
-struct Stringify<CUDA_EnqueueRequest>
-{
-    static string impl(const CUDA_EnqueueRequest& value)
-    {
-        return stringify<Fields>(
-            tuple_list_of
-            ("type", stringify(value.type))
-            ("time", stringify(value.time))
-            ("context", stringify(value.context))
-            ("stream", stringify(value.stream))
-            ("call_site", stringify(value.call_site))
-            );
-    }
-};
-
-template <>
-struct Stringify<CUDA_ExecutedKernel>
-{
-    static string impl(const CUDA_ExecutedKernel& value)
-    {
-        return stringify<Fields>(
-            tuple_list_of
-            ("context", stringify(value.context))
-            ("stream", stringify(value.stream))
-            ("time_begin", stringify(value.time_begin))
-            ("time_end", stringify(value.time_end))
-            ("function", stringify<FunctionName>(value.function))
-            ("grid", 
-             stringify<vector<int32_t> >(
-                 list_of(value.grid[0])(value.grid[1])(value.grid[2])
-                 ))
-            ("block",
-             stringify<vector<int32_t> >(
-                 list_of(value.block[0])(value.block[1])(value.block[2])
-                 ))
-            ("cache_preference", stringify(value.cache_preference))
-            ("registers_per_thread", stringify(value.registers_per_thread))
-            ("static_shared_memory", 
-             stringify<ByteCount>(value.static_shared_memory))
-            ("dynamic_shared_memory",
-             stringify<ByteCount>(value.dynamic_shared_memory))
-            ("local_memory", stringify<ByteCount>(value.local_memory))
-            );
-    }
-};
-
-template <>
-struct Stringify<CUDA_LoadedModule>
-{
-    static string impl(const CUDA_LoadedModule& value)
-    {
-        return stringify<Fields>(
-            tuple_list_of
-            ("time", stringify(value.time))
-            ("module", stringify(value.module))
-            ("handle", stringify(value.handle))
-            );
-    }
-};
-
-template <>
-struct Stringify<CUDA_OverflowSamples>
-{
-    static string impl(const CUDA_OverflowSamples& value)
-    {
-        stringstream stream;
-
-        stream << stringify<Fields>(
-            tuple_list_of
-            ("time_begin", stringify(value.time_begin))
-            ("time_end", stringify(value.time_end))
-            );
-
-        stream << endl << "    pcs = ";
-        for (u_int i = 0; i < value.pcs.pcs_len; ++i)
-        {
-            if ((i % 4) == 0)
-            {
-                stream << endl << (format("[%1$4d] ") % i);
-            }
-            
-            stream << stringify(value.pcs.pcs_val[i]) << " ";
-        }
-        if ((value.pcs.pcs_len % 4) != 0)
-        {
-            stream << endl;
-        }
-
-        stream << endl << "    counts = ";
-        for (u_int i = 0; i < value.counts.counts_len; ++i)
-        {
-            if ((i % 4) == 0)
-            {
-                stream << endl << (format("[%1$4d] ") % i);
-            }
-            
-            stream << stringify(value.counts.counts_val[i]) << " ";
-        }
-        if ((value.counts.counts_len % 4) != 0)
-        {
-            stream << endl;
-        }
-        
-        return string();
-    }
-};
-
-template <>
-struct Stringify<CUDA_PeriodicSamples>
-{
-    static string impl(const CUDA_PeriodicSamples& value)
-    {
-        static int N[4] = { 0, 2, 3, 8 };
-
-        stringstream stream;
-        stream << "    deltas = " << endl;
-
-        for (u_int i = 0; i < value.deltas.deltas_len;)
-        {
-            uint8_t* ptr = &value.deltas.deltas_val[i];
-            
-            uint64_t delta = 0;
-            uint8_t encoding = ptr[0] >> 6;
-            if (encoding < 3)
-            {
-                delta = static_cast<uint64_t>(ptr[0]) & 0x3F;
-            }
-            else
-            {
-                delta = 0;
-            }            
-            for (int j = 0; j < N[encoding]; ++j)
-            {
-                delta <<= 8;
-                delta |= static_cast<uint64_t>(ptr[1 + j]);
-            }
-
-            stringstream bytes;
-            for (int j = 0; j < (1 + N[encoding]); ++j)
-            {
-                bytes << (format("%02X ") % static_cast<unsigned int>(ptr[j]));
-            }
-
-            stream << (format("    [%4d] %-27s (%s)") % i % bytes.str() %
-                       stringify(delta)) << endl;
-            
-            i += 1 + N[encoding];
-        }
-
-        return stream.str();
-    }
-};
-
-template <>
-struct Stringify<CUDA_ResolvedFunction>
-{
-    static string impl(const CUDA_ResolvedFunction& value)
-    {
-        return stringify<Fields>(
-            tuple_list_of
-            ("time", stringify(value.time))
-            ("module_handle", stringify(value.module_handle))
-            ("function", stringify<FunctionName>(value.function))
-            ("handle", stringify(value.handle))
-            );
-    }
-};
-
-template <>
-struct Stringify<CUDA_SamplingConfig>
-{
-    static string impl(const CUDA_SamplingConfig& value)
-    {
-        Fields fields = tuple_list_of("interval", stringify(value.interval));
-        
-        for (u_int i = 0; i < value.events.events_len; ++i)
-        {
-            fields.push_back(
-                tuple<string, string>(
-                    str(format("event %1%") % i),
-                    stringify(value.events.events_val[i])
-                    )
-                );
-        }
-
-        return stringify(fields);
-    }
-};
-
-template <>
-struct Stringify<CUDA_SetMemory>
-{
-    static string impl(const CUDA_SetMemory& value)
-    {
-        return stringify<Fields>(
-            tuple_list_of
-            ("context", stringify(value.context))
-            ("stream", stringify(value.stream))
-            ("time_begin", stringify(value.time_begin))
-            ("time_end", stringify(value.time_end))
-            ("size", stringify<ByteCount>(value.size))
-            );
-    }
-};
-
-template <>
-struct Stringify<CUDA_UnloadedModule>
-{
-    static string impl(const CUDA_UnloadedModule& value)
-    {
-        return stringify<Fields>(
-            tuple_list_of
-            ("time", stringify(value.time))
-            ("handle", stringify(value.handle))
-            );
-    }
-};
-
-template <>
-struct Stringify<CBTF_cuda_message>
-{
-    static string impl(const CBTF_cuda_message& value)
-    {
-        switch (value.type)
-        {
-        case ContextInfo:
-            return stringify(value.CBTF_cuda_message_u.context_info);
-            
-        case CopiedMemory:
-            return stringify(value.CBTF_cuda_message_u.copied_memory);
-            
-        case DeviceInfo:
-            return stringify(value.CBTF_cuda_message_u.device_info);
-            
-        case EnqueueRequest:
-            return stringify(value.CBTF_cuda_message_u.enqueue_request);
-            
-        case ExecutedKernel:
-            return stringify(value.CBTF_cuda_message_u.executed_kernel);
- 
-        case LoadedModule:
-            return stringify(value.CBTF_cuda_message_u.loaded_module);
-
-        case OverflowSamples:
-            return stringify(value.CBTF_cuda_message_u.overflow_samples);
-
-        case PeriodicSamples:
-            return stringify(value.CBTF_cuda_message_u.periodic_samples);
-
-        case ResolvedFunction:
-            return stringify(value.CBTF_cuda_message_u.resolved_function);
-
-        case SamplingConfig:
-            return stringify(value.CBTF_cuda_message_u.sampling_config);
-            
-        case SetMemory:
-            return stringify(value.CBTF_cuda_message_u.set_memory);
-            
-        case UnloadedModule:
-            return stringify(value.CBTF_cuda_message_u.unloaded_module);
-        }
-        
-        return string();
-    }
-};
-
-template <>
-struct Stringify<CBTF_cuda_data>
-{
-    static string impl(const CBTF_cuda_data& value)
-    {
-        stringstream stream;
-
-        for (u_int i = 0; i < value.messages.messages_len; ++i)
-        {
-            const CBTF_cuda_message& msg = value.messages.messages_val[i];
-            
-            stream << endl
-                   << (format("[%1$3d] %2%") % i % 
-                       stringify(static_cast<CUDA_MessageTypes>(msg.type))) 
-                   << endl << endl << stringify(msg);
-        }
-
-        stream << endl << "stack_traces = ";
-        for (u_int i = 0, n = 0;
-             i < value.stack_traces.stack_traces_len;
-             ++i, ++n)
-        {
-            if (((n % 4) == 0) ||
-                ((i > 0) && (value.stack_traces.stack_traces_val[i - 1] == 0)))
-            {
-                stream << endl << (format("[%1$4d] ") % i);
-                n = 0;
-            }
-            
-            stream << stringify(value.stack_traces.stack_traces_val[i]) << " ";
-        }
-        stream << endl;
-        
-        return stream.str();
-    }
-};
-
-
-
-/**
  * Parse the command-line arguments and dump the requested CUDA performance
  * data blobs.
  *
@@ -995,7 +246,7 @@
                          None, Green, Bold) << endl
              << colorize(str(format("Address Range: %1%") % range),
                          None, Green, Bold) << endl
-             << stringify(data);
+             << ArgoNavis::CUDA::stringify(data);
         
         xdr_free(reinterpret_cast<xdrproc_t>(xdr_CBTF_cuda_data),
                  reinterpret_cast<char*>(&data));        
diff -Naur OpenSpeedShop.ORIGINAL/plugins/collectors/CMakeLists.txt OpenSpeedShop.NEW-CUPTI/plugins/collectors/CMakeLists.txt
--- OpenSpeedShop.ORIGINAL/plugins/collectors/CMakeLists.txt	2016-01-19 21:05:46.000000000 -0600
+++ OpenSpeedShop.NEW-CUPTI/plugins/collectors/CMakeLists.txt	2016-02-26 16:07:37.774987654 -0600
@@ -1,5 +1,6 @@
 ################################################################################
 # Copyright (c) 2014-2016 Krell Institute. All Rights Reserved.
+# Copyright (c) 2015,2016 Argo Navis Technologies. All Rights Reserved.
 #
 # This program is free software; you can redistribute it and/or modify it under
 # the terms of the GNU General Public License as published by the Free Software
@@ -40,13 +41,8 @@
     add_subdirectory(pthreads)
     add_subdirectory(iop)
     add_subdirectory(mpip)
-
-    # If cuda related plugins and messages were found
-    # then build the client portion of the cuda collector.
-    if (CBTF-ARGO-MESSAGESCUDA_FOUND)
-        add_subdirectory(cuda)
-    endif()
 endif()
 
-
-
+if(BUILD_CUDA_SUPPORT)
+    add_subdirectory(cuda)
+endif()
diff -Naur OpenSpeedShop.ORIGINAL/plugins/collectors/CMakeLists.txt.~1.9.~ OpenSpeedShop.NEW-CUPTI/plugins/collectors/CMakeLists.txt.~1.9.~
--- OpenSpeedShop.ORIGINAL/plugins/collectors/CMakeLists.txt.~1.9.~	1969-12-31 18:00:00.000000000 -0600
+++ OpenSpeedShop.NEW-CUPTI/plugins/collectors/CMakeLists.txt.~1.9.~	2016-02-26 16:07:37.774987654 -0600
@@ -0,0 +1,52 @@
+################################################################################
+# Copyright (c) 2014-2016 Krell Institute. All Rights Reserved.
+#
+# This program is free software; you can redistribute it and/or modify it under
+# the terms of the GNU General Public License as published by the Free Software
+# Foundation; either version 2 of the License, or (at your option) any later
+# version.
+#
+# This program is distributed in the hope that it will be useful, but WITHOUT
+# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
+# details.
+#
+# You should have received a copy of the GNU General Public License along with
+# this program; if not, write to the Free Software Foundation, Inc., 59 Temple
+# Place, Suite 330, Boston, MA  02111-1307  USA
+################################################################################
+
+add_subdirectory(pcsamp)
+add_subdirectory(usertime)
+add_subdirectory(io)
+
+# FIXME iot has problems with syscall types on ARM
+if (NOT RUNTIME_PLATFORM MATCHES "arm")
+   add_subdirectory(iot)
+endif()
+
+add_subdirectory(hwc)
+add_subdirectory(hwcsamp)
+add_subdirectory(hwctime)
+add_subdirectory(mpi)
+add_subdirectory(mpit)
+add_subdirectory(fpe)
+if (VT_FOUND)
+    add_subdirectory(mpiotf)
+endif()
+
+if (INSTRUMENTOR MATCHES "cbtf")
+    add_subdirectory(mem)
+    add_subdirectory(pthreads)
+    add_subdirectory(iop)
+    add_subdirectory(mpip)
+
+    # If cuda related plugins and messages were found
+    # then build the client portion of the cuda collector.
+    if (CBTF-ARGO-MESSAGESCUDA_FOUND)
+        add_subdirectory(cuda)
+    endif()
+endif()
+
+
+
diff -Naur OpenSpeedShop.ORIGINAL/plugins/collectors/cuda/CMakeLists.txt OpenSpeedShop.NEW-CUPTI/plugins/collectors/cuda/CMakeLists.txt
--- OpenSpeedShop.ORIGINAL/plugins/collectors/cuda/CMakeLists.txt	2015-06-02 17:38:24.000000000 -0500
+++ OpenSpeedShop.NEW-CUPTI/plugins/collectors/cuda/CMakeLists.txt	2016-02-26 16:07:37.774987654 -0600
@@ -1,5 +1,6 @@
 ################################################################################
 # Copyright (c) 2014-2015 Krell Institute. All Rights Reserved.
+# Copyright (c) 2015,2016 Argo Navis Technologies. All Rights Reserved.
 #
 # This program is free software; you can redistribute it and/or modify it under
 # the terms of the GNU General Public License as published by the Free Software
@@ -16,42 +17,34 @@
 # Place, Suite 330, Boston, MA  02111-1307  USA
 ################################################################################
 
-set(PLUGIN_SOURCES
-	CUDACollector.hxx CUDACollector.cxx
-	CUDACountsDetail.hxx
-	CUDADeviceDetail.hxx
-	CUDAExecDetail.hxx
-	CUDAXferDetail.hxx
-    )
-
 add_library(cuda MODULE
-        ${PLUGIN_SOURCES}
-)
+    CUDACollector.hxx CUDACollector.cxx
+    CUDACountsDetail.hxx
+    CUDAExecDetail.hxx
+    CUDAXferDetail.hxx
+    )
 
 target_include_directories(cuda PUBLIC
-        ${CMAKE_CURRENT_SOURCE_DIR}
-        ${CMAKE_CURRENT_BINARY_DIR}
-        ${Boost_INCLUDE_DIRS}
-        ${CBTF_MESSAGES_INCLUDE_DIRS}
-        ${CBTF_MESSAGES_CUDA_INCLUDE_DIRS}
-        ${PROJECT_SOURCE_DIR}/libopenss-runtime
-        ${PROJECT_SOURCE_DIR}/libopenss-framework
-        ${CMAKE_CURRENT_BINARY_DIR}/../../../libopenss-runtime
-)
+    ${CMAKE_CURRENT_SOURCE_DIR}
+    ${PROJECT_SOURCE_DIR}/libopenss-framework
+    ${PROJECT_SOURCE_DIR}/libopenss-queries-cuda
+    ${Boost_INCLUDE_DIRS}
+    ${CBTF_MESSAGES_CUDA_INCLUDE_DIRS}
+    ${ARGONAVIS_BASE_INCLUDE_DIRS}
+    ${ARGONAVIS_CUDA_INCLUDE_DIRS}
+    )
 
 target_link_libraries(cuda
-	pthread
-	openss-framework
-        ${CBTF_MESSAGES_LIBRARIES}
-        ${CBTF_MESSAGES_CUDA_LIBRARIES}
+    openss-framework
+    openss-queries-cuda
+    ${CBTF_MESSAGES_CUDA_LIBRARIES}
+    ${ARGONAVIS_BASE_LIBRARIES}
+    ${ARGONAVIS_CUDA_LIBRARIES}
 	${CMAKE_DL_LIBS}
-)
-
-#set_target_properties(cuda PROPERTIES VERSION 1.1.0)
-
-install(TARGETS cuda
-	LIBRARY DESTINATION lib${LIB_SUFFIX}/openspeedshop
-)
+    )
 
 set_target_properties(cuda PROPERTIES PREFIX "")
-
+  
+install(TARGETS cuda
+    LIBRARY DESTINATION lib${LIB_SUFFIX}/openspeedshop
+    )
diff -Naur OpenSpeedShop.ORIGINAL/plugins/collectors/cuda/CUDACollector.cxx OpenSpeedShop.NEW-CUPTI/plugins/collectors/cuda/CUDACollector.cxx
--- OpenSpeedShop.ORIGINAL/plugins/collectors/cuda/CUDACollector.cxx	2014-10-27 21:41:24.000000000 -0500
+++ OpenSpeedShop.NEW-CUPTI/plugins/collectors/cuda/CUDACollector.cxx	2016-02-26 16:07:37.774987654 -0600
@@ -1,5 +1,5 @@
 ////////////////////////////////////////////////////////////////////////////////
-// Copyright (c) 2014 Argo Navis Technologies. All Rights Reserved.
+// Copyright (c) 2014-2016 Argo Navis Technologies. All Rights Reserved.
 //
 // This library is free software; you can redistribute it and/or modify it under
 // the terms of the GNU Lesser General Public License as published by the Free
@@ -18,25 +18,34 @@
 
 /** @file Definition of the CUDACollector class. */
 
-#include <boost/algorithm/string/predicate.hpp>
 #include <boost/bind.hpp>
 #include <boost/ref.hpp>
-#include <cstring>
+#include <map>
 #include <utility>
+#include <vector>
+
+#include <ArgoNavis/Base/TimeInterval.hpp>
+#include <ArgoNavis/CUDA/DataTransfer.hpp>
+#include <ArgoNavis/CUDA/DataTransferVisitor.hpp>
+#include <ArgoNavis/CUDA/KernelExecution.hpp>
+#include <ArgoNavis/CUDA/KernelExecutionVisitor.hpp>
  
 #include "AddressRange.hxx"
-#include "Assert.hxx"
-#include "Path.hxx"
+#include "Metadata.hxx"
 #include "StackTrace.hxx"
 #include "TimeInterval.hxx"
 
+#include "CUDAQueries.hxx"
+
 #include "CUDACollector.hxx"
 #include "CUDACountsDetail.hxx"
 #include "CUDAExecDetail.hxx"
 #include "CUDAXferDetail.hxx"
 
+using namespace ArgoNavis;
 using namespace boost;
 using namespace OpenSpeedShop::Framework;
+using namespace OpenSpeedShop::Queries;
 using namespace std;
 
 
@@ -50,91 +59,24 @@
     /** Type returned for the CUDA data transfer detail metrics. */
     typedef map<StackTrace, vector<CUDAXferDetail> > XferDetails;
 
-    /** Type of function invoked when visiting individual addresses. */
-    typedef function<void (uint64_t)> AddressVisitor;
-
-    /** Type of function invoked when visiting individual CUDA messages. */
-    typedef function<
-        void (const CBTF_cuda_data&, const CBTF_cuda_message&)
-        > MessageVisitor;
-
-    /** Convert a CUDA request call site into a stack trace. */
-    StackTrace toStackTrace(const Thread& thread, const Time& time,
-                            const vector<Address>& call_site)
-    {
-        StackTrace trace(thread, time);
-
-        // Initially include ALL frames for the call site of this CUDA request
-        for (int i = 0; i < call_site.size(); ++i)
-        {
-            trace.push_back(call_site[i]);
-        }
-        
-        //
-        // Trim all of the frames that preceeded the actual entry into main(),
-        // assuming, of course, that main() can actually be found.
-        //
-        
-        for (int i = 0; i < trace.size(); ++i)
-        {
-            pair<bool, Function> function = trace.getFunctionAt(i);
-            if (function.first && (function.second.getName() == "main"))
-            {
-                trace.erase(trace.begin() + i + 1, trace.end());
-                break;
-            }            
-        }
-
-        //
-        // Now iterate over the frames from main() towards the final call site,
-        // looking for the first frame that is part of the CUDA implementation
-        // or our collector. Trim the stack trace from that point all the way
-        // to the final call site.
-        //
-        
-        for (int i = trace.size(); i > 0; --i)
-        {
-            pair<bool, LinkedObject> linked_object = trace.getLinkedObjectAt(i);
-            if (linked_object.first)
-            {
-                Path base_name = linked_object.second.getPath().getBaseName();
-                if (starts_with(base_name, "cuda-collector") ||
-                    starts_with(base_name, "libcu"))
-                {
-                    trace.erase(trace.begin(), trace.begin() + i + 1);
-                    break;
-                }
-            }
-            
-            pair<bool, Function> function = trace.getFunctionAt(i);
-            if (function.first &&
-                starts_with(function.second.getName(), "__device_stub"))
-            {
-                trace.erase(trace.begin(), trace.begin() + i + 1);
-                break;
-            }
-        }
-
-        return trace;
-    }
-
     /**
      * Visitor used to compute [exec|xfer]_[exclusive|inclusive]_detail metrics.
      */
     template <typename T, typename U, bool Inclusive>
-    void computeDetails(const CUDA_EnqueueRequest& request,
-                        const T& completion,
-                        const vector<Address>& call_site,
-                        const SmartPtr<CUDADeviceDetail>& device,
+    bool computeDetails(const T& info,
+                        const CUDA::PerformanceData& data,
                         const Thread& thread,
                         const ExtentGroup& subextents,
-                        vector<U>& data)
+                        vector<U>& results)
     {
-        TimeInterval interval(Time(completion.time_begin),
-                              Time(completion.time_end));
+        TimeInterval interval = ConvertFromArgoNavis(
+            Base::TimeInterval(info.time_begin, info.time_end)
+            );
         
-        StackTrace trace = toStackTrace(
-            thread, interval.getBegin(), call_site
+        StackTrace trace = ConvertFromArgoNavis(
+            data.sites()[info.call_site],
+            thread,
+            ConvertFromArgoNavis(info.time)
             );
 
         for (StackTrace::const_iterator
@@ -157,34 +99,37 @@
                     (interval & subextents[*j].getTimeInterval()).getWidth()
                     ) / 1000000000.0;
 
-                typename U::iterator k = data[*j].insert(
+                typename U::iterator k = results[*j].insert(
                     make_pair(trace, typename U::mapped_type())
                     ).first;
 
                 typename U::mapped_type::value_type details(
-                    t_intersection, device, request, completion
+                    info, data.devices()[info.device], t_intersection
                     );
                 
                 k->second.push_back(details);
             }
         }
+
+        return true; // Always continue the visitation        
     }
     
     /** Visitor used to compute [exec|xfer]_time metrics. */
     template <typename T>
-    void computeTime(const CUDA_EnqueueRequest& request,
-                     const T& completion,
-                     const vector<Address>& call_site,
-                     const SmartPtr<CUDADeviceDetail>& device,
+    bool computeTime(const T& info,
+                     const CUDA::PerformanceData& data,
                      const Thread& thread,
                      const ExtentGroup& subextents,
-                     vector<double>& data)
+                     vector<double>& results)
     {
-        TimeInterval interval(Time(completion.time_begin),
-                              Time(completion.time_end));
+        TimeInterval interval = ConvertFromArgoNavis(
+            Base::TimeInterval(info.time_begin, info.time_end)
+            );
         
-        StackTrace trace = toStackTrace(
-            thread, interval.getBegin(), call_site
+        StackTrace trace = ConvertFromArgoNavis(
+            data.sites()[info.call_site],
+            thread,
+            ConvertFromArgoNavis(info.time)
             );
         
         set<ExtentGroup::size_type> intersection = 
@@ -199,183 +144,10 @@
                 (interval & subextents[*i].getTimeInterval()).getWidth()
                 ) / 1000000000.0;
             
-            data[*i] += t_intersection;
-        }
-    }
-    
-    /**
-     * Visitor extracting the unique addresses referenced by a CUDA message.
-     */
-    void extractUniqueAddresses(const CBTF_cuda_data& data,
-                                const CBTF_cuda_message& message,
-                                AddressVisitor& visitor)
-    {
-        switch (message.type)
-        {
-            
-        case EnqueueRequest:
-            {
-                const CUDA_EnqueueRequest& msg = 
-                    message.CBTF_cuda_message_u.enqueue_request;
-                
-                for (uint32_t i = msg.call_site;
-                     (i < data.stack_traces.stack_traces_len) &&
-                         (data.stack_traces.stack_traces_val[i] != 0);
-                     ++i)
-                {
-                    visitor(data.stack_traces.stack_traces_val[i]);
-                }
-            }
-            break;
-            
-        case OverflowSamples:
-            {
-                const CUDA_OverflowSamples& msg =
-                    message.CBTF_cuda_message_u.overflow_samples;
-                
-                for (uint32_t i = 0; i < msg.pcs.pcs_len; ++i)
-                {
-                    visitor(msg.pcs.pcs_val[i]);
-                }
-            }
-            break;
-            
-        default:
-            break;
+            results[*i] += t_intersection;
         }
-    }
 
-    /**
-     * Visitor extracting periodic samples contained in a CUDA message.
-     */
-    void extractPeriodicSamples(const CBTF_cuda_data& data,
-                                const CBTF_cuda_message& message,
-                                const TimeInterval& query_interval,
-                                vector<string> event_names,
-                                vector<uint64_t> event_counts)
-    {
-        static int N[4] = { 0, 2, 3, 8 };
-        
-        switch (message.type)
-        {
-            
-        case PeriodicSamples:
-            {
-                const CUDA_PeriodicSamples& msg =
-                    message.CBTF_cuda_message_u.periodic_samples;
-
-                Assert(event_counts.size() > 0);
-
-                bool is_first = true;
-
-                const uint8_t* ptr = msg.deltas.deltas_val;
-                vector<uint64_t> deltas(1 + event_counts.size());
-
-                uint64_t t_previous = 0;
-                                
-                while (ptr < (msg.deltas.deltas_val + msg.deltas.deltas_len))
-                {
-                    for (int d = 0; d < deltas.size(); ++d)
-                    {
-                        uint8_t encoding = ptr[0] >> 6;
-
-                        if (encoding < 3)
-                        {
-                            deltas[d] = static_cast<uint64_t>(ptr[0]) & 0x3F;
-                        }
-                        else
-                        {
-                            deltas[d] = 0;
-                        }
-                        
-                        for (int i = 0; i < N[encoding]; ++i)
-                        {
-                            deltas[d] <<= 8;
-                            deltas[d] |= static_cast<uint64_t>(ptr[1 + i]);
-                        }
-
-                        ptr += 1 + N[encoding];
-                    }
-
-                    if (is_first)
-                    {
-                        is_first = false;
-                        t_previous = deltas[0];
-                    }
-                    else
-                    {                    
-                        TimeInterval sample_interval(
-                            t_previous, t_previous + deltas[0]
-                            );
-
-                        TimeInterval intersection = 
-                            query_interval & sample_interval;
-
-                        for (int d = 1; d < deltas.size(); ++d)
-                        {
-                            event_counts[d - 1] +=
-                                deltas[d] * intersection.getWidth() /
-                                sample_interval.getWidth();
-                        }
-                        
-                        t_previous += deltas[0];
-                    }
-                }
-            }
-            break;
-            
-        case SamplingConfig:
-            {
-                const CUDA_SamplingConfig& msg =
-                    message.CBTF_cuda_message_u.sampling_config;
-
-                Assert(event_names.size() == 0);
-                Assert(event_counts.size() == 0);
-
-                for (u_int i = 0; i < msg.events.events_len; ++i)
-                {
-                    event_names.push_back(msg.events.events_val[i].name);
-                    event_counts.push_back(0);
-                }
-            }
-            break;
-            
-        default:
-            break;
-        }
-    }
-    
-    /** Visitor used to insert an address into a buffer of unique addresses. */
-    void insertIntoAddressBuffer(uint64_t address, PCBuffer& buffer)
-    {
-        UpdatePCBuffer(address, &buffer);
-    }
-
-    /** Visitor used to insert an address into a set of unique addresses. */
-    void insertIntoAddressSet(uint64_t address, set<Address>& addresses)
-    {
-        addresses.insert(address);
-    }
-
-    /**
-     * Visit the individual CUDA messages packed within a performance data blob.
-     */
-    void visit(const Blob& blob, const MessageVisitor& visitor)
-    {
-        CBTF_cuda_data data;
-        memset(&data, 0, sizeof(data));
-        
-        blob.getXDRDecoding(
-            reinterpret_cast<xdrproc_t>(xdr_CBTF_cuda_data), &data
-            );
-        
-        for (u_int i = 0; i < data.messages.messages_len; ++i)
-        {
-            visitor(data, data.messages.messages_val[i]);
-        }
-        
-        xdr_free(reinterpret_cast<xdrproc_t>(xdr_CBTF_cuda_data),
-                 reinterpret_cast<char*>(&data));        
+        return true; // Always continue the visitation
     }
     
 } // namespace <anonymous>
@@ -404,14 +176,14 @@
  */
 CUDACollector::CUDACollector() :
     CollectorImpl("cuda", "CUDA",
-                  "Intercepts all calls to CUDA memory copy/set and kernel "
+                  "Intercepts all calls to CUDA data transfers and kernel "
                   "executions and records, for each call, the current stack "
                   "trace and start/end time, as well as additional relevant "
                   "information depending on the operation. In addition, has "
                   "the ability to periodically sample hardware event counts "
                   "via PAPI for both the CPU and GPU."),
-    dm_contexts(),
-    dm_devices(),
+    dm_current(),
+    dm_data(),
     dm_threads()
 {
     // Declare our metrics
@@ -572,144 +344,143 @@
                                     void* ptr) const
 {
     //
-    // The "count_exclusive_details" metric is handled different from all the
-    // others in that it doesn't require the thread-specific data, and it can't
-    // be computed for specific address ranges (only time intervals). So short-
-    // circuit everything else for this metric...
-    //
-    
-    if (metric == "count_exclusive_details")
-    {
-        vector<string> event_names;
-        vector<uint64_t> event_counts;
-        
-        MessageVisitor visitor = bind(
-            extractPeriodicSamples, _1, _2,
-            cref(subextents[0].getTimeInterval()),
-            ref(event_names), ref(event_counts)
-            );
-        visit(blob, visitor);
-
-        vector<CUDACountsDetail>& data =
-            *reinterpret_cast<vector<CUDACountsDetail>*>(ptr);
-
-        data[0] = CUDACountsDetail(event_names, event_counts);
-
-        return;
-    }
-
-    //
-    // Locate the thread-specific data belonging to the thread for which this
-    // performance data blob was gathered, creating new thread-specific data
-    // when necessary.
-    //
-
-    map<Thread, ThreadSpecificData>::iterator t = dm_threads.find(thread);
-    
-    if (t == dm_threads.end())
-    {
-        t = dm_threads.insert(make_pair(thread, ThreadSpecificData())).first;
-    }
-
-    //
     // CUDA allows requests to be executed asynchronously. Thus the issue and
     // completion of a given request can be found within 2 separate blobs. As
     // a consequence, it is necessary to have a stateful collector in order to
     // correctly compute certain metrics. But the collector API isn't designed
     // for that. I.e. there is no direct indication of whether a given call to
     // getMetricValues() is part of a previous or new metric computation. The
-    // fix employed here is to assume:
+    // fix employed here is to:
+    //
+    //     * Always find and pre-process all of the CUDA performance data
+    //       blobs for a given thread the first time that thread is seen.
     //
-    //     * Each blob is seen once per metric computation.
-    //     * A blob can be unique identified by its extent.
-    //     * Blobs are visited in time order.
+    //     * Compare the thread and subextents of each getMetricValues()
+    //       call against the same values from the previous call and, when
+    //       they differ, assume a new metric computation is beginning.
     //
-    // And so when a given blob's extent is seen a 2nd time, it can be assumed
-    // that a new metric computation has begun and the thread-specific data for
-    // the thread should be cleared to reset the state.
+    //     * Generate all of the metric values immediately upon the first
+    //       call to getMetricValues() for a new metric computation.
     //
 
-    if (t->second.extents.find(extent) != t->second.extents.end())
+    if (dm_current &&
+        (dm_current->first == thread) && (dm_current->second == subextents))
     {
-        t->second = ThreadSpecificData();
+        // No need to do anything else if the thread and subextents
+        // for this call matches that of the previous call (if any).
+        return;
+    }
+    
+    if (dm_threads.find(thread) == dm_threads.end())
+    {
+        dm_threads.insert(thread);
+        GetCUDAPerformanceData(collector, thread, dm_data);
     }
 
-    t->second.extents.insert(extent);
-
-    //
-    // Compute the requested metric by processing the individual CUDA messages
-    // in the specified performance data blob. This involves constructing a set
-    // of appropriate visitors, chaining them together, and then applying them.
-    //
-
-    ExecutedKernelVisitor executed_kernel_visitor;    
-    MemoryCopyVisitor memory_copy_visitor;
-    MemorySetVisitor memory_set_visitor;
+    dm_current = make_pair(thread, subextents);
 
-    MessageVisitor request_visitor = bind(
-        &CUDACollector::handleRequests, this, _1, _2,
-        ref(t->second), ref(executed_kernel_visitor),
-        ref(memory_copy_visitor), ref(memory_set_visitor)
-        );
+    if (metric == "count_exclusive_details")
+    {
+        vector<CUDACountsDetail>& data =
+            *reinterpret_cast<vector<CUDACountsDetail>*>(ptr);
+        
+        data[0] = CUDACountsDetail(
+            dm_data.counters(),
+            dm_data.counts(
+                ConvertToArgoNavis(thread),
+                ConvertToArgoNavis(subextents[0].getTimeInterval())
+                )
+            );
+    }
 
-    if (metric == "exec_time")
+    else if (metric == "exec_time")
     {
-        executed_kernel_visitor = bind(
-            &computeTime<CUDA_ExecutedKernel>, _1, _2, _3, _4, cref(thread),
-            cref(subextents), ref(*reinterpret_cast<vector<double>*>(ptr))
+        CUDA::KernelExecutionVisitor visitor = bind(
+            &computeTime<CUDA::KernelExecution>,
+            _1, cref(dm_data), cref(thread), cref(subextents),
+            ref(*reinterpret_cast<vector<double>*>(ptr))
             );
         
-        visit(blob, request_visitor);
+        dm_data.visitKernelExecutions(
+            ConvertToArgoNavis(thread),
+            ConvertToArgoNavis(subextents.getBounds().getTimeInterval()),
+            visitor
+            );
     }
+
     else if (metric == "exec_inclusive_details")
     {
-        executed_kernel_visitor = bind(
-            &computeDetails<CUDA_ExecutedKernel, ExecDetails, true>,
-            _1, _2, _3, _4, cref(thread), cref(subextents),
+        CUDA::KernelExecutionVisitor visitor = bind(
+            &computeDetails<CUDA::KernelExecution, ExecDetails, true>,
+            _1, cref(dm_data), cref(thread), cref(subextents),
             ref(*reinterpret_cast<vector<ExecDetails>*>(ptr))
             );
         
-        visit(blob, request_visitor);
+        dm_data.visitKernelExecutions(
+            ConvertToArgoNavis(thread),
+            ConvertToArgoNavis(subextents.getBounds().getTimeInterval()),
+            visitor
+            );
     }
+
     else if (metric == "exec_exclusive_details")
     {
-        executed_kernel_visitor = bind(
-            &computeDetails<CUDA_ExecutedKernel, ExecDetails, false>,
-            _1, _2, _3, _4, cref(thread), cref(subextents),
+        CUDA::KernelExecutionVisitor visitor = bind(
+            &computeDetails<CUDA::KernelExecution, ExecDetails, false>,
+            _1, cref(dm_data), cref(thread), cref(subextents),
             ref(*reinterpret_cast<vector<ExecDetails>*>(ptr))
             );
         
-        visit(blob, request_visitor);
+        dm_data.visitKernelExecutions(
+            ConvertToArgoNavis(thread),
+            ConvertToArgoNavis(subextents.getBounds().getTimeInterval()),
+            visitor
+            );
     }
 
     else if (metric == "xfer_time")
     {
-        memory_copy_visitor = bind(
-            &computeTime<CUDA_CopiedMemory>, _1, _2, _3, _4, cref(thread),
-            cref(subextents), ref(*reinterpret_cast<vector<double>*>(ptr))
+        CUDA::DataTransferVisitor visitor = bind(
+            &computeTime<CUDA::DataTransfer>, _1,
+            cref(dm_data), cref(thread), cref(subextents),
+            ref(*reinterpret_cast<vector<double>*>(ptr))
             );
         
-        visit(blob, request_visitor);
+        dm_data.visitDataTransfers(
+            ConvertToArgoNavis(thread),
+            ConvertToArgoNavis(subextents.getBounds().getTimeInterval()),
+            visitor
+            );
     }
+
     else if (metric == "xfer_inclusive_details")
     {
-        memory_copy_visitor = bind(
-            &computeDetails<CUDA_CopiedMemory, XferDetails, true>,
-            _1, _2, _3, _4, cref(thread), cref(subextents),
+        CUDA::DataTransferVisitor visitor = bind(
+            &computeDetails<CUDA::DataTransfer, XferDetails, true>,
+            _1, cref(dm_data), cref(thread), cref(subextents),
             ref(*reinterpret_cast<vector<XferDetails>*>(ptr))
             );
         
-        visit(blob, request_visitor);
+        dm_data.visitDataTransfers(
+            ConvertToArgoNavis(thread),
+            ConvertToArgoNavis(subextents.getBounds().getTimeInterval()),
+            visitor
+            );
     }
+
     else if (metric == "xfer_exclusive_details")
     {
-        memory_copy_visitor = bind(
-            &computeDetails<CUDA_CopiedMemory, XferDetails, false>,
-            _1, _2, _3, _4, cref(thread), cref(subextents),
+        CUDA::DataTransferVisitor visitor = bind(
+            &computeDetails<CUDA::DataTransfer, XferDetails, false>,
+            _1, cref(dm_data), cref(thread), cref(subextents),
             ref(*reinterpret_cast<vector<XferDetails>*>(ptr))
             );
         
-        visit(blob, request_visitor);
+        dm_data.visitDataTransfers(
+            ConvertToArgoNavis(thread),
+            ConvertToArgoNavis(subextents.getBounds().getTimeInterval()),
+            visitor
+            );
     }
 }
 
@@ -729,11 +500,7 @@
                                       const Blob& blob,
                                       PCBuffer* buffer) const
 {
-    AddressVisitor inserter = bind(insertIntoAddressBuffer, _1, ref(*buffer));
-    MessageVisitor visitor = bind(
-        extractUniqueAddresses, _1, _2, ref(inserter)
-        );
-    visit(blob, visitor);
+    GetCUDAUniquePCs(blob, buffer);
 }
 
 
@@ -752,188 +519,5 @@
                                       const Blob& blob,
                                       set<Address>& addresses) const
 {
-    AddressVisitor inserter = bind(insertIntoAddressSet, _1, ref(addresses));
-    MessageVisitor visitor = bind(
-        extractUniqueAddresses, _1, _2, ref(inserter)
-        );
-    visit(blob, visitor);
-}
-
-
-
-/**
- * Visitor handling an asynchronously executed CUDA request described by a
- * CUDA message. Manages pushing individual requests onto, and popping them
- * off of, the given thread-specific data's table of pending requests. Also
- * manages the mapping of contexts to their corresponding device details.
- */
-void CUDACollector::handleRequests(
-    const CBTF_cuda_data& data, const CBTF_cuda_message& message,
-    ThreadSpecificData& tsd,
-    ExecutedKernelVisitor& executed_kernel_visitor,
-    MemoryCopyVisitor& memory_copy_visitor,
-    MemorySetVisitor& memory_set_visitor
-    ) const
-{
-    switch (message.type)
-    {
-
-    case ContextInfo:
-        {
-            const CUDA_ContextInfo& msg =
-                message.CBTF_cuda_message_u.context_info;
-            
-            dm_contexts.insert(make_pair(msg.context, msg.device));
-        }
-        break;
-
-    case CopiedMemory:
-        {
-            const CUDA_CopiedMemory& msg =
-                message.CBTF_cuda_message_u.copied_memory;
-            
-            for (list<CUDACollector::Request>::iterator 
-                     i = tsd.requests.begin(); i != tsd.requests.end(); ++i)
-            {
-                if ((i->message.type == MemoryCopy) &&
-                    (i->message.context == msg.context) &&
-                    (i->message.stream == msg.stream))
-                {
-                    if (memory_copy_visitor)
-                    {
-                        memory_copy_visitor(
-                            cref(i->message),
-                            cref(msg),
-                            cref(i->call_site),
-                            cref(getDeviceDetail(msg.context))
-                            );
-                    }
-                    tsd.requests.erase(i);
-                    break;
-                }
-            }
-        }
-        break;
-
-    case DeviceInfo:
-        {
-            const CUDA_DeviceInfo& msg =
-                message.CBTF_cuda_message_u.device_info;
-
-            if (dm_devices.find(msg.device) != dm_devices.end())
-            {
-                dm_devices.insert(make_pair(
-                    msg.device,
-                    SmartPtr<CUDADeviceDetail>(new CUDADeviceDetail(msg))
-                    ));
-            }
-        }
-        break;
-        
-    case EnqueueRequest:
-        {
-            const CUDA_EnqueueRequest& msg = 
-                message.CBTF_cuda_message_u.enqueue_request;
-            
-            CUDACollector::Request request;
-            request.message = msg;
-            
-            for (uint32_t i = msg.call_site;
-                 (i < data.stack_traces.stack_traces_len) &&
-                     (data.stack_traces.stack_traces_val[i] != 0);
-                 ++i)
-            {
-                request.call_site.push_back(
-                    data.stack_traces.stack_traces_val[i]
-                    );
-            }
-            
-            tsd.requests.push_back(request);
-        }
-        break;
-        
-    case ExecutedKernel:
-        {
-            const CUDA_ExecutedKernel& msg =
-                message.CBTF_cuda_message_u.executed_kernel;
-            
-            for (list<CUDACollector::Request>::iterator 
-                     i = tsd.requests.begin(); i != tsd.requests.end(); ++i)
-            {
-                if ((i->message.type == LaunchKernel) &&
-                    (i->message.context == msg.context) &&
-                    (i->message.stream == msg.stream))
-                {
-                    if (executed_kernel_visitor)
-                    {
-                        executed_kernel_visitor(
-                            cref(i->message),
-                            cref(msg),
-                            cref(i->call_site),
-                            cref(getDeviceDetail(msg.context))
-                            );
-                    }
-                    tsd.requests.erase(i);
-                    break;
-                }
-            }
-        }
-        break;
-        
-    case SetMemory:
-        {
-            const CUDA_SetMemory& msg =
-                message.CBTF_cuda_message_u.set_memory;
-            
-            for (list<CUDACollector::Request>::iterator 
-                     i = tsd.requests.begin(); i != tsd.requests.end(); ++i)
-            {
-                if ((i->message.type == MemorySet) &&
-                    (i->message.context == msg.context) &&
-                    (i->message.stream == msg.stream))
-                {
-                    if (memory_set_visitor)
-                    {
-                        memory_set_visitor(
-                            cref(i->message),
-                            cref(msg),
-                            cref(i->call_site),
-                            cref(getDeviceDetail(msg.context))
-                            );
-                    }
-                    tsd.requests.erase(i);
-                    break;
-                }
-            }
-        }
-        break;
-        
-    default:
-        break;
-    }
-}
-
-
-
-/** Get the device details (if any) corresponding to the specified context. */
-SmartPtr<CUDADeviceDetail> CUDACollector::getDeviceDetail(
-    const Address& context
-    ) const
-{
-    map<Address, unsigned int>::const_iterator i =  dm_contexts.find(context);
-    
-    if (i == dm_contexts.end())
-    {
-        return SmartPtr<CUDADeviceDetail>();
-    }
-    
-    map<unsigned int, SmartPtr<CUDADeviceDetail> >::const_iterator j = 
-        dm_devices.find(i->second);
-
-    if (j == dm_devices.end())
-    {
-        return SmartPtr<CUDADeviceDetail>();
-    }
-
-    return j->second;
+    GetCUDAUniquePCs(blob, addresses);
 }
diff -Naur OpenSpeedShop.ORIGINAL/plugins/collectors/cuda/CUDACollector.hxx OpenSpeedShop.NEW-CUPTI/plugins/collectors/cuda/CUDACollector.hxx
--- OpenSpeedShop.ORIGINAL/plugins/collectors/cuda/CUDACollector.hxx	2014-10-13 20:36:05.000000000 -0500
+++ OpenSpeedShop.NEW-CUPTI/plugins/collectors/cuda/CUDACollector.hxx	2016-02-26 16:07:37.774987654 -0600
@@ -1,5 +1,5 @@
 ////////////////////////////////////////////////////////////////////////////////
-// Copyright (c) 2014 Argo Navis Technologies. All Rights Reserved.
+// Copyright (c) 2014-2016 Argo Navis Technologies. All Rights Reserved.
 //
 // This library is free software; you can redistribute it and/or modify it under
 // the terms of the GNU Lesser General Public License as published by the Free
@@ -24,35 +24,29 @@
 #include "config.h"
 #endif
 
-#include <boost/function.hpp>
-#include <list>
-#include <map>
+#include <boost/optional.hpp>
 #include <set>
 #include <string>
-#include <vector>
+#include <utility>
 
-#include "KrellInstitute/Messages/CUDA_data.h"
+#include <ArgoNavis/CUDA/PerformanceData.hpp>
 
 #include "Address.hxx"
 #include "Blob.hxx"
 #include "Collector.hxx"
 #include "CollectorImpl.hxx"
-#include "Collector.hxx"
 #include "Extent.hxx"
 #include "ExtentGroup.hxx"
 #include "PCBuffer.hxx"
-#include "SmartPtr.hxx"
 #include "Thread.hxx"
 #include "ThreadGroup.hxx"
 
-#include "CUDADeviceDetail.hxx"
-
 namespace OpenSpeedShop { namespace Framework {
 
     /**
      * CUDA collector.
      *
-     * Intercepts all calls to CUDA memory copy/set and kernel executions and
+     * Intercepts all calls to CUDA data transfers and kernel executions and
      * records, for each call, the current stack trace and start/end time, as
      * well as additional relevant information depending on the operation. In
      * addition, has the ability to periodically sample hardware event counts
@@ -90,76 +84,15 @@
 
     private:
 
-        /** Type of function invoked to process a completed kernel execution. */
-        typedef boost::function<
-            void (const CUDA_EnqueueRequest&,
-                  const CUDA_ExecutedKernel&,
-                  const std::vector<Address>&,
-                  const SmartPtr<CUDADeviceDetail>&)
-            > ExecutedKernelVisitor;
-
-        /** Type of function invoked to process a completed memory copy. */
-        typedef boost::function<
-            void (const CUDA_EnqueueRequest&,
-                  const CUDA_CopiedMemory&,
-                  const std::vector<Address>&,
-                  const SmartPtr<CUDADeviceDetail>&)
-            > MemoryCopyVisitor;
-
-        /** Type of function invoked to process a completed memory set. */
-        typedef boost::function<
-            void (const CUDA_EnqueueRequest&,
-                  const CUDA_SetMemory&,
-                  const std::vector<Address>&,
-                  const SmartPtr<CUDADeviceDetail>&)
-            > MemorySetVisitor;
-
-        /**
-         * Plain old data (POD) structure describing a single pending request.
-         *
-         * @sa http://en.wikipedia.org/wiki/Plain_old_data_structure
-         */
-        struct Request
-        {
-            /** Original CUDA message describing the enqueued request. */
-            CUDA_EnqueueRequest message;
-            
-            /** Call site of the enqueued request. */
-            std::vector<Address> call_site;
-        };
-        
-        /**
-         * Plain old data (POD) structure holding thread-specific data.
-         *
-         * @sa http://en.wikipedia.org/wiki/Plain_old_data_structure
-         */
-        struct ThreadSpecificData
-        {
-            /** Set of previously-seen extents. */
-            std::set<Extent> extents;
-
-            /** Table of pending requests. */
-            std::list<Request> requests;
-        };
-        
-        void handleRequests(const CBTF_cuda_data&,
-                            const CBTF_cuda_message&,
-                            ThreadSpecificData&,
-                            ExecutedKernelVisitor&,
-                            MemoryCopyVisitor&,
-                            MemorySetVisitor&) const;
-        
-        SmartPtr<CUDADeviceDetail> getDeviceDetail(const Address&) const;
-        
-        /** Device for all known contexts. */
-        mutable std::map<Address, unsigned int> dm_contexts;
-        
-        /** Device details for each known device. */
-        mutable std::map<unsigned int, SmartPtr<CUDADeviceDetail> > dm_devices;
-        
-        /** Thread-specific data for all known threads. */
-        mutable std::map<Thread, ThreadSpecificData> dm_threads;
+        /** Current (if any) query's thread and subextents. */
+        mutable boost::optional<std::pair<Thread, ExtentGroup> > dm_current;
+
+        /** CUDA performance data for all known threads. */
+        mutable ArgoNavis::CUDA::PerformanceData dm_data;
         
+        /** Set of all known threads. */
+        mutable std::set<Thread> dm_threads;
+                
     }; // class CUDACollector
         
 } } // namespace OpenSpeedShop::Framework
diff -Naur OpenSpeedShop.ORIGINAL/plugins/collectors/cuda/CUDACountsDetail.hxx OpenSpeedShop.NEW-CUPTI/plugins/collectors/cuda/CUDACountsDetail.hxx
--- OpenSpeedShop.ORIGINAL/plugins/collectors/cuda/CUDACountsDetail.hxx	2014-10-27 21:41:24.000000000 -0500
+++ OpenSpeedShop.NEW-CUPTI/plugins/collectors/cuda/CUDACountsDetail.hxx	2016-02-26 16:07:37.774987654 -0600
@@ -1,5 +1,5 @@
 ////////////////////////////////////////////////////////////////////////////////
-// Copyright (c) 2014 Argo Navis Technologies. All Rights Reserved.
+// Copyright (c) 2014-2016 Argo Navis Technologies. All Rights Reserved.
 //
 // This library is free software; you can redistribute it and/or modify it under
 // the terms of the GNU Lesser General Public License as published by the Free
@@ -24,7 +24,7 @@
 #include "config.h"
 #endif
 
-#include <stdint.h>
+#include <boost/cstdint.hpp>
 #include <string>
 #include <vector>
 
@@ -48,7 +48,7 @@
        
         /** Constructor from event names and counts. */
         CUDACountsDetail(const std::vector<std::string>& event_names,
-                         const std::vector<uint64_t>& event_counts) :
+                         const std::vector<boost::uint64_t>& event_counts) :
             dm_event_names(event_names),
             dm_event_counts(event_counts)
         {
@@ -69,7 +69,7 @@
         }
         
         /** Count for the specified event. */
-        uint64_t getEventCount(unsigned int event) const
+        boost::uint64_t getEventCount(unsigned int event) const
         {
             Assert(event < dm_event_counts.size());
             return dm_event_counts[event];
@@ -81,7 +81,7 @@
         std::vector<std::string> dm_event_names;
         
         /** Counts for all events. */
-        std::vector<uint64_t> dm_event_counts;
+        std::vector<boost::uint64_t> dm_event_counts;
         
     }; // class CUDACountsDetail
         
diff -Naur OpenSpeedShop.ORIGINAL/plugins/collectors/cuda/CUDADeviceDetail.hxx OpenSpeedShop.NEW-CUPTI/plugins/collectors/cuda/CUDADeviceDetail.hxx
--- OpenSpeedShop.ORIGINAL/plugins/collectors/cuda/CUDADeviceDetail.hxx	2014-10-13 20:56:33.000000000 -0500
+++ OpenSpeedShop.NEW-CUPTI/plugins/collectors/cuda/CUDADeviceDetail.hxx	1969-12-31 18:00:00.000000000 -0600
@@ -1,183 +0,0 @@
-////////////////////////////////////////////////////////////////////////////////
-// Copyright (c) 2014 Argo Navis Technologies. All Rights Reserved.
-//
-// This library is free software; you can redistribute it and/or modify it under
-// the terms of the GNU Lesser General Public License as published by the Free
-// Software Foundation; either version 2.1 of the License, or (at your option)
-// any later version.
-//
-// This library is distributed in the hope that it will be useful, but WITHOUT
-// ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
-// FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more
-// details.
-//
-// You should have received a copy of the GNU Lesser General Public License
-// along with this library; if not, write to the Free Software Foundation, Inc.,
-// 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
-////////////////////////////////////////////////////////////////////////////////
-
-/** @file Declaration and definition of the CUDADeviceDetail class. */
-
-#pragma once
-
-#ifdef HAVE_CONFIG_H
-#include "config.h"
-#endif
-
-#include <boost/tuple/tuple.hpp>
-#include <string>
-
-#include "KrellInstitute/Messages/CUDA_data.h"
-
-namespace OpenSpeedShop { namespace Framework {
-
-    /**
-     * CUDA device details.
-     *
-     * Encapsulate the device details associated with CUDA kernel executions
-     * and data transfers recorded by the CUDA collector.
-     */
-    class CUDADeviceDetail
-    {
-
-    public:
-
-        /** Vector of two unsigned integers. */
-        typedef boost::tuple<unsigned int, unsigned int> Vector2u;
-
-        /** Vector of three unsigned integers. */
-        typedef boost::tuple<unsigned int, unsigned int, unsigned int> Vector3u;
-
-        /** Constructor from a raw CUDA message. */
-        CUDADeviceDetail(const CUDA_DeviceInfo& device_info):
-            dm_device_info(device_info),
-            dm_name(device_info.name)
-        {
-        }
-        
-        /* Name of this device. */
-        std::string getName() const
-        {
-            return dm_name;
-        }
-
-        /** Compute capability (major/minor) of this device. */
-        Vector2u getComputeCapability() const
-        {
-            return Vector2u(dm_device_info.compute_capability[0],
-                            dm_device_info.compute_capability[1]);
-        }
-        
-        /** Maximum allowed dimensions of grids with this device. */
-        Vector3u getMaxGrid() const
-        {
-            return Vector3u(dm_device_info.max_grid[0],
-                            dm_device_info.max_grid[1],
-                            dm_device_info.max_grid[2]);
-        }
-
-        /** Maximum allowed dimensions of grids blocks this device. */
-        Vector3u getMaxBlock() const
-        {
-            return Vector3u(dm_device_info.max_block[0],
-                            dm_device_info.max_block[1],
-                            dm_device_info.max_block[2]);
-        }
-
-        /** Global memory bandwidth of this device (in KBytes/sec). */
-        unsigned long long getGlobalMemoryBandwidth() const
-        {
-            return dm_device_info.global_memory_bandwidth;
-        }
-        
-        /** Global memory size of this device (in bytes). */
-        unsigned long long getGlobalMemorySize() const
-        {
-            return dm_device_info.global_memory_size;
-        }
-
-        /** Constant memory size of this device (in bytes). */
-        unsigned long long getConstantMemorySize() const
-        {
-            return dm_device_info.constant_memory_size;
-        }
-        
-        /** L2 cache size of this device (in bytes). */
-        unsigned int getL2CacheSize() const
-        {
-            return dm_device_info.l2_cache_size;
-        }
-        
-        /** Number of threads per warp for this device. */
-        unsigned int getThreadsPerWarp() const
-        {
-            return dm_device_info.threads_per_warp;
-        }
-        
-        /** Core clock rate of this device (in KHz). */
-        unsigned int getCoreClockRate() const
-        {
-            return dm_device_info.core_clock_rate;
-        }
-        
-        /** Number of memory copy engines on this device. */
-        unsigned int getMemcpyEngines() const
-        {
-            return dm_device_info.memcpy_engines;
-        }
-        
-        /** Number of multiprocessors on this device. */
-        unsigned int getMultiprocessors() const
-        {
-            return dm_device_info.multiprocessors;
-        }
-        
-        /**
-         * Maximum instructions/cycle possible on this device's multiprocessors.
-         */
-        unsigned int getMaxIPC() const
-        {
-            return dm_device_info.max_ipc;
-        }
-        
-        /** Maximum warps/multiprocessor for this device. */
-        unsigned int getMaxWarpsPerMultiprocessor() const
-        {
-            return dm_device_info.max_warps_per_multiprocessor;
-        }
-        
-        /** Maximum blocks/multiprocessor for this device. */
-        unsigned int getMaxBlocksPerMultiprocessor() const
-        {
-            return dm_device_info.max_blocks_per_multiprocessor;
-        }
-        
-        /** Maximum registers/block for this device. */
-        unsigned int getMaxRegistersPerBlock() const
-        {
-            return dm_device_info.max_registers_per_block;
-        }
-        
-        /** Maximium shared memory / block for this device. */
-        unsigned int getMaxSharedMemoryPerBlock() const
-        {
-            return dm_device_info.max_shared_memory_per_block;
-        }
-        
-        /** Maximum threads/block for this device. */
-        unsigned int getMaxThreadsPerBlock() const
-        {
-            return dm_device_info.max_threads_per_block;
-        }
-                
-    private:
-
-        /** Raw CUDA message containing the device details. */
-        CUDA_DeviceInfo dm_device_info;
-        
-        /** Device name stored in a copy-safe C++ string. */
-        std::string dm_name;
-        
-    }; // class CUDADeviceDetail
-    
-} } // namespace OpenSpeedShop::Framework
diff -Naur OpenSpeedShop.ORIGINAL/plugins/collectors/cuda/CUDAExecDetail.hxx OpenSpeedShop.NEW-CUPTI/plugins/collectors/cuda/CUDAExecDetail.hxx
--- OpenSpeedShop.ORIGINAL/plugins/collectors/cuda/CUDAExecDetail.hxx	2014-12-16 23:37:09.000000000 -0600
+++ OpenSpeedShop.NEW-CUPTI/plugins/collectors/cuda/CUDAExecDetail.hxx	2016-02-26 16:07:37.774987654 -0600
@@ -1,5 +1,5 @@
 ////////////////////////////////////////////////////////////////////////////////
-// Copyright (c) 2014 Argo Navis Technologies. All Rights Reserved.
+// Copyright (c) 2014-2016 Argo Navis Technologies. All Rights Reserved.
 //
 // This library is free software; you can redistribute it and/or modify it under
 // the terms of the GNU Lesser General Public License as published by the Free
@@ -24,17 +24,12 @@
 #include "config.h"
 #endif
 
-#include <boost/tuple/tuple.hpp>
-#include <string>
+#include <ArgoNavis/Base/TimeInterval.hpp>
+#include <ArgoNavis/CUDA/Device.hpp>
+#include <ArgoNavis/CUDA/KernelExecution.hpp>
 
-#include "KrellInstitute/Messages/CUDA_data.h"
-
-#include "SmartPtr.hxx"
-#include "Time.hxx"
 #include "TotallyOrdered.hxx"
 
-#include "CUDADeviceDetail.hxx"
-
 namespace OpenSpeedShop { namespace Framework {
 
     /**
@@ -44,147 +39,49 @@
      * executions recorded by the CUDA collector.
      */
     class CUDAExecDetail :
+        public ArgoNavis::CUDA::KernelExecution,
         public TotallyOrdered<CUDAExecDetail>
     {
 
     public:
 
-        /** Vector of three unsigned integers. */
-        typedef boost::tuple<unsigned int, unsigned int, unsigned int> Vector3u;
-        
-        /** Constructor from raw CUDA messages. */
-        CUDAExecDetail(const double& time,
-                       const SmartPtr<CUDADeviceDetail>& device_detail,
-                       const CUDA_EnqueueRequest& enqueue_request,
-                       const CUDA_ExecutedKernel& executed_kernel) :
-            dm_time(time),
-            dm_device_detail(device_detail),
-            dm_enqueue_request(enqueue_request),
-            dm_executed_kernel(executed_kernel),
-            dm_function(executed_kernel.function)
+        /** Constructor. */
+        CUDAExecDetail(const ArgoNavis::CUDA::KernelExecution& event,
+                       const ArgoNavis::CUDA::Device& device,
+                       const double& t) :
+            ArgoNavis::CUDA::KernelExecution(event),
+            dm_device(device),
+            dm_time(time)
         {
         }
 
         /** Operator "<" defined for two CUDAExecDetail objects. */
         bool operator<(const CUDAExecDetail& other) const
         {
-            return TimeInterval(getTimeBegin(), getTimeEnd()) <
-                TimeInterval(other.getTimeBegin(), other.getTimeEnd());
+            return ArgoNavis::Base::TimeInterval(time_begin, time_end) <
+                ArgoNavis::Base::TimeInterval(other.time_begin, other.time_end);
         }
         
-        /** Time spent in the kernel execution (in seconds). */
-        double getTime() const
-        {
-            return dm_time;
-        }
-
         /** Device performing the kernel execution. */
-        const SmartPtr<CUDADeviceDetail>& getDevice() const
-        {
-            return dm_device_detail;
-        }
-
-        /** Time at which the kernel execution was enqueued. */
-        Time getTimeEnqueue() const
+        const ArgoNavis::CUDA::Device& getDevice() const
         {
-            return dm_enqueue_request.time;
-        }
-
-        /** Time at which the kernel execution began. */
-        Time getTimeBegin() const
-        {
-            return dm_executed_kernel.time_begin;
-        }
-
-        /** Time at which the kernel execution ended. */
-        Time getTimeEnd() const
-        {
-            return dm_executed_kernel.time_end;
-        }
-
-        /** Name of the kernel function being executed. */
-        std::string getName() const
-        {
-            return dm_function;
-        }
-        
-        /** Dimensions of the grid. */
-        Vector3u getGrid() const
-        {
-            return Vector3u(dm_executed_kernel.grid[0],
-                            dm_executed_kernel.grid[1],
-                            dm_executed_kernel.grid[2]);
+            return dm_device;
         }
         
-        /** Dimensions of each block. */
-        Vector3u getBlock() const
-        {
-            return Vector3u(dm_executed_kernel.block[0],
-                            dm_executed_kernel.block[1],
-                            dm_executed_kernel.block[2]);
-        }
-        
-        /** Cache preference used. */
-        std::string getCachePreference() const
-        {
-            return stringify(dm_executed_kernel.cache_preference);
-        }
-        
-        /** Registers required for each thread. */
-        unsigned int getRegistersPerThread() const
-        {
-            return dm_executed_kernel.registers_per_thread;
-        }
-        
-        /** Total amount (in bytes) of static shared memory reserved. */
-        unsigned long long getStaticSharedMemory() const
-        {
-            return dm_executed_kernel.static_shared_memory;
-        }
-        
-        /** Total amount (in bytes) of dynamic shared memory reserved. */
-        unsigned long long getDynamicSharedMemory() const
-        {
-            return dm_executed_kernel.dynamic_shared_memory;
-        }
-        
-        /** Total amount (in bytes) of local memory reserved. */
-        unsigned long long getLocalMemory() const
+        /** Time spent in the kernel execution (in seconds). */
+        double getTime() const
         {
-            return dm_executed_kernel.local_memory;
+            return dm_time;
         }
         
     private:
 
-        /** Stringify a CUDA_CachePreference. */
-        std::string stringify(const CUDA_CachePreference& value) const
-        {
-            switch (value)
-            {
-            case InvalidCachePreference: return "Invalid";
-            case NoPreference: return "None";
-            case PreferShared: return "Shared";
-            case PreferCache: return "Cache";
-            case PreferEqual: return "Equal";
-            }
-            return "?";
-        }
+        /** Device performing the kernel execution. */
+        ArgoNavis::CUDA::Device dm_device;
 
         /** Time spent in the kernel execution. */
         double dm_time;
         
-        /** Details for the device performing the kernel execution. */
-        SmartPtr<CUDADeviceDetail> dm_device_detail;
-        
-        /** Raw CUDA message describing the enqueued request. */
-        CUDA_EnqueueRequest dm_enqueue_request;
-
-        /** Raw CUDA message describing the kernel execution. */
-        CUDA_ExecutedKernel dm_executed_kernel;
-
-        /** Kernel name stored in a copy-safe C++ string. */
-        std::string dm_function;
-        
     }; // class CUDAExecDetail
     
 } } // namespace OpenSpeedShop::Framework
diff -Naur OpenSpeedShop.ORIGINAL/plugins/collectors/cuda/CUDAXferDetail.hxx OpenSpeedShop.NEW-CUPTI/plugins/collectors/cuda/CUDAXferDetail.hxx
--- OpenSpeedShop.ORIGINAL/plugins/collectors/cuda/CUDAXferDetail.hxx	2014-12-16 23:37:09.000000000 -0600
+++ OpenSpeedShop.NEW-CUPTI/plugins/collectors/cuda/CUDAXferDetail.hxx	2016-02-26 16:07:37.774987654 -0600
@@ -1,5 +1,5 @@
 ////////////////////////////////////////////////////////////////////////////////
-// Copyright (c) 2014 Argo Navis Technologies. All Rights Reserved.
+// Copyright (c) 2014-2016 Argo Navis Technologies. All Rights Reserved.
 //
 // This library is free software; you can redistribute it and/or modify it under
 // the terms of the GNU Lesser General Public License as published by the Free
@@ -24,15 +24,12 @@
 #include "config.h"
 #endif
 
-#include <string>
+#include <ArgoNavis/Base/TimeInterval.hpp>
+#include <ArgoNavis/CUDA/DataTransfer.hpp>
+#include <ArgoNavis/CUDA/Device.hpp>
 
-#include "KrellInstitute/Messages/CUDA_data.h"
-
-#include "SmartPtr.hxx"
 #include "TotallyOrdered.hxx"
 
-#include "CUDADeviceDetail.hxx"
-
 namespace OpenSpeedShop { namespace Framework {
 
     /**
@@ -42,139 +39,49 @@
      * transfers recorded by the CUDA collector.
      */
     class CUDAXferDetail :
+        public ArgoNavis::CUDA::DataTransfer,
         public TotallyOrdered<CUDAXferDetail>
     {
 
     public:
 
-        /** Constructor from raw CUDA messages. */
-        CUDAXferDetail(const double& time,
-                       const SmartPtr<CUDADeviceDetail>& device_detail,
-                       const CUDA_EnqueueRequest& enqueue_request,
-                       const CUDA_CopiedMemory& copied_memory) :
-            dm_time(time),
-            dm_device_detail(device_detail),
-            dm_enqueue_request(enqueue_request),
-            dm_copied_memory(copied_memory)
+        /** Constructor. */
+        CUDAXferDetail(const ArgoNavis::CUDA::DataTransfer& event,
+                       const ArgoNavis::CUDA::Device& device,
+                       const double& time):
+            ArgoNavis::CUDA::DataTransfer(event),
+            dm_device(device),
+            dm_time(time)
         {
         }
 
         /** Operator "<" defined for two CUDAXferDetail objects. */
         bool operator<(const CUDAXferDetail& other) const
         {
-            return TimeInterval(getTimeBegin(), getTimeEnd()) <
-                TimeInterval(other.getTimeBegin(), other.getTimeEnd());
-        }
-
-        /** Time spent in the kernel execution (in seconds). */
-        double getTime() const
-        {
-            return dm_time;
+            return ArgoNavis::Base::TimeInterval(time_begin, time_end) <
+                ArgoNavis::Base::TimeInterval(other.time_begin, other.time_end);
         }
 
         /** Device performing the data transfer. */
-        const SmartPtr<CUDADeviceDetail>& getDevice() const
-        {
-            return dm_device_detail;
-        }
-
-        /** Time at which the data transfer was enqueued. */
-        Time getTimeEnqueue() const
+        const ArgoNavis::CUDA::Device& getDevice() const
         {
-            return dm_enqueue_request.time;
+            return dm_device;
         }
-
-        /** Time at which the data transfer began. */
-        Time getTimeBegin() const
-        {
-            return dm_copied_memory.time_begin;
-        }
-
-        /** Time at which the data transfer ended. */
-        Time getTimeEnd() const
+         
+        /** Time spent in the kernel execution (in seconds). */
+        double getTime() const
         {
-            return dm_copied_memory.time_end;
+            return dm_time;
         }
 
-        /** Number of bytes being transferred. */
-        unsigned long long getSize() const
-        {
-            return dm_copied_memory.size;
-        }
-        
-        /** Kind of data transfer performed. */
-        std::string getKind() const
-        {
-            return stringify(dm_copied_memory.kind);
-        }
-        
-        /** Kind of memory from which the data transfer was performed. */
-        std::string getSourceKind() const
-        {
-            return stringify(dm_copied_memory.source_kind);
-        }
-        
-        /** Kind of memory to which the data transfer was performed. */
-        std::string getDestinationKind() const
-        {
-            return stringify(dm_copied_memory.destination_kind);
-        }
-        
-        /** Was the data transfer asynchronous? */
-        bool getAsynchronous() const
-        {
-            return dm_copied_memory.asynchronous;
-        }
-        
     private:
  
-        /** Stringify a CUDA_CopyKind. */
-        std::string stringify(const CUDA_CopyKind& value) const
-        {
-            switch (value)
-            {
-            case InvalidCopyKind: return "Invalid";
-            case UnknownCopyKind: return "Unknown";
-            case HostToDevice: return "HostToDevice";
-            case DeviceToHost: return "DeviceToHost";
-            case HostToArray: return "HostToArray";
-            case ArrayToHost: return "ArrayToHost";
-            case ArrayToArray: return "ArrayToArray";
-            case ArrayToDevice: return "ArrayToDevice";
-            case DeviceToArray: return "DeviceToArray";
-            case DeviceToDevice: return "DeviceToDevice";
-            case HostToHost: return "HostToHost";
-            }
-            return "?";
-        }
-
-        /** Stringify a CUDA_MemoryKind. */
-        std::string stringify(const CUDA_MemoryKind& value) const
-        {
-            switch (value)
-            {
-            case InvalidMemoryKind: return "Invalid";
-            case UnknownMemoryKind: return "Unknown";
-            case Pageable: return "Pageable";
-            case Pinned: return "Pinned";
-            case Device: return "Device";
-            case Array: return "Array";
-            }
-            return "?";
-        }
+        /** Device performing the data transfer. */
+        ArgoNavis::CUDA::Device dm_device;
 
         /** Time spent in the data transfer. */
         double dm_time;
         
-        /** Details for the device performing the data transfer. */
-        SmartPtr<CUDADeviceDetail> dm_device_detail;
-        
-        /** Raw CUDA message describing the enqueued request. */
-        CUDA_EnqueueRequest dm_enqueue_request;
-        
-        /** Raw CUDA message describing the data transfer. */
-        CUDA_CopiedMemory dm_copied_memory;
-
     }; // class CUDAXferDetail
     
 } } // namespace OpenSpeedShop::Framework
diff -Naur OpenSpeedShop.ORIGINAL/plugins/collectors/cuda/Makefile.am OpenSpeedShop.NEW-CUPTI/plugins/collectors/cuda/Makefile.am
--- OpenSpeedShop.ORIGINAL/plugins/collectors/cuda/Makefile.am	2014-10-27 23:12:10.000000000 -0500
+++ OpenSpeedShop.NEW-CUPTI/plugins/collectors/cuda/Makefile.am	1969-12-31 18:00:00.000000000 -0600
@@ -1,51 +0,0 @@
-################################################################################
-# Copyright (c) 2014 Argo Navis Technologies. All Rights Reserved.
-#
-# This library is free software; you can redistribute it and/or modify it under
-# the terms of the GNU Lesser General Public License as published by the Free
-# Software Foundation; either version 2.1 of the License, or (at your option)
-# any later version.
-#
-# This library is distributed in the hope that it will be useful, but WITHOUT
-# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
-# FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more
-# details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this library; if not, write to the Free Software Foundation, Inc.,
-# 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
-################################################################################
-
-pkglib_LTLIBRARIES =
-
-if BUILD_CBTF
-if HAVE_CBTF_MESSAGES_CUDA
-
-pkglib_LTLIBRARIES += cuda.la
-
-cuda_la_CXXFLAGS = \
-	@BOOST_CPPFLAGS@ \
-	@MESSAGES_CPPFLAGS@ \
-	@MESSAGES_CUDA_CPPFLAGS@ \
-	-I$(top_srcdir)/libopenss-framework
-
-cuda_la_LDFLAGS = \
-	@MESSAGES_LDFLAGS@ \
-	@MESSAGES_CUDA_LDFLAGS@ \
-	-L$(top_srcdir)/libopenss-framework \
-	-no-undefined -module -avoid-version
-
-cuda_la_LIBADD = \
-	@MESSAGES_LIBS@ \
-	@MESSAGES_CUDA_LIBS@ \
-	-lopenss-framework
-
-cuda_la_SOURCES = \
-	CUDACollector.hxx CUDACollector.cxx \
-	CUDACountsDetail.hxx \
-	CUDADeviceDetail.hxx \
-	CUDAExecDetail.hxx \
-	CUDAXferDetail.hxx
-
-endif
-endif
diff -Naur OpenSpeedShop.ORIGINAL/plugins/views/CMakeLists.txt OpenSpeedShop.NEW-CUPTI/plugins/views/CMakeLists.txt
--- OpenSpeedShop.ORIGINAL/plugins/views/CMakeLists.txt	2016-01-19 21:05:47.000000000 -0600
+++ OpenSpeedShop.NEW-CUPTI/plugins/views/CMakeLists.txt	2016-02-26 16:07:37.774987654 -0600
@@ -1,5 +1,6 @@
 ################################################################################
 # Copyright (c) 2014-2016 Krell Institute. All Rights Reserved.
+# Copyright (c) 2015,2016 Argo Navis Technologies. All Rights Reserved.
 #
 # This program is free software; you can redistribute it and/or modify it under
 # the terms of the GNU General Public License as published by the Free Software
@@ -38,9 +39,7 @@
 add_subdirectory(pthreads)
 add_subdirectory(iop)
 add_subdirectory(mpip)
-# If cuda related plugins and messages were found
-# then build the cuda views.
-if (CBTF-ARGO-MESSAGESCUDA_FOUND)
+
+if(BUILD_CUDA_SUPPORT)
     add_subdirectory(cuda)
 endif()
-
diff -Naur OpenSpeedShop.ORIGINAL/plugins/views/CMakeLists.txt.~1.6.~ OpenSpeedShop.NEW-CUPTI/plugins/views/CMakeLists.txt.~1.6.~
--- OpenSpeedShop.ORIGINAL/plugins/views/CMakeLists.txt.~1.6.~	1969-12-31 18:00:00.000000000 -0600
+++ OpenSpeedShop.NEW-CUPTI/plugins/views/CMakeLists.txt.~1.6.~	2016-02-26 16:07:37.774987654 -0600
@@ -0,0 +1,46 @@
+################################################################################
+# Copyright (c) 2014-2016 Krell Institute. All Rights Reserved.
+#
+# This program is free software; you can redistribute it and/or modify it under
+# the terms of the GNU General Public License as published by the Free Software
+# Foundation; either version 2 of the License, or (at your option) any later
+# version.
+#
+# This program is distributed in the hope that it will be useful, but WITHOUT
+# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
+# details.
+#
+# You should have received a copy of the GNU General Public License along with
+# this program; if not, write to the Free Software Foundation, Inc., 59 Temple
+# Place, Suite 330, Boston, MA  02111-1307  USA
+################################################################################
+
+add_subdirectory(pcsamp)
+add_subdirectory(usertime)
+add_subdirectory(hwc)
+add_subdirectory(hwcsamp)
+add_subdirectory(hwctime)
+add_subdirectory(io)
+
+# FIXME iot has problems with syscall types on ARM
+if (NOT RUNTIME_PLATFORM MATCHES "arm")
+   add_subdirectory(iot)
+endif()
+
+add_subdirectory(mpi)
+add_subdirectory(mpit)
+
+# with cmake are we deciding to not support fpe?
+#add_subdirectory(fpe)
+
+add_subdirectory(mem)
+add_subdirectory(pthreads)
+add_subdirectory(iop)
+add_subdirectory(mpip)
+# If cuda related plugins and messages were found
+# then build the cuda views.
+if (CBTF-ARGO-MESSAGESCUDA_FOUND)
+    add_subdirectory(cuda)
+endif()
+
diff -Naur OpenSpeedShop.ORIGINAL/plugins/views/cuda/CMakeLists.txt OpenSpeedShop.NEW-CUPTI/plugins/views/cuda/CMakeLists.txt
--- OpenSpeedShop.ORIGINAL/plugins/views/cuda/CMakeLists.txt	2015-06-02 17:38:27.000000000 -0500
+++ OpenSpeedShop.NEW-CUPTI/plugins/views/cuda/CMakeLists.txt	2016-02-26 16:07:37.774987654 -0600
@@ -1,5 +1,6 @@
 ################################################################################
 # Copyright (c) 2014-2015 Krell Institute. All Rights Reserved.
+# Copyright (c) 2015,2016 Argo Navis Technologies. All Rights Reserved.
 #
 # This program is free software; you can redistribute it and/or modify it under
 # the terms of the GNU General Public License as published by the Free Software
@@ -16,40 +17,34 @@
 # Place, Suite 330, Boston, MA  02111-1307  USA
 ################################################################################
 
-set(VIEW_SOURCES
-	cuda_exec_view.cxx
+add_library(cuda_view MODULE
+    cuda_exec_view.cxx
 	cuda_hwpc_view.cxx
 	cuda_xfer_view.cxx
 	cuda_view.cxx
-)
-
-add_library(cuda_view MODULE ${VIEW_SOURCES})
+    )
 
 target_include_directories(cuda_view PUBLIC
-        ${CMAKE_CURRENT_SOURCE_DIR}
-        ${CMAKE_CURRENT_BINARY_DIR}
-        ${Boost_INCLUDE_DIRS}
-        ${Python_INCLUDE_DIRS}
-        ${CBTF_MESSAGES_INCLUDE_DIRS}
-        ${CBTF_MESSAGES_CUDA_INCLUDE_DIRS}
-        ${PROJECT_SOURCE_DIR}/libopenss-cli
-        ${PROJECT_SOURCE_DIR}/libopenss-runtime
-        ${PROJECT_SOURCE_DIR}/libopenss-message
-        ${PROJECT_SOURCE_DIR}/libopenss-queries
-        ${PROJECT_SOURCE_DIR}/plugins/collectors/cuda
-)
+    ${CMAKE_CURRENT_SOURCE_DIR}
+    ${PROJECT_SOURCE_DIR}/libopenss-cli
+    ${PROJECT_SOURCE_DIR}/libopenss-runtime
+    ${PROJECT_SOURCE_DIR}/libopenss-message
+    ${PROJECT_SOURCE_DIR}/libopenss-queries
+    ${PROJECT_SOURCE_DIR}/libopenss-queries-cuda
+    ${PROJECT_SOURCE_DIR}/plugins/collectors/cuda
+    ${Boost_INCLUDE_DIRS}
+    ${Python_INCLUDE_DIRS}
+    )
 
 target_link_libraries(cuda_view
-	pthread
-        ${CBTF_MESSAGES_LIBRARIES}
-        ${CBTF_MESSAGES_CUDA_LIBRARIES}
-	openss-cli
-	${CMAKE_DL_LIBS}
-)
+    openss-cli
+    openss-queries-cuda
+    pthread
+    ${CMAKE_DL_LIBS}
+    )
 
 set_target_properties(cuda_view PROPERTIES PREFIX "")
 
 install(TARGETS cuda_view
-	LIBRARY DESTINATION lib${LIB_SUFFIX}/openspeedshop
-)
-
+    LIBRARY DESTINATION lib${LIB_SUFFIX}/openspeedshop
+    )
diff -Naur OpenSpeedShop.ORIGINAL/plugins/views/cuda/cuda_exec_view.cxx OpenSpeedShop.NEW-CUPTI/plugins/views/cuda/cuda_exec_view.cxx
--- OpenSpeedShop.ORIGINAL/plugins/views/cuda/cuda_exec_view.cxx	2015-01-14 17:25:46.000000000 -0600
+++ OpenSpeedShop.NEW-CUPTI/plugins/views/cuda/cuda_exec_view.cxx	2016-02-26 16:07:37.774987654 -0600
@@ -1,6 +1,6 @@
 ////////////////////////////////////////////////////////////////////////////////
 // Copyright (c) 2014 Krell Institute. All Rights Reserved.
-// Copyright (c) 2014,2015 Argo Navis Technologies. All Rights Reserved.
+// Copyright (c) 2014-2016 Argo Navis Technologies. All Rights Reserved.
 //
 // This library is free software; you can redistribute it and/or modify it under
 // the terms of the GNU Lesser General Public License as published by the Free
@@ -23,8 +23,8 @@
 #include "SS_View_Expr.hxx"
 
 #include "CUDACollector.hxx"
-#include "CUDADeviceDetail.hxx"
 #include "CUDAExecDetail.hxx"
+#include "CUDAQueries.hxx"
 
 #define PUSH_HV(x) HV.push_back(x)
 #define PUSH_IV(...) IV.push_back(new ViewInstruction(__VA_ARGS__))
@@ -103,30 +103,30 @@
     uint64_t detail_dsm = 0;          \
     uint64_t detail_lm = 0;
 
-#define get_CUDA_invalues(primary, num_calls, function_name)  \
-    double v = primary.getTime() / num_calls;                 \
-    intime += v;                                              \
-    incnt++;                                                  \
-    start = min(start, primary.getTimeBegin());               \
-    end = max(end, primary.getTimeEnd());                     \
-    vmin = min(vmin, v);                                      \
-    vmax = max(vmax, v);                                      \
-    sum_squares += v * v;                                     \
-    detail_grid = str(format("%1%,%2%,%3%") %                 \
-        primary.getGrid().get<0>() %                          \
-        primary.getGrid().get<1>() %                          \
-        primary.getGrid().get<2>()                            \
-        );                                                    \
-    detail_block = str(format("%1%,%2%,%3%") %                \
-        primary.getBlock().get<0>() %                         \
-        primary.getBlock().get<1>() %                         \
-        primary.getBlock().get<2>()                           \
-        );                                                    \
-    detail_cache = primary.getCachePreference();              \
-    detail_rpt = primary.getRegistersPerThread();             \
-    detail_ssm = primary.getStaticSharedMemory();             \
-    detail_dsm = primary.getDynamicSharedMemory();            \
-    detail_lm = primary.getLocalMemory();
+#define get_CUDA_invalues(primary, num_calls, function_name)               \
+    double v = primary.getTime() / num_calls;                              \
+    intime += v;                                                           \
+    incnt++;                                                               \
+    start = min(start, Queries::ConvertFromArgoNavis(primary.time_begin)); \
+    end = max(end, Queries::ConvertFromArgoNavis(primary.time_end));       \
+    vmin = min(vmin, v);                                                   \
+    vmax = max(vmax, v);                                                   \
+    sum_squares += v * v;                                                  \
+    detail_grid = str(format("%1%,%2%,%3%") %                              \
+        primary.grid.get<0>() %                                            \
+        primary.grid.get<1>() %                                            \
+        primary.grid.get<2>()                                              \
+        );                                                                 \
+    detail_block = str(format("%1%,%2%,%3%") %                             \
+        primary.block.get<0>() %                                           \
+        primary.block.get<1>() %                                           \
+        primary.block.get<2>()                                             \
+        );                                                                 \
+    detail_cache = primary.cache_preference;                               \
+    detail_rpt = primary.registers_per_thread;                             \
+    detail_ssm = primary.static_shared_memory;                             \
+    detail_dsm = primary.dynamic_shared_memory;                            \
+    detail_lm = primary.local_memory;
 
 #define get_CUDA_exvalues(secondary, num_calls)  \
     extime += secondary.getTime() / num_calls;   \
diff -Naur OpenSpeedShop.ORIGINAL/plugins/views/cuda/cuda_hwpc_view.cxx OpenSpeedShop.NEW-CUPTI/plugins/views/cuda/cuda_hwpc_view.cxx
--- OpenSpeedShop.ORIGINAL/plugins/views/cuda/cuda_hwpc_view.cxx	2015-01-14 17:25:46.000000000 -0600
+++ OpenSpeedShop.NEW-CUPTI/plugins/views/cuda/cuda_hwpc_view.cxx	2016-02-26 16:07:37.774987654 -0600
@@ -1,6 +1,6 @@
 ////////////////////////////////////////////////////////////////////////////////
 // Copyright (c) 2010-2014 Krell Institute. All Rights Reserved.
-// Copyright (c) 2015 Argo Navis Technologies. All Rights Reserved.
+// Copyright (c) 2015,2016 Argo Navis Technologies. All Rights Reserved.
 //
 // This library is free software; you can redistribute it and/or modify it under
 // the terms of the GNU Lesser General Public License as published by the Free
@@ -21,7 +21,6 @@
 #include "SS_View_Expr.hxx"
 
 #include "CUDACollector.hxx"
-#include "CUDADeviceDetail.hxx"
 #include "CUDACountsDetail.hxx"
 
 #define PUSH_HV(x) HV.push_back(x)
diff -Naur OpenSpeedShop.ORIGINAL/plugins/views/cuda/cuda_xfer_view.cxx OpenSpeedShop.NEW-CUPTI/plugins/views/cuda/cuda_xfer_view.cxx
--- OpenSpeedShop.ORIGINAL/plugins/views/cuda/cuda_xfer_view.cxx	2015-01-14 17:25:46.000000000 -0600
+++ OpenSpeedShop.NEW-CUPTI/plugins/views/cuda/cuda_xfer_view.cxx	2016-02-26 16:07:37.774987654 -0600
@@ -1,6 +1,6 @@
 ////////////////////////////////////////////////////////////////////////////////
 // Copyright (c) 2014 Krell Institute. All Rights Reserved.
-// Copyright (c) 2014,2015 Argo Navis Technologies. All Rights Reserved.
+// Copyright (c) 2014-2016 Argo Navis Technologies. All Rights Reserved.
 //
 // This library is free software; you can redistribute it and/or modify it under
 // the terms of the GNU Lesser General Public License as published by the Free
@@ -23,8 +23,8 @@
 #include "SS_View_Expr.hxx"
 
 #include "CUDACollector.hxx"
-#include "CUDADeviceDetail.hxx"
 #include "CUDAXferDetail.hxx"
+#include "CUDAQueries.hxx"
 
 #define PUSH_HV(x) HV.push_back(x)
 #define PUSH_IV(...) IV.push_back(new ViewInstruction(__VA_ARGS__))
@@ -99,20 +99,20 @@
     string detail_dest = "";          \
     string detail_async = "";
 
-#define get_CUDA_invalues(primary, num_calls, function_name)  \
-    double v = primary.getTime() / num_calls;                 \
-    intime += v;                                              \
-    incnt++;                                                  \
-    start = min(start, primary.getTimeBegin());               \
-    end = max(end, primary.getTimeEnd());                     \
-    vmin = min(vmin, v);                                      \
-    vmax = max(vmax, v);                                      \
-    sum_squares += v * v;                                     \
-    detail_size = primary.getSize();                          \
-    detail_kind = primary.getKind();                          \
-    detail_src = primary.getSourceKind();                     \
-    detail_dest = primary.getDestinationKind();               \
-    detail_async = primary.getAsynchronous() ? "Yes" : "No";
+#define get_CUDA_invalues(primary, num_calls, function_name)                \
+    double v = primary.getTime() / num_calls;                               \
+    intime += v;                                                            \
+    incnt++;                                                                \
+    start = min(start, Queries::ConvertFromArgoNavis(primary.time_begin));  \
+    end = max(end, Queries::ConvertFromArgoNavis(primary.time_end));        \
+    vmin = min(vmin, v);                                                    \
+    vmax = max(vmax, v);                                                    \
+    sum_squares += v * v;                                                   \
+    detail_size = primary.size;                                             \
+    detail_kind = Queries::ConvertFromArgoNavis(primary.kind);              \
+    detail_src = Queries::ConvertFromArgoNavis(primary.source_kind);        \
+    detail_dest = Queries::ConvertFromArgoNavis(primary.destination_kind);  \
+    detail_async = primary.asynchronous ? "Yes" : "No";
 
 #define get_CUDA_exvalues(secondary, num_calls)  \
     extime += secondary.getTime() / num_calls;   \
diff -Naur OpenSpeedShop.ORIGINAL/plugins/views/cuda/Makefile.am OpenSpeedShop.NEW-CUPTI/plugins/views/cuda/Makefile.am
--- OpenSpeedShop.ORIGINAL/plugins/views/cuda/Makefile.am	2015-01-14 17:25:46.000000000 -0600
+++ OpenSpeedShop.NEW-CUPTI/plugins/views/cuda/Makefile.am	1969-12-31 18:00:00.000000000 -0600
@@ -1,48 +0,0 @@
-################################################################################
-# Copyright (c) 2014 Krell Institute. All Rights Reserved.
-# Copyright (c) 2014,2015 Argo Navis Technologies. All Rights Reserved.
-#
-# This library is free software; you can redistribute it and/or modify it under
-# the terms of the GNU Lesser General Public License as published by the Free
-# Software Foundation; either version 2.1 of the License, or (at your option)
-# any later version.
-#
-# This library is distributed in the hope that it will be useful, but WITHOUT
-# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
-# FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more
-# details.
-#
-# You should have received a copy of the GNU Lesser General Public License
-# along with this library; if not, write to the Free Software Foundation, Inc.,
-# 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
-################################################################################
-
-pkglib_LTLIBRARIES = cuda_view.la
-
-cuda_view_la_CXXFLAGS = \
-	@BOOST_CPPFLAGS@ \
-	@MESSAGES_CPPFLAGS@ \
-	@MESSAGES_CUDA_CPPFLAGS@ \
-	@PYTHON_CPPFLAGS@ \
-	-I$(top_srcdir)/libopenss-cli \
-	-I$(top_srcdir)/libopenss-framework \
-	-I$(top_srcdir)/libopenss-message \
-	-I$(top_srcdir)/libopenss-queries \
-	-I$(top_srcdir)/plugins/collectors/cuda
-
-cuda_view_la_LDFLAGS = \
-	@MESSAGES_LDFLAGS@ \
-	@MESSAGES_CUDA_LDFLAGS@ \
-	-L$(top_srcdir)/libopenss-cli \
-	-no-undefined -module -avoid-version
-
-cuda_view_la_LIBADD = \
-	@MESSAGES_LIBS@ \
-	@MESSAGES_CUDA_LIBS@ \
-	-lopenss-cli
-
-cuda_view_la_SOURCES = \
-	cuda_exec_view.cxx \
-	cuda_hwpc_view.cxx \
-	cuda_xfer_view.cxx \
-	cuda_view.cxx
